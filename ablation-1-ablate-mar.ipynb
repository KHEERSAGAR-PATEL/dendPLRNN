{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Cell 1: Environment Setup & Reset ---\n\n!rm -rf /kaggle/working/dendPLRNN\n!git clone --depth 1 https://github.com/DurstewitzLab/dendPLRNN.git /kaggle/working/dendPLRNN\nprint(\"✅ Repository cloned.\")\n\n# Install dependencies\n!pip install numpy scipy scikit-learn matplotlib tensorboardX\n!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\nprint(\"✅ Dependencies installed.\")\n\n# Ensure Python can find project modules\nimport sys\nsys.path.insert(0, \"/kaggle/working/dendPLRNN/BPTT_TF\")\nsys.path.insert(0, \"/kaggle/working/dendPLRNN\")\nprint(\"✅ PYTHONPATH ready.\")\n\n# Change to the correct directory\n%cd /kaggle/working/dendPLRNN/BPTT_TF\nprint(f\"✅ Current directory: $(pwd)\")\n\n# Fix known import and experiment issues\n!sed -i \"s/from pandas.core.indexes import numeric/from pandas.api.types import is_numeric_dtype as numeric/\" main_eval.py\n!sed -i \"s/Argument('n_epochs', \\[[0-9]*\\])/Argument('n_epochs', [10])/\" \\\n/kaggle/working/dendPLRNN/BPTT_TF/Experiments/Table1/ECG/ubermain.py\nprint(\"✅ Pandas and Epoch fixes applied.\")\n\n!nvidia-smi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T18:17:20.001299Z","iopub.execute_input":"2025-11-12T18:17:20.001530Z","iopub.status.idle":"2025-11-12T18:18:43.156393Z","shell.execute_reply.started":"2025-11-12T18:17:20.001512Z","shell.execute_reply":"2025-11-12T18:18:43.155589Z"}},"outputs":[{"name":"stdout","text":"Cloning into '/kaggle/working/dendPLRNN'...\nremote: Enumerating objects: 137, done.\u001b[K\nremote: Counting objects: 100% (137/137), done.\u001b[K\nremote: Compressing objects: 100% (117/117), done.\u001b[K\nremote: Total 137 (delta 27), reused 122 (delta 19), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (137/137), 60.51 MiB | 39.62 MiB/s, done.\nResolving deltas: 100% (27/27), done.\n✅ Repository cloned.\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.3)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.2)\nCollecting tensorboardX\n  Downloading tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.3.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (6.33.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\nDownloading tensorboardx-2.6.4-py3-none-any.whl (87 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: tensorboardX\nSuccessfully installed tensorboardX-2.6.4\nLooking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu113\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\nRequirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.20.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.15.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.10.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n✅ Dependencies installed.\n✅ PYTHONPATH ready.\n/kaggle/working/dendPLRNN/BPTT_TF\n✅ Current directory: $(pwd)\n✅ Pandas and Epoch fixes applied.\nWed Nov 12 18:18:43 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n| N/A   36C    P0             27W /  250W |       0MiB /  16384MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# --- Cell 2 (Ablate MAR): Installs TSD + Noise ONLY ---\n\nimport os\nimport textwrap\n\n# --- 1. Define the \"Novelty\" Module Code ---\n# This code is identical to your original\nnovelty_code = \"\"\"\n\\\"\\\"\\\"\nMinimal, defensive novelty utilities for PLRNN training.\nKeeps logic isolated to avoid repeated in-place edits of big files.\n\\\"\\\"\\\"\n\nimport torch as tc\nimport torch.nn as nn\n\ndef inject_latent_noise(x, training=True, noise_std=0.02):\n    \\\"\\\"\\\"Add Gaussian noise to input tensor x if training is True.\\\"\\\"\\\"\n    if not training:\n        return x\n    try:\n        return x + tc.randn_like(x) * float(noise_std)\n    except Exception:\n        return x\n\ndef manifold_attractor_regularization(model, lambda_mar=0.01, frac=0.2):\n    \\\"\\\"\\\"\n    Minimal manifold-attractor regularizer.\n    - If model has attribute 'A' (tensor or parameter), compute small penalty.\n    - Returns a scalar tensor on same device; returns 0.0 tensor if not applicable.\n    \\\"\\\"\\\"\n    # safe guards\n    try:\n        if not hasattr(model, \"A\"):\n            return tc.tensor(0.0, device=next(model.parameters()).device)\n        A = getattr(model, \"A\")\n        # convert to tensor if Parameter\n        if isinstance(A, nn.Parameter):\n            A = A.data\n        A = tc.as_tensor(A)\n        device = A.device\n        W = getattr(model, \"W\", None)\n        h = getattr(model, \"h\", None)\n        M = A.shape[0]\n        Mreg = max(1, int(M * float(frac)))\n        A_diag = tc.diag(A)\n        mar_loss = tc.tensor(0.0, device=device)\n        for i in range(Mreg):\n            mar_loss = mar_loss + (A_diag[i] - 1.0) ** 2\n            if h is not None:\n                try:\n                    mar_loss = mar_loss + (tc.as_tensor(h[i]) ** 2)\n                except Exception:\n                    pass\n            if W is not None:\n                try:\n                    row = tc.as_tensor(W[i, :])\n                    mar_loss = mar_loss + (tc.sum(row ** 2) - row[i] ** 2)\n                except Exception:\n                    pass\n        return float(lambda_mar) * mar_loss\n    except Exception:\n        # On any unexpected issue, return zero tensor safely\n        try:\n            return tc.tensor(0.0, device=next(model.parameters()).device)\n        except Exception:\n            return tc.tensor(0.0)\n\ndef temporal_self_distillation_from_model(model, weight=0.05):\n    \\\"\\\"\\\"\n    Try to extract a latent trajectory from the model and compute MSE between adjacent timesteps.\n    Looks for common attributes (Z, last_z, z_hist). If none found, returns 0.\n    \\\"\\\"\\\"\n    try:\n        z = None\n        for attr in (\"Z\", \"z_hist\", \"last_z\", \"Z_hist\"):\n            if hasattr(model, attr):\n                z = getattr(model, attr)\n                break\n        # if z is a list or tuple, coerce to tensor if possible\n        if z is None:\n            return tc.tensor(0.0, device=next(model.parameters()).device)\n        \n        # Ensure z is a tensor\n        if not isinstance(z, tc.Tensor):\n            # Try to convert list of tensors, e.g., from a .cpu() loop\n            if isinstance(z, (list, tuple)) and len(z) > 1 and isinstance(z[0], tc.Tensor):\n                 zt = tc.stack(z)\n            else:\n                 zt = tc.as_tensor(z)\n        else:\n            zt = z\n\n        if zt.dim() < 2 or zt.shape[0] < 2:\n            return tc.tensor(0.0, device=zt.device)\n        diff = zt[1:] - zt[:-1].detach()\n        return float(weight) * tc.mean(diff ** 2)\n    except Exception:\n        try:\n            return tc.tensor(0.0, device=next(model.parameters()).device)\n        except Exception:\n            return tc.tensor(0.0)\n\ndef init_dendritic_if_possible(model, init_scale=0.05):\n    \\\"\\\"\\\"\n    If model has W and supports register_parameter, create and register 'U' parameter.\n    Safe no-op if not possible.\n    \\\"\\\"\\\"\n    try:\n        if hasattr(model, \"W\") and hasattr(model, \"register_parameter\") and not hasattr(model, \"U\"):\n            W = getattr(model, \"W\")\n            # create a parameter of same shape as W\n            U = nn.Parameter(tc.randn_like(tc.as_tensor(W)) * float(init_scale))\n            model.register_parameter(\"U\", U)\n            return True\n    except Exception:\n        pass\n    return False\n\ndef apply_dendritic_gate(z, model):\n    \\\"\\\"\\\"\n    Apply gating if model has parameter U; else return z unchanged.\n    \\\"\\\"\\\"\n    try:\n        if hasattr(model, \"U\"):\n            U = getattr(model, \"U\")\n            # Apply dendritic-style gating\n            return tc.sigmoid(tc.matmul(z, U)) * tc.relu(z)\n    except Exception:\n        pass\n    # Fallback to standard relu if no U or if error\n    return tc.relu(z)\n\"\"\"\n\n# --- 2. Define the Code to Inject (MAR ABLATED) ---\n# This block is modified to comment out the MAR section.\ninsertion_block = textwrap.dedent(\"\"\"\n    # --- novelties (TSD + safe noise) ---\n    \n    # --- MAR ABLATED ---\n    # try:\n    #     # manifold-attractor regularization (safe)\n    #     mar = novelties.manifold_attractor_regularization(self.model, lambda_mar=0.01)\n    #     if isinstance(mar, float) or (hasattr(mar, \"item\") and callable(getattr(mar, \"item\"))):\n    #         loss = loss + (mar if isinstance(mar, float) else mar)\n    # except Exception:\n    #     pass\n        \n    try:\n        # temporal self-distillation (if model exposes latent trajectory)\n        tsd = novelties.temporal_self_distillation_from_model(self.model, weight=0.05)\n        if hasattr(tsd, \"item\"):\n            loss = loss + tsd\n    except Exception:\n        pass\n    try:\n        # add an extra small noise via helper (this is optional and safe)\n        if \"inp\" in locals():\n            inp = novelties.inject_latent_noise(inp, training=self.model.training if hasattr(self.model, \"training\") else True, noise_level=getattr(self, \"noise_level\", 0.02))\n    except Exception:\n        pass\n\"\"\")\n\n# Define file paths\nnovelties_file = \"bptt/novelties.py\"\nalgo_file = \"bptt/bptt_algorithm.py\"\n\n# --- 3. Write the new novelties.py file ---\ntry:\n    with open(novelties_file, \"w\") as f:\n        f.write(novelty_code)\n    print(f\"✅ Successfully created {novelties_file}\")\n\n    # --- 4. Patch bptt_algorithm.py ---\n    \n    # Add the import at the top\n    with open(algo_file, \"r\") as f:\n        content = f.read()\n    \n    if \"from bptt import novelties\" not in content:\n        content = \"from bptt import novelties\\n\" + content\n        with open(algo_file, \"w\") as f:\n            f.write(content)\n        print(f\"✅ Added import to {algo_file}\")\n    \n    # Insert the loss block\n    with open(algo_file, \"r\") as f:\n        lines = f.readlines()\n\n    new_lines = []\n    inserted = False\n    target_line = \"loss = self.compute_loss(pred, target)\"\n\n    for line in lines:\n        new_lines.append(line)\n        # Check if this is the target line and we haven't inserted yet\n        if target_line in line and not inserted:\n            # Find the indentation of the target line\n            indentation = line[:len(line) - len(line.lstrip())]\n            # Add the insertion block with the same indentation\n            indented_insertion = \"\\n\".join([f\"{indentation}{l}\" for l in insertion_block.splitlines() if l])\n            new_lines.append(indented_insertion + \"\\n\")\n            inserted = True\n            print(f\"✅ Successfully inserted (Ablated-MAR) block into {algo_file}\")\n\n    if not inserted:\n        print(f\"⚠️ WARNING: Could not find target line '{target_line}' in {algo_file}. Patch failed.\")\n    else:\n        # Write the modified content back\n        with open(algo_file, \"w\") as f:\n            f.writelines(new_lines)\n        print(\"✅ File patching complete.\")\n\nexcept Exception as e:\n    print(f\"❌ An error occurred during file modification: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T18:18:43.160587Z","iopub.execute_input":"2025-11-12T18:18:43.160874Z","iopub.status.idle":"2025-11-12T18:18:43.173713Z","shell.execute_reply.started":"2025-11-12T18:18:43.160849Z","shell.execute_reply":"2025-11-12T18:18:43.172893Z"}},"outputs":[{"name":"stdout","text":"✅ Successfully created bptt/novelties.py\n✅ Added import to bptt/bptt_algorithm.py\n✅ Successfully inserted (Ablated-MAR) block into bptt/bptt_algorithm.py\n✅ File patching complete.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# --- Cell 3: Syntax Check and Run Training ---\n\n!echo \"Running python syntax check...\"\n!python -m py_compile bptt/novelties.py\n!python -m py_compile bptt/bptt_algorithm.py\n!echo \"✅ Syntax OK.\"\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T18:18:59.110594Z","iopub.execute_input":"2025-11-12T18:18:59.110888Z","iopub.status.idle":"2025-11-12T18:18:59.711870Z","shell.execute_reply.started":"2025-11-12T18:18:59.110865Z","shell.execute_reply":"2025-11-12T18:18:59.711153Z"}},"outputs":[{"name":"stdout","text":"Running python syntax check...\n✅ Syntax OK.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# --- GPU Diagnostic Cell ---\nimport torch\n\nprint(f\"PyTorch Version: {torch.__version__}\")\nprint(\"---\")\nprint(f\"Is CUDA (GPU) available? ==> {torch.cuda.is_available()}\")\nprint(\"---\")\n\nif torch.cuda.is_available():\n    print(f\"Current GPU Name: {torch.cuda.get_device_name(0)}\")\nelse:\n    print(\"WARNING: PyTorch cannot find the GPU.\")\n    print(\"Please go to 'Settings' on the right and ensure 'Accelerator' is set to 'GPU'.\")\n\nprint(\"\\n--- nvidia-smi (Hardware Check) ---\")\n!nvidia-smi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T18:19:08.384734Z","iopub.execute_input":"2025-11-12T18:19:08.385549Z","iopub.status.idle":"2025-11-12T18:19:11.814053Z","shell.execute_reply.started":"2025-11-12T18:19:08.385515Z","shell.execute_reply":"2025-11-12T18:19:11.813205Z"}},"outputs":[{"name":"stdout","text":"PyTorch Version: 2.6.0+cu124\n---\nIs CUDA (GPU) available? ==> True\n---\nCurrent GPU Name: Tesla P100-PCIE-16GB\n\n--- nvidia-smi (Hardware Check) ---\nWed Nov 12 18:19:11 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n| N/A   35C    P0             27W /  250W |       3MiB /  16384MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!grep -n \"numeric\" main_eval.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T18:19:37.588960Z","iopub.execute_input":"2025-11-12T18:19:37.590091Z","iopub.status.idle":"2025-11-12T18:19:37.719186Z","shell.execute_reply.started":"2025-11-12T18:19:37.590054Z","shell.execute_reply":"2025-11-12T18:19:37.718495Z"}},"outputs":[{"name":"stdout","text":"3:from pandas.api.types import is_numeric_dtype as numeric\n260:        mse5 = (df.mean(0, numeric_only=True)['5'], df.std(numeric_only=True)['5'])\n261:        mse10 = (df.mean(0, numeric_only=True)['10'], df.std(numeric_only=True)['10'])\n262:        mse20 = (df.mean(0, numeric_only=True)['20'], df.std(numeric_only=True)['20'])\n272:        pse = (df.mean(0, numeric_only=True)['mean'], df.std(numeric_only=True)['mean'])\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"%cd /kaggle/working/dendPLRNN/BPTT_TF/Experiments/Table1/ECG\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T18:19:46.058655Z","iopub.execute_input":"2025-11-12T18:19:46.059426Z","iopub.status.idle":"2025-11-12T18:19:46.064595Z","shell.execute_reply.started":"2025-11-12T18:19:46.059397Z","shell.execute_reply":"2025-11-12T18:19:46.063852Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/dendPLRNN/BPTT_TF/Experiments/Table1/ECG\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!sed -i \"s/Argument('use_gpu', \\[[0-9]\\+\\])/Argument('use_gpu', [1])/\" ubermain.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T18:19:55.911187Z","iopub.execute_input":"2025-11-12T18:19:55.911825Z","iopub.status.idle":"2025-11-12T18:19:56.038327Z","shell.execute_reply.started":"2025-11-12T18:19:55.911801Z","shell.execute_reply":"2025-11-12T18:19:56.037480Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"!grep -n \"use_gpu\" ubermain.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T18:20:04.038611Z","iopub.execute_input":"2025-11-12T18:20:04.038962Z","iopub.status.idle":"2025-11-12T18:20:04.166014Z","shell.execute_reply.started":"2025-11-12T18:20:04.038933Z","shell.execute_reply":"2025-11-12T18:20:04.165245Z"}},"outputs":[{"name":"stdout","text":"11:    When using GPU for training (i.e. Argument 'use_gpu 1')  it is generally\n17:    args.append(Argument('use_gpu', [1])) # may wanna use gpu here\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"!sed -i \"s/n_runs = [0-9]\\+/n_runs = 1/\" ubermain.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T18:20:13.228235Z","iopub.execute_input":"2025-11-12T18:20:13.228533Z","iopub.status.idle":"2025-11-12T18:20:13.356621Z","shell.execute_reply.started":"2025-11-12T18:20:13.228505Z","shell.execute_reply":"2025-11-12T18:20:13.355765Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"!grep -n \"n_runs\" ubermain.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T18:20:20.867534Z","iopub.execute_input":"2025-11-12T18:20:20.867847Z","iopub.status.idle":"2025-11-12T18:20:20.995153Z","shell.execute_reply.started":"2025-11-12T18:20:20.867818Z","shell.execute_reply":"2025-11-12T18:20:20.994397Z"}},"outputs":[{"name":"stdout","text":"4:def ubermain(n_runs):\n28:    args.append(Argument('run', list(range(1, 1 + n_runs))))\n36:    n_runs = 1\n42:    args = ubermain(n_runs)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# --- Configuration Cell: Set 250 Epochs & 50-Epoch Metrics ---\n\n# We must be in the experiment directory to edit the file\n%cd /kaggle/working/dendPLRNN/BPTT_TF/Experiments/Table1/ECG\n\nprint(\"--- Modifying n_epochs ---\")\n# 1. Change n_epochs from [10] (or whatever it is) to [250]\n!sed -i \"s/Argument('n_epochs', \\[[0-9]*\\])/Argument('n_epochs', [250])/\" ubermain.py\n# Verify the change\n!grep -n \"n_epochs\" ubermain.py\n\nprint(\"\\n--- Modifying test_interval (for metrics) ---\")\n# 2. Change test_interval from its default (e.g., 10) to [50]\n!sed -i \"s/Argument('test_interval', \\[[0-9]*\\])/Argument('test_interval', [50])/\" ubermain.py\n# Verify the change\n!grep -n \"test_interval\" ubermain.py\n\n# 3. Go back to the main BPTT_TF directory\n%cd /kaggle/working/dendPLRNN/BPTT_TF\n\nprint(\"\\n✅ Configuration complete. Ready to run final training.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T18:20:54.043696Z","iopub.execute_input":"2025-11-12T18:20:54.044372Z","iopub.status.idle":"2025-11-12T18:20:54.541037Z","shell.execute_reply.started":"2025-11-12T18:20:54.044333Z","shell.execute_reply":"2025-11-12T18:20:54.540168Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/dendPLRNN/BPTT_TF/Experiments/Table1/ECG\n--- Modifying n_epochs ---\n23:    args.append(Argument('n_epochs', [250]))\n\n--- Modifying test_interval (for metrics) ---\n/kaggle/working/dendPLRNN/BPTT_TF\n\n✅ Configuration complete. Ready to run final training.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# --- Cell 3 (FINAL EXPERIMENT): Run 250 Epochs ---\n\n!echo \"Running python syntax check...\"\n!python -m py_compile bptt/novelties.py\n!python -m py_compile bptt/bptt_algorithm.py\n!echo \"✅ Syntax OK.\"\n!echo \" \"\n\n!echo \"Launching FINAL 250-epoch training (metrics every 50 epochs)...\"\n\n# Get the full, absolute paths for PYTHONPATH\nROOT_DIR = \"/kaggle/working/dendPLRNN\"\nBPTT_DIR = \"/kaggle/working/dendPLRNN/BPTT_TF\"\nFULL_PYTHONPATH = f\"{BPTT_DIR}:{ROOT_DIR}\"\n\n# This command will now run your full experiment\n!PYTHONPATH=\"{FULL_PYTHONPATH}\" python -u Experiments/Table1/ECG/ubermain.py\n\n!echo \"--- Training finished ---\"\n!echo \" \"\n\n# Find and copy the latest model\n!bash -c 'LATEST_MODEL=$(find results -type f -name \"*.pt\" | sort | tail -n 1 || true); \\\nif [ -f \"$LATEST_MODEL\" ]; then \\\n  cp \"$LATEST_MODEL\" /kaggle/working/final_trained_model.pt; \\\n  echo \"✅ Saved model to /kaggle/working/final_trained_model.pt\"; \\\nelse \\\n  echo \"⚠️ No .pt checkpoint found in results/ — check the log above for errors.\"; \\\nfi'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T18:21:12.766230Z","iopub.execute_input":"2025-11-12T18:21:12.766536Z"}},"outputs":[{"name":"stdout","text":"Running python syntax check...\n✅ Syntax OK.\n \nLaunching FINAL 250-epoch training (metrics every 50 epochs)...\n'use_gpu' flag is set.\nWill distribute tasks to GPUs automatically.\nThere are not enough GPU Resources available to spawn 20 processes. Reducing number of parallel runs to 1\nUsing device Tesla P100-PCIE-16GB for training (cuda:0).\nForcing interval set by user: 10\nEpoch 1 took 15.77s | Cumulative time (h:mm:ss): 0:00:15 | Loss = 0.9451268315315247\nEpoch 2 took 15.27s | Cumulative time (h:mm:ss): 0:00:31 | Loss = 0.839176595211029\nEpoch 3 took 15.18s | Cumulative time (h:mm:ss): 0:00:46 | Loss = 0.8462734818458557\nEpoch 4 took 15.0s | Cumulative time (h:mm:ss): 0:01:01 | Loss = 0.8352178931236267\nEpoch 5 took 15.09s | Cumulative time (h:mm:ss): 0:01:16 | Loss = 0.7532029747962952\nEpoch 6 took 15.08s | Cumulative time (h:mm:ss): 0:01:31 | Loss = 0.7789081931114197\nEpoch 7 took 15.23s | Cumulative time (h:mm:ss): 0:01:46 | Loss = 0.7318127751350403\nEpoch 8 took 15.08s | Cumulative time (h:mm:ss): 0:02:01 | Loss = 0.7349398136138916\nEpoch 9 took 15.08s | Cumulative time (h:mm:ss): 0:02:16 | Loss = 0.613722026348114\nEpoch 10 took 15.08s | Cumulative time (h:mm:ss): 0:02:31 | Loss = 0.47659263014793396\nEpoch 11 took 15.19s | Cumulative time (h:mm:ss): 0:02:47 | Loss = 0.4451256990432739\nEpoch 12 took 15.47s | Cumulative time (h:mm:ss): 0:03:02 | Loss = 0.37149664759635925\nEpoch 13 took 15.17s | Cumulative time (h:mm:ss): 0:03:17 | Loss = 0.3282397985458374\nEpoch 14 took 15.14s | Cumulative time (h:mm:ss): 0:03:32 | Loss = 0.29617059230804443\nEpoch 15 took 15.34s | Cumulative time (h:mm:ss): 0:03:48 | Loss = 0.2477598339319229\nEpoch 16 took 15.15s | Cumulative time (h:mm:ss): 0:04:03 | Loss = 0.23495161533355713\nEpoch 17 took 15.1s | Cumulative time (h:mm:ss): 0:04:18 | Loss = 0.23206579685211182\nEpoch 18 took 15.18s | Cumulative time (h:mm:ss): 0:04:33 | Loss = 0.20104917883872986\nEpoch 19 took 15.13s | Cumulative time (h:mm:ss): 0:04:48 | Loss = 0.17062655091285706\nEpoch 20 took 15.18s | Cumulative time (h:mm:ss): 0:05:03 | Loss = 0.24667365849018097\nEpoch 21 took 15.21s | Cumulative time (h:mm:ss): 0:05:19 | Loss = 0.15381622314453125\nEpoch 22 took 15.24s | Cumulative time (h:mm:ss): 0:05:34 | Loss = 0.13872148096561432\nEpoch 23 took 15.28s | Cumulative time (h:mm:ss): 0:05:49 | Loss = 0.12807300686836243\nEpoch 24 took 15.2s | Cumulative time (h:mm:ss): 0:06:04 | Loss = 0.1385403275489807\nEpoch 25 took 15.22s | Cumulative time (h:mm:ss): 0:06:20 | Loss = 0.11994367092847824\n\tMSE-1 0.05063305050134659\n\tMSE-5 0.277794748544693\n\tMSE-10 0.5761833786964417\n\tMSE-20 0.7509123086929321\n20 20000\n\tPSE 0.9008486144744694\n\tPSE per dim [0.92030438859442, 0.8512224584756397, 0.8840030534245774, 0.9120246707091628, 0.923406222877368, 0.9076650676566924, 0.9073144395834248]\nComputing KLx-GMM\n\tKLx 1.1846829652786255\nEpoch 26 took 15.65s | Cumulative time (h:mm:ss): 0:06:35 | Loss = 0.11530749499797821\nEpoch 27 took 15.45s | Cumulative time (h:mm:ss): 0:06:51 | Loss = 0.12372195720672607\nEpoch 28 took 15.35s | Cumulative time (h:mm:ss): 0:07:06 | Loss = 0.12008707970380783\nEpoch 29 took 15.52s | Cumulative time (h:mm:ss): 0:07:22 | Loss = 0.13515736162662506\nEpoch 30 took 15.39s | Cumulative time (h:mm:ss): 0:07:37 | Loss = 0.10779925435781479\nEpoch 31 took 15.44s | Cumulative time (h:mm:ss): 0:07:52 | Loss = 0.13757076859474182\nEpoch 32 took 15.55s | Cumulative time (h:mm:ss): 0:08:08 | Loss = 0.11258958280086517\nEpoch 33 took 15.46s | Cumulative time (h:mm:ss): 0:08:23 | Loss = 0.11513688415288925\nEpoch 34 took 15.25s | Cumulative time (h:mm:ss): 0:08:39 | Loss = 0.12255597859621048\nEpoch 35 took 15.42s | Cumulative time (h:mm:ss): 0:08:54 | Loss = 0.11498001217842102\nEpoch 36 took 15.37s | Cumulative time (h:mm:ss): 0:09:09 | Loss = 0.11099536716938019\nEpoch 37 took 15.36s | Cumulative time (h:mm:ss): 0:09:25 | Loss = 0.10988549888134003\nEpoch 38 took 15.5s | Cumulative time (h:mm:ss): 0:09:40 | Loss = 0.10817556828260422\nEpoch 39 took 15.39s | Cumulative time (h:mm:ss): 0:09:56 | Loss = 0.10132589191198349\nEpoch 40 took 15.3s | Cumulative time (h:mm:ss): 0:10:11 | Loss = 0.10227721929550171\nEpoch 41 took 15.41s | Cumulative time (h:mm:ss): 0:10:26 | Loss = 0.10780595988035202\nEpoch 42 took 15.51s | Cumulative time (h:mm:ss): 0:10:42 | Loss = 0.12342691421508789\nEpoch 43 took 15.27s | Cumulative time (h:mm:ss): 0:10:57 | Loss = 0.11420503258705139\nEpoch 44 took 15.25s | Cumulative time (h:mm:ss): 0:11:12 | Loss = 0.10542874783277512\nEpoch 45 took 15.16s | Cumulative time (h:mm:ss): 0:11:28 | Loss = 0.10753787308931351\nEpoch 46 took 15.19s | Cumulative time (h:mm:ss): 0:11:43 | Loss = 0.10265035182237625\nEpoch 47 took 15.26s | Cumulative time (h:mm:ss): 0:11:58 | Loss = 0.10032521933317184\nEpoch 48 took 15.31s | Cumulative time (h:mm:ss): 0:12:13 | Loss = 0.10813771933317184\nEpoch 49 took 15.27s | Cumulative time (h:mm:ss): 0:12:29 | Loss = 0.10608009248971939\nEpoch 50 took 15.28s | Cumulative time (h:mm:ss): 0:12:44 | Loss = 0.10210757702589035\n\tMSE-1 0.05350413918495178\n\tMSE-5 0.27792295813560486\n\tMSE-10 0.5637707710266113\n\tMSE-20 0.7240349650382996\n20 20000\n\tPSE 0.8792780955181249\n\tPSE per dim [0.9023624583752539, 0.8677431436444947, 0.853547761235847, 0.8819792419668828, 0.9130797180455597, 0.8749675152976039, 0.8612668300612327]\nComputing KLx-GMM\n\tKLx 1.512620449066162\nEpoch 51 took 15.31s | Cumulative time (h:mm:ss): 0:12:59 | Loss = 0.09887602925300598\nEpoch 52 took 15.24s | Cumulative time (h:mm:ss): 0:13:14 | Loss = 0.10278184711933136\nEpoch 53 took 15.33s | Cumulative time (h:mm:ss): 0:13:30 | Loss = 0.09912000596523285\nEpoch 54 took 15.19s | Cumulative time (h:mm:ss): 0:13:45 | Loss = 0.11189013719558716\nEpoch 55 took 15.26s | Cumulative time (h:mm:ss): 0:14:00 | Loss = 0.10417843610048294\nEpoch 56 took 15.22s | Cumulative time (h:mm:ss): 0:14:15 | Loss = 0.1052212044596672\nEpoch 57 took 15.17s | Cumulative time (h:mm:ss): 0:14:31 | Loss = 0.08638037741184235\nEpoch 58 took 15.16s | Cumulative time (h:mm:ss): 0:14:46 | Loss = 0.11074496805667877\nEpoch 59 took 15.25s | Cumulative time (h:mm:ss): 0:15:01 | Loss = 0.09284256398677826\nEpoch 60 took 15.21s | Cumulative time (h:mm:ss): 0:15:16 | Loss = 0.09123671054840088\nEpoch 61 took 15.21s | Cumulative time (h:mm:ss): 0:15:31 | Loss = 0.0894756019115448\nEpoch 62 took 15.19s | Cumulative time (h:mm:ss): 0:15:47 | Loss = 0.0817360058426857\nEpoch 63 took 15.22s | Cumulative time (h:mm:ss): 0:16:02 | Loss = 0.08586671203374863\nEpoch 64 took 15.21s | Cumulative time (h:mm:ss): 0:16:17 | Loss = 0.0825323835015297\nEpoch 65 took 15.23s | Cumulative time (h:mm:ss): 0:16:32 | Loss = 0.08007887750864029\nEpoch 66 took 15.42s | Cumulative time (h:mm:ss): 0:16:48 | Loss = 0.10442259162664413\nEpoch 67 took 15.77s | Cumulative time (h:mm:ss): 0:17:03 | Loss = 0.09038085490465164\nEpoch 68 took 15.38s | Cumulative time (h:mm:ss): 0:17:19 | Loss = 0.088381327688694\nEpoch 69 took 15.5s | Cumulative time (h:mm:ss): 0:17:34 | Loss = 0.09774145483970642\nEpoch 70 took 15.71s | Cumulative time (h:mm:ss): 0:17:50 | Loss = 0.08286138623952866\nEpoch 71 took 15.47s | Cumulative time (h:mm:ss): 0:18:06 | Loss = 0.08279599249362946\nEpoch 72 took 15.4s | Cumulative time (h:mm:ss): 0:18:21 | Loss = 0.08774681389331818\nEpoch 73 took 15.26s | Cumulative time (h:mm:ss): 0:18:36 | Loss = 0.08271252363920212\nEpoch 74 took 15.28s | Cumulative time (h:mm:ss): 0:18:51 | Loss = 0.0925518274307251\nEpoch 75 took 15.44s | Cumulative time (h:mm:ss): 0:19:07 | Loss = 0.09287801384925842\n\tMSE-1 0.05356449633836746\n\tMSE-5 0.2793479561805725\n\tMSE-10 0.5593512058258057\n\tMSE-20 0.7871642112731934\n20 20000\n\tPSE 0.8113079684328878\n\tPSE per dim [0.8249072034046867, 0.9085637480252889, 0.7543957777827838, 0.7465169136654094, 0.8274902499544503, 0.8900063351362208, 0.7272755510613753]\nComputing KLx-GMM\n\tKLx 0.6368202567100525\nEpoch 76 took 15.38s | Cumulative time (h:mm:ss): 0:19:22 | Loss = 0.07972647249698639\nEpoch 77 took 15.24s | Cumulative time (h:mm:ss): 0:19:38 | Loss = 0.0829489678144455\nEpoch 78 took 14.99s | Cumulative time (h:mm:ss): 0:19:53 | Loss = 0.08165889978408813\nEpoch 79 took 15.36s | Cumulative time (h:mm:ss): 0:20:08 | Loss = 0.08814441412687302\nEpoch 80 took 15.12s | Cumulative time (h:mm:ss): 0:20:23 | Loss = 0.07814960181713104\nEpoch 81 took 15.12s | Cumulative time (h:mm:ss): 0:20:38 | Loss = 0.08237891644239426\nEpoch 82 took 15.05s | Cumulative time (h:mm:ss): 0:20:53 | Loss = 0.0823172926902771\nEpoch 83 took 15.19s | Cumulative time (h:mm:ss): 0:21:08 | Loss = 0.07885999232530594\nEpoch 84 took 15.35s | Cumulative time (h:mm:ss): 0:21:24 | Loss = 0.07633399963378906\nEpoch 85 took 15.19s | Cumulative time (h:mm:ss): 0:21:39 | Loss = 0.07435890287160873\nEpoch 86 took 15.18s | Cumulative time (h:mm:ss): 0:21:54 | Loss = 0.08848505467176437\nEpoch 87 took 15.09s | Cumulative time (h:mm:ss): 0:22:09 | Loss = 0.0803057849407196\nEpoch 88 took 15.13s | Cumulative time (h:mm:ss): 0:22:24 | Loss = 0.06978902965784073\nEpoch 89 took 15.07s | Cumulative time (h:mm:ss): 0:22:39 | Loss = 0.06847772747278214\nEpoch 90 took 15.14s | Cumulative time (h:mm:ss): 0:22:54 | Loss = 0.08359738439321518\nEpoch 91 took 15.0s | Cumulative time (h:mm:ss): 0:23:09 | Loss = 0.08242509514093399\nEpoch 92 took 15.06s | Cumulative time (h:mm:ss): 0:23:25 | Loss = 0.07835210114717484\nEpoch 93 took 15.04s | Cumulative time (h:mm:ss): 0:23:40 | Loss = 0.07187729328870773\nEpoch 94 took 15.49s | Cumulative time (h:mm:ss): 0:23:55 | Loss = 0.07285725325345993\nEpoch 95 took 15.07s | Cumulative time (h:mm:ss): 0:24:10 | Loss = 0.06926801055669785\nEpoch 96 took 15.22s | Cumulative time (h:mm:ss): 0:24:25 | Loss = 0.06594761461019516\nEpoch 97 took 15.26s | Cumulative time (h:mm:ss): 0:24:41 | Loss = 0.0637708380818367\nEpoch 98 took 15.17s | Cumulative time (h:mm:ss): 0:24:56 | Loss = 0.06504392623901367\nEpoch 99 took 15.05s | Cumulative time (h:mm:ss): 0:25:11 | Loss = 0.071653813123703\nEpoch 100 took 15.19s | Cumulative time (h:mm:ss): 0:25:26 | Loss = 0.0777263343334198\n\tMSE-1 0.053619351238012314\n\tMSE-5 0.2731863260269165\n\tMSE-10 0.5385770797729492\n\tMSE-20 0.7842056155204773\n20 20000\n\tPSE 0.904260761454783\n\tPSE per dim [0.9152200121325681, 0.9449248848954639, 0.8650623289924209, 0.8736033405128877, 0.9179471255758681, 0.9512393047472127, 0.8618283333270601]\nComputing KLx-GMM\n\tKLx 0.9494787454605103\nEpoch 101 took 15.52s | Cumulative time (h:mm:ss): 0:25:42 | Loss = 0.08804924786090851\nEpoch 102 took 15.26s | Cumulative time (h:mm:ss): 0:25:57 | Loss = 0.07565084099769592\nEpoch 103 took 15.17s | Cumulative time (h:mm:ss): 0:26:12 | Loss = 0.06820700317621231\nEpoch 104 took 15.1s | Cumulative time (h:mm:ss): 0:26:27 | Loss = 0.08018451929092407\nEpoch 105 took 15.19s | Cumulative time (h:mm:ss): 0:26:42 | Loss = 0.07064598798751831\nEpoch 106 took 15.06s | Cumulative time (h:mm:ss): 0:26:57 | Loss = 0.05733541399240494\nEpoch 107 took 15.15s | Cumulative time (h:mm:ss): 0:27:12 | Loss = 0.07074397057294846\nEpoch 108 took 15.09s | Cumulative time (h:mm:ss): 0:27:28 | Loss = 0.06568784266710281\nEpoch 109 took 15.19s | Cumulative time (h:mm:ss): 0:27:43 | Loss = 0.07345413416624069\nEpoch 110 took 14.98s | Cumulative time (h:mm:ss): 0:27:58 | Loss = 0.06597425043582916\nEpoch 111 took 15.11s | Cumulative time (h:mm:ss): 0:28:13 | Loss = 0.07022114098072052\nEpoch 112 took 15.06s | Cumulative time (h:mm:ss): 0:28:28 | Loss = 0.06981267035007477\nEpoch 113 took 15.15s | Cumulative time (h:mm:ss): 0:28:43 | Loss = 0.06554808467626572\nEpoch 114 took 15.07s | Cumulative time (h:mm:ss): 0:28:58 | Loss = 0.06752299517393112\nEpoch 115 took 15.05s | Cumulative time (h:mm:ss): 0:29:13 | Loss = 0.06981800496578217\nEpoch 116 took 14.96s | Cumulative time (h:mm:ss): 0:29:28 | Loss = 0.06252442300319672\nEpoch 117 took 15.45s | Cumulative time (h:mm:ss): 0:29:44 | Loss = 0.0645531490445137\nEpoch 118 took 15.38s | Cumulative time (h:mm:ss): 0:29:59 | Loss = 0.06299924850463867\nEpoch 119 took 15.17s | Cumulative time (h:mm:ss): 0:30:14 | Loss = 0.057895079255104065\nEpoch 120 took 15.03s | Cumulative time (h:mm:ss): 0:30:29 | Loss = 0.06319841742515564\nEpoch 121 took 15.12s | Cumulative time (h:mm:ss): 0:30:44 | Loss = 0.06272555887699127\nEpoch 122 took 15.13s | Cumulative time (h:mm:ss): 0:30:59 | Loss = 0.05893488973379135\nEpoch 123 took 15.22s | Cumulative time (h:mm:ss): 0:31:15 | Loss = 0.06338364630937576\nEpoch 124 took 15.1s | Cumulative time (h:mm:ss): 0:31:30 | Loss = 0.060100968927145004\nEpoch 125 took 15.15s | Cumulative time (h:mm:ss): 0:31:45 | Loss = 0.05654115602374077\n\tMSE-1 0.05453094467520714\n\tMSE-5 0.259213924407959\n\tMSE-10 0.5075868964195251\n\tMSE-20 0.7289695143699646\n20 20000\n\tPSE 0.9196135346557737\n\tPSE per dim [0.9300378428166455, 0.9560414411677, 0.8936723352903959, 0.8955246760489233, 0.9407108042172975, 0.9539302240767076, 0.8673774189727455]\nComputing KLx-GMM\n\tKLx 1.2929489612579346\nEpoch 126 took 15.39s | Cumulative time (h:mm:ss): 0:32:00 | Loss = 0.06262611597776413\nEpoch 127 took 15.5s | Cumulative time (h:mm:ss): 0:32:16 | Loss = 0.06045827642083168\nEpoch 128 took 15.53s | Cumulative time (h:mm:ss): 0:32:31 | Loss = 0.06018094718456268\nEpoch 129 took 15.48s | Cumulative time (h:mm:ss): 0:32:47 | Loss = 0.06603050976991653\nEpoch 130 took 15.28s | Cumulative time (h:mm:ss): 0:33:02 | Loss = 0.07017137855291367\nEpoch 131 took 15.28s | Cumulative time (h:mm:ss): 0:33:17 | Loss = 0.05948406830430031\nEpoch 132 took 15.29s | Cumulative time (h:mm:ss): 0:33:33 | Loss = 0.05530001223087311\nEpoch 133 took 15.22s | Cumulative time (h:mm:ss): 0:33:48 | Loss = 0.06138840690255165\nEpoch 134 took 15.28s | Cumulative time (h:mm:ss): 0:34:03 | Loss = 0.06603812426328659\nEpoch 135 took 15.18s | Cumulative time (h:mm:ss): 0:34:18 | Loss = 0.05694977566599846\nEpoch 136 took 15.11s | Cumulative time (h:mm:ss): 0:34:33 | Loss = 0.06152418628334999\nEpoch 137 took 15.43s | Cumulative time (h:mm:ss): 0:34:49 | Loss = 0.05935840308666229\nEpoch 138 took 15.61s | Cumulative time (h:mm:ss): 0:35:04 | Loss = 0.06053550913929939\nEpoch 139 took 15.36s | Cumulative time (h:mm:ss): 0:35:20 | Loss = 0.057013139128685\nEpoch 140 took 15.3s | Cumulative time (h:mm:ss): 0:35:35 | Loss = 0.06887024641036987\nEpoch 141 took 15.15s | Cumulative time (h:mm:ss): 0:35:50 | Loss = 0.06786111742258072\nEpoch 142 took 15.11s | Cumulative time (h:mm:ss): 0:36:05 | Loss = 0.05551128834486008\nEpoch 143 took 15.2s | Cumulative time (h:mm:ss): 0:36:21 | Loss = 0.05779711902141571\nEpoch 144 took 15.38s | Cumulative time (h:mm:ss): 0:36:36 | Loss = 0.053359661251306534\nEpoch 145 took 15.23s | Cumulative time (h:mm:ss): 0:36:51 | Loss = 0.05899598449468613\nEpoch 146 took 15.19s | Cumulative time (h:mm:ss): 0:37:06 | Loss = 0.05355038121342659\nEpoch 147 took 15.26s | Cumulative time (h:mm:ss): 0:37:22 | Loss = 0.06095069274306297\nEpoch 148 took 15.19s | Cumulative time (h:mm:ss): 0:37:37 | Loss = 0.04920288175344467\nEpoch 149 took 15.12s | Cumulative time (h:mm:ss): 0:37:52 | Loss = 0.05868964269757271\nEpoch 150 took 15.04s | Cumulative time (h:mm:ss): 0:38:07 | Loss = 0.05748731270432472\n\tMSE-1 0.055192653089761734\n\tMSE-5 0.2500421702861786\n\tMSE-10 0.4909164011478424\n\tMSE-20 0.7040239572525024\n20 20000\n\tPSE 0.8453630252475309\n\tPSE per dim [0.8409976094920251, 0.8976125210342498, 0.7864502406584383, 0.8240289644579306, 0.8690885307548765, 0.9088841221152042, 0.790479188219993]\nComputing KLx-GMM\n\tKLx 0.8204079866409302\nEpoch 151 took 15.15s | Cumulative time (h:mm:ss): 0:38:22 | Loss = 0.06267174333333969\nEpoch 152 took 15.09s | Cumulative time (h:mm:ss): 0:38:37 | Loss = 0.053291138261556625\nEpoch 153 took 15.12s | Cumulative time (h:mm:ss): 0:38:52 | Loss = 0.05424124374985695\nEpoch 154 took 15.42s | Cumulative time (h:mm:ss): 0:39:08 | Loss = 0.05554657801985741\nEpoch 155 took 15.21s | Cumulative time (h:mm:ss): 0:39:23 | Loss = 0.060549743473529816\nEpoch 156 took 15.22s | Cumulative time (h:mm:ss): 0:39:38 | Loss = 0.05072870850563049\nEpoch 157 took 15.1s | Cumulative time (h:mm:ss): 0:39:53 | Loss = 0.058940500020980835\nEpoch 158 took 15.52s | Cumulative time (h:mm:ss): 0:40:09 | Loss = 0.06158551946282387\nEpoch 159 took 15.49s | Cumulative time (h:mm:ss): 0:40:24 | Loss = 0.06989601999521255\nEpoch 160 took 15.17s | Cumulative time (h:mm:ss): 0:40:39 | Loss = 0.050406306982040405\nEpoch 161 took 15.14s | Cumulative time (h:mm:ss): 0:40:55 | Loss = 0.052825503051280975\nEpoch 162 took 15.14s | Cumulative time (h:mm:ss): 0:41:10 | Loss = 0.05271847918629646\nEpoch 163 took 15.08s | Cumulative time (h:mm:ss): 0:41:25 | Loss = 0.05392998829483986\nEpoch 164 took 15.17s | Cumulative time (h:mm:ss): 0:41:40 | Loss = 0.053321473300457\nEpoch 165 took 15.03s | Cumulative time (h:mm:ss): 0:41:55 | Loss = 0.04925002530217171\nEpoch 166 took 15.21s | Cumulative time (h:mm:ss): 0:42:10 | Loss = 0.04492270201444626\nEpoch 167 took 15.27s | Cumulative time (h:mm:ss): 0:42:26 | Loss = 0.051897093653678894\nEpoch 168 took 15.23s | Cumulative time (h:mm:ss): 0:42:41 | Loss = 0.050920870155096054\nEpoch 169 took 15.53s | Cumulative time (h:mm:ss): 0:42:56 | Loss = 0.053157538175582886\nEpoch 170 took 15.41s | Cumulative time (h:mm:ss): 0:43:12 | Loss = 0.04241715744137764\nEpoch 171 took 15.3s | Cumulative time (h:mm:ss): 0:43:27 | Loss = 0.04586294665932655\nEpoch 172 took 15.32s | Cumulative time (h:mm:ss): 0:43:42 | Loss = 0.057755447924137115\nEpoch 173 took 15.26s | Cumulative time (h:mm:ss): 0:43:58 | Loss = 0.042191099375486374\nEpoch 174 took 15.26s | Cumulative time (h:mm:ss): 0:44:13 | Loss = 0.05139237269759178\nEpoch 175 took 15.21s | Cumulative time (h:mm:ss): 0:44:28 | Loss = 0.053871769458055496\n\tMSE-1 0.05600501969456673\n\tMSE-5 0.2455807477235794\n\tMSE-10 0.48491641879081726\n\tMSE-20 0.7048516273498535\n20 20000\n\tPSE 0.938774925381721\n\tPSE per dim [0.9284346852422466, 0.9596201502734449, 0.9326395136901765, 0.9362670665235547, 0.9372096549921433, 0.952608037639791, 0.9246453693106897]\nComputing KLx-GMM\n\tKLx 0.6952466368675232\nEpoch 176 took 15.73s | Cumulative time (h:mm:ss): 0:44:44 | Loss = 0.050782088190317154\nEpoch 177 took 15.64s | Cumulative time (h:mm:ss): 0:44:59 | Loss = 0.050800301134586334\nEpoch 178 took 15.45s | Cumulative time (h:mm:ss): 0:45:15 | Loss = 0.048258014023303986\nEpoch 179 took 15.77s | Cumulative time (h:mm:ss): 0:45:31 | Loss = 0.04930886626243591\nEpoch 180 took 16.39s | Cumulative time (h:mm:ss): 0:45:47 | Loss = 0.04342294856905937\nEpoch 181 took 15.91s | Cumulative time (h:mm:ss): 0:46:03 | Loss = 0.050278615206480026\nEpoch 182 took 15.27s | Cumulative time (h:mm:ss): 0:46:18 | Loss = 0.04814589023590088\nEpoch 183 took 15.24s | Cumulative time (h:mm:ss): 0:46:33 | Loss = 0.050873734056949615\nEpoch 184 took 15.17s | Cumulative time (h:mm:ss): 0:46:49 | Loss = 0.05288812890648842\nEpoch 185 took 15.27s | Cumulative time (h:mm:ss): 0:47:04 | Loss = 0.04341280460357666\nEpoch 186 took 15.31s | Cumulative time (h:mm:ss): 0:47:19 | Loss = 0.05652591213583946\nEpoch 187 took 15.28s | Cumulative time (h:mm:ss): 0:47:35 | Loss = 0.04226566106081009\nEpoch 188 took 15.29s | Cumulative time (h:mm:ss): 0:47:50 | Loss = 0.04719451069831848\nEpoch 189 took 15.34s | Cumulative time (h:mm:ss): 0:48:05 | Loss = 0.05408424511551857\nEpoch 190 took 15.22s | Cumulative time (h:mm:ss): 0:48:20 | Loss = 0.042542118579149246\nEpoch 191 took 15.35s | Cumulative time (h:mm:ss): 0:48:36 | Loss = 0.05736294761300087\nEpoch 192 took 15.58s | Cumulative time (h:mm:ss): 0:48:51 | Loss = 0.04529885947704315\nEpoch 193 took 15.24s | Cumulative time (h:mm:ss): 0:49:07 | Loss = 0.0486779622733593\nEpoch 194 took 15.37s | Cumulative time (h:mm:ss): 0:49:22 | Loss = 0.046574562788009644\nEpoch 195 took 15.54s | Cumulative time (h:mm:ss): 0:49:37 | Loss = 0.05341501161456108\nEpoch 196 took 15.3s | Cumulative time (h:mm:ss): 0:49:53 | Loss = 0.04617783799767494\nEpoch 197 took 15.38s | Cumulative time (h:mm:ss): 0:50:08 | Loss = 0.04675730690360069\nEpoch 198 took 15.38s | Cumulative time (h:mm:ss): 0:50:24 | Loss = 0.040098339319229126\nEpoch 199 took 15.51s | Cumulative time (h:mm:ss): 0:50:39 | Loss = 0.04837656393647194\nEpoch 200 took 15.54s | Cumulative time (h:mm:ss): 0:50:55 | Loss = 0.045313335955142975\n\tMSE-1 0.05650090053677559\n\tMSE-5 0.24039135873317719\n\tMSE-10 0.4656532108783722\n\tMSE-20 0.6446946859359741\n20 20000\n\tPSE 0.8442447856895151\n\tPSE per dim [0.8275517604794962, 0.8968195661868421, 0.8281144187443565, 0.818707867174217, 0.8699611686549139, 0.8951819432125228, 0.7733767753742572]\nComputing KLx-GMM\n\tKLx 1.028470754623413\nEpoch 201 took 15.27s | Cumulative time (h:mm:ss): 0:51:10 | Loss = 0.04137390851974487\nEpoch 202 took 15.38s | Cumulative time (h:mm:ss): 0:51:25 | Loss = 0.042120836675167084\nEpoch 203 took 15.39s | Cumulative time (h:mm:ss): 0:51:41 | Loss = 0.04472115635871887\nEpoch 204 took 15.26s | Cumulative time (h:mm:ss): 0:51:56 | Loss = 0.04258308932185173\nEpoch 205 took 15.25s | Cumulative time (h:mm:ss): 0:52:11 | Loss = 0.04012923687696457\nEpoch 206 took 15.28s | Cumulative time (h:mm:ss): 0:52:26 | Loss = 0.04883819818496704\nEpoch 207 took 15.39s | Cumulative time (h:mm:ss): 0:52:42 | Loss = 0.040795013308525085\nEpoch 208 took 15.34s | Cumulative time (h:mm:ss): 0:52:57 | Loss = 0.04374338313937187\nEpoch 209 took 15.21s | Cumulative time (h:mm:ss): 0:53:12 | Loss = 0.046994537115097046\nEpoch 210 took 15.1s | Cumulative time (h:mm:ss): 0:53:27 | Loss = 0.0413256473839283\nEpoch 211 took 15.14s | Cumulative time (h:mm:ss): 0:53:43 | Loss = 0.052308253943920135\nEpoch 212 took 15.11s | Cumulative time (h:mm:ss): 0:53:58 | Loss = 0.042105913162231445\nEpoch 213 took 15.28s | Cumulative time (h:mm:ss): 0:54:13 | Loss = 0.04077381268143654\nEpoch 214 took 15.49s | Cumulative time (h:mm:ss): 0:54:28 | Loss = 0.04410818591713905\nEpoch 215 took 15.08s | Cumulative time (h:mm:ss): 0:54:44 | Loss = 0.04260167479515076\nEpoch 216 took 15.11s | Cumulative time (h:mm:ss): 0:54:59 | Loss = 0.039374999701976776\nEpoch 217 took 15.11s | Cumulative time (h:mm:ss): 0:55:14 | Loss = 0.04148293659090996\nEpoch 218 took 15.28s | Cumulative time (h:mm:ss): 0:55:29 | Loss = 0.04305875301361084\nEpoch 219 took 15.21s | Cumulative time (h:mm:ss): 0:55:44 | Loss = 0.04018215090036392\nEpoch 220 took 15.21s | Cumulative time (h:mm:ss): 0:55:59 | Loss = 0.04375537112355232\nEpoch 221 took 15.13s | Cumulative time (h:mm:ss): 0:56:15 | Loss = 0.043488312512636185\nEpoch 222 took 15.4s | Cumulative time (h:mm:ss): 0:56:30 | Loss = 0.040026478469371796\nEpoch 223 took 15.45s | Cumulative time (h:mm:ss): 0:56:45 | Loss = 0.045588087290525436\nEpoch 224 took 15.24s | Cumulative time (h:mm:ss): 0:57:01 | Loss = 0.0429881326854229\nEpoch 225 took 15.08s | Cumulative time (h:mm:ss): 0:57:16 | Loss = 0.044279515743255615\n\tMSE-1 0.05694291740655899\n\tMSE-5 0.23967623710632324\n\tMSE-10 0.4656597971916199\n\tMSE-20 0.6428257822990417\n20 20000\n\tPSE 0.9185488013777532\n\tPSE per dim [0.8994081454177728, 0.9401725211051241, 0.9212059882562821, 0.8857655356498048, 0.9265934149479983, 0.9407305236320846, 0.9159654806352059]\nComputing KLx-GMM\n\tKLx 0.7507379055023193\nEpoch 226 took 15.17s | Cumulative time (h:mm:ss): 0:57:31 | Loss = 0.04874978959560394\nEpoch 227 took 15.13s | Cumulative time (h:mm:ss): 0:57:46 | Loss = 0.04879267141222954\nEpoch 228 took 15.23s | Cumulative time (h:mm:ss): 0:58:01 | Loss = 0.04425373300909996\nEpoch 229 took 15.22s | Cumulative time (h:mm:ss): 0:58:16 | Loss = 0.05000592768192291\nEpoch 230 took 15.19s | Cumulative time (h:mm:ss): 0:58:32 | Loss = 0.04268930107355118\nEpoch 231 took 15.27s | Cumulative time (h:mm:ss): 0:58:47 | Loss = 0.04403780773282051\nEpoch 232 took 15.23s | Cumulative time (h:mm:ss): 0:59:02 | Loss = 0.04127148538827896\nEpoch 233 took 15.21s | Cumulative time (h:mm:ss): 0:59:17 | Loss = 0.039752665907144547\nEpoch 234 took 15.22s | Cumulative time (h:mm:ss): 0:59:33 | Loss = 0.043838802725076675\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}