{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!rm -rf /kaggle/working/dendPLRNN\n!git clone --depth 1 https://github.com/DurstewitzLab/dendPLRNN.git /kaggle/working/dendPLRNN\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:14:07.664182Z","iopub.execute_input":"2025-11-12T12:14:07.664568Z","iopub.status.idle":"2025-11-12T12:14:11.025016Z","shell.execute_reply.started":"2025-11-12T12:14:07.664525Z","shell.execute_reply":"2025-11-12T12:14:11.024189Z"}},"outputs":[{"name":"stdout","text":"Cloning into '/kaggle/working/dendPLRNN'...\nremote: Enumerating objects: 137, done.\u001b[K\nremote: Counting objects: 100% (137/137), done.\u001b[K\nremote: Compressing objects: 100% (117/117), done.\u001b[K\nremote: Total 137 (delta 27), reused 122 (delta 19), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (137/137), 60.51 MiB | 45.86 MiB/s, done.\nResolving deltas: 100% (27/27), done.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install numpy scipy scikit-learn matplotlib tensorboardX\n!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:14:11.026433Z","iopub.execute_input":"2025-11-12T12:14:11.026689Z","iopub.status.idle":"2025-11-12T12:15:32.651260Z","shell.execute_reply.started":"2025-11-12T12:14:11.026664Z","shell.execute_reply":"2025-11-12T12:15:32.650479Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.3)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.2)\nCollecting tensorboardX\n  Downloading tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.3.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (6.33.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\nDownloading tensorboardx-2.6.4-py3-none-any.whl (87 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: tensorboardX\nSuccessfully installed tensorboardX-2.6.4\nLooking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu113\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\nRequirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.20.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.15.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.10.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nprint(torch.cuda.is_available(), torch.cuda.get_device_name(0))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:15:32.652423Z","iopub.execute_input":"2025-11-12T12:15:32.653208Z","iopub.status.idle":"2025-11-12T12:15:35.696785Z","shell.execute_reply.started":"2025-11-12T12:15:32.653166Z","shell.execute_reply":"2025-11-12T12:15:35.696086Z"}},"outputs":[{"name":"stdout","text":"True Tesla P100-PCIE-16GB\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Ensure Python can find project modules\nimport sys\nsys.path.insert(0, \"/kaggle/working/dendPLRNN/BPTT_TF\")\nsys.path.insert(0, \"/kaggle/working/dendPLRNN\")\nprint(\"PYTHONPATH ready.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:15:35.698136Z","iopub.execute_input":"2025-11-12T12:15:35.698495Z","iopub.status.idle":"2025-11-12T12:15:35.702754Z","shell.execute_reply.started":"2025-11-12T12:15:35.698448Z","shell.execute_reply":"2025-11-12T12:15:35.701982Z"}},"outputs":[{"name":"stdout","text":"PYTHONPATH ready.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"%cd /kaggle/working/dendPLRNN/BPTT_TF\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:15:35.703401Z","iopub.execute_input":"2025-11-12T12:15:35.703632Z","iopub.status.idle":"2025-11-12T12:15:35.720137Z","shell.execute_reply.started":"2025-11-12T12:15:35.703616Z","shell.execute_reply":"2025-11-12T12:15:35.719583Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/dendPLRNN/BPTT_TF\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"%cd /kaggle/working/dendPLRNN/BPTT_TF\n\n# Replace the outdated pandas import inside main_eval.py\n!sed -i \"s/from pandas.core.indexes import numeric/from pandas.api.types import is_numeric_dtype as numeric/\" main_eval.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:15:35.720857Z","iopub.execute_input":"2025-11-12T12:15:35.721054Z","iopub.status.idle":"2025-11-12T12:15:35.858023Z","shell.execute_reply.started":"2025-11-12T12:15:35.721038Z","shell.execute_reply":"2025-11-12T12:15:35.857224Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/dendPLRNN/BPTT_TF\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!sed -i \"s/Argument('n_epochs', \\[[0-9]*\\])/Argument('n_epochs', [10])/\" \\\n/kaggle/working/dendPLRNN/BPTT_TF/Experiments/Table1/ECG/ubermain.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:15:35.859037Z","iopub.execute_input":"2025-11-12T12:15:35.859278Z","iopub.status.idle":"2025-11-12T12:15:35.989724Z","shell.execute_reply.started":"2025-11-12T12:15:35.859250Z","shell.execute_reply":"2025-11-12T12:15:35.988775Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"!nvidia-smi\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:15:35.990789Z","iopub.execute_input":"2025-11-12T12:15:35.991036Z","iopub.status.idle":"2025-11-12T12:15:36.169370Z","shell.execute_reply.started":"2025-11-12T12:15:35.991012Z","shell.execute_reply":"2025-11-12T12:15:36.168601Z"}},"outputs":[{"name":"stdout","text":"Wed Nov 12 12:15:36 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n| N/A   34C    P0             25W /  250W |       3MiB /  16384MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"!grep -n \"numeric\" main_eval.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:15:36.172164Z","iopub.execute_input":"2025-11-12T12:15:36.172411Z","iopub.status.idle":"2025-11-12T12:15:36.304918Z","shell.execute_reply.started":"2025-11-12T12:15:36.172390Z","shell.execute_reply":"2025-11-12T12:15:36.304121Z"}},"outputs":[{"name":"stdout","text":"3:from pandas.api.types import is_numeric_dtype as numeric\n260:        mse5 = (df.mean(0, numeric_only=True)['5'], df.std(numeric_only=True)['5'])\n261:        mse10 = (df.mean(0, numeric_only=True)['10'], df.std(numeric_only=True)['10'])\n262:        mse20 = (df.mean(0, numeric_only=True)['20'], df.std(numeric_only=True)['20'])\n272:        pse = (df.mean(0, numeric_only=True)['mean'], df.std(numeric_only=True)['mean'])\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"%cd /kaggle/working/dendPLRNN/BPTT_TF/Experiments/Table1/ECG\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:15:36.306050Z","iopub.execute_input":"2025-11-12T12:15:36.306363Z","iopub.status.idle":"2025-11-12T12:15:36.312632Z","shell.execute_reply.started":"2025-11-12T12:15:36.306326Z","shell.execute_reply":"2025-11-12T12:15:36.311568Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/dendPLRNN/BPTT_TF/Experiments/Table1/ECG\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"!sed -i \"s/Argument('use_gpu', \\[[0-9]\\+\\])/Argument('use_gpu', [1])/\" ubermain.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:15:36.313459Z","iopub.execute_input":"2025-11-12T12:15:36.313695Z","iopub.status.idle":"2025-11-12T12:15:36.454064Z","shell.execute_reply.started":"2025-11-12T12:15:36.313678Z","shell.execute_reply":"2025-11-12T12:15:36.453014Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"!grep -n \"use_gpu\" ubermain.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:15:36.455158Z","iopub.execute_input":"2025-11-12T12:15:36.455488Z","iopub.status.idle":"2025-11-12T12:15:36.585048Z","shell.execute_reply.started":"2025-11-12T12:15:36.455436Z","shell.execute_reply":"2025-11-12T12:15:36.584041Z"}},"outputs":[{"name":"stdout","text":"11:    When using GPU for training (i.e. Argument 'use_gpu 1')  it is generally\n17:    args.append(Argument('use_gpu', [1])) # may wanna use gpu here\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"!sed -i \"s/n_runs = [0-9]\\+/n_runs = 1/\" ubermain.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:15:36.586299Z","iopub.execute_input":"2025-11-12T12:15:36.587019Z","iopub.status.idle":"2025-11-12T12:15:36.717079Z","shell.execute_reply.started":"2025-11-12T12:15:36.586991Z","shell.execute_reply":"2025-11-12T12:15:36.716196Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"!grep -n \"n_runs\" ubermain.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:15:36.718130Z","iopub.execute_input":"2025-11-12T12:15:36.718365Z","iopub.status.idle":"2025-11-12T12:15:36.848518Z","shell.execute_reply.started":"2025-11-12T12:15:36.718340Z","shell.execute_reply":"2025-11-12T12:15:36.847834Z"}},"outputs":[{"name":"stdout","text":"4:def ubermain(n_runs):\n28:    args.append(Argument('run', list(range(1, 1 + n_runs))))\n36:    n_runs = 1\n42:    args = ubermain(n_runs)\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"!grep -n \"save_step\" /kaggle/working/dendPLRNN/BPTT_TF/Experiments/Table1/ECG/ubermain.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:15:36.849498Z","iopub.execute_input":"2025-11-12T12:15:36.849711Z","iopub.status.idle":"2025-11-12T12:15:36.978782Z","shell.execute_reply.started":"2025-11-12T12:15:36.849690Z","shell.execute_reply":"2025-11-12T12:15:36.977887Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"!grep -nE \"n_epochs|save_step|teacher_forcing_interval\" /kaggle/working/dendPLRNN/BPTT_TF/Experiments/Table1/ECG/ubermain.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:15:36.979902Z","iopub.execute_input":"2025-11-12T12:15:36.980492Z","iopub.status.idle":"2025-11-12T12:15:37.109693Z","shell.execute_reply.started":"2025-11-12T12:15:36.980434Z","shell.execute_reply":"2025-11-12T12:15:37.108898Z"}},"outputs":[{"name":"stdout","text":"23:    args.append(Argument('n_epochs', [10]))\n24:    args.append(Argument('teacher_forcing_interval', [10], add_to_name_as=\"tau\"))\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"!grep -nE \"save_step|eval_test\" /kaggle/working/dendPLRNN/BPTT_TF/Experiments/Table1/ECG/ubermain.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:16:48.881897Z","iopub.execute_input":"2025-11-12T12:16:48.882641Z","iopub.status.idle":"2025-11-12T12:16:49.012676Z","shell.execute_reply.started":"2025-11-12T12:16:48.882607Z","shell.execute_reply":"2025-11-12T12:16:49.011653Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"!sed -i \"s/Argument('save_step', \\[[0-9]*\\])/Argument('save_step', [5])/\" \\\n/kaggle/working/dendPLRNN/BPTT_TF/Experiments/Table1/ECG/ubermain.py\n!sed -i \"s/Argument('eval_test', \\[[0-9]*\\])/Argument('eval_test', [5])/\" \\\n/kaggle/working/dendPLRNN/BPTT_TF/Experiments/Table1/ECG/ubermain.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:16:50.497416Z","iopub.execute_input":"2025-11-12T12:16:50.497772Z","iopub.status.idle":"2025-11-12T12:16:50.754698Z","shell.execute_reply.started":"2025-11-12T12:16:50.497742Z","shell.execute_reply":"2025-11-12T12:16:50.753439Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"!grep -nE \"save_step|eval_test\" /kaggle/working/dendPLRNN/BPTT_TF/Experiments/Table1/ECG/ubermain.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:16:52.962788Z","iopub.execute_input":"2025-11-12T12:16:52.963124Z","iopub.status.idle":"2025-11-12T12:16:53.093378Z","shell.execute_reply.started":"2025-11-12T12:16:52.963092Z","shell.execute_reply":"2025-11-12T12:16:53.092558Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"!find /kaggle/working/dendPLRNN/BPTT_TF/Experiments/Table1/ECG -maxdepth 3 -type d -name \"run*\" | sort\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:16:54.910137Z","iopub.execute_input":"2025-11-12T12:16:54.910832Z","iopub.status.idle":"2025-11-12T12:16:55.042568Z","shell.execute_reply.started":"2025-11-12T12:16:54.910802Z","shell.execute_reply":"2025-11-12T12:16:55.041681Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"!sed -i \"/eval_test/d\" /kaggle/working/dendPLRNN/BPTT_TF/Experiments/Table1/ECG/ubermain.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:17:14.759094Z","iopub.execute_input":"2025-11-12T12:17:14.759808Z","iopub.status.idle":"2025-11-12T12:17:14.890595Z","shell.execute_reply.started":"2025-11-12T12:17:14.759777Z","shell.execute_reply":"2025-11-12T12:17:14.889549Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"!find /kaggle/working/dendPLRNN/BPTT_TF -maxdepth 4 -type d -name \"run*\" | sort\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:17:26.514703Z","iopub.execute_input":"2025-11-12T12:17:26.515026Z","iopub.status.idle":"2025-11-12T12:17:26.648027Z","shell.execute_reply.started":"2025-11-12T12:17:26.514997Z","shell.execute_reply":"2025-11-12T12:17:26.647002Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"!find /kaggle/working/dendPLRNN/BPTT_TF -type f -name \"*.pt\" | sort | head\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:17:26.850517Z","iopub.execute_input":"2025-11-12T12:17:26.851319Z","iopub.status.idle":"2025-11-12T12:17:26.983662Z","shell.execute_reply.started":"2025-11-12T12:17:26.851290Z","shell.execute_reply":"2025-11-12T12:17:26.982821Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"!grep -A3 \"save_step\" /kaggle/working/dendPLRNN/BPTT_TF/main.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:17:27.300642Z","iopub.execute_input":"2025-11-12T12:17:27.300935Z","iopub.status.idle":"2025-11-12T12:17:27.432100Z","shell.execute_reply.started":"2025-11-12T12:17:27.300911Z","shell.execute_reply":"2025-11-12T12:17:27.431028Z"}},"outputs":[{"name":"stdout","text":"    parser.add_argument('--save_step', '-ss',\n        type=int,\n        help=\"Interval of computing and saving metrics to be stored to TB.\",\n        default=25\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"!sed -i '/for epoch in range(args.n_epochs):/a \\        if (epoch + 1) % args.save_step == 0:\\n            os.makedirs(args.output_dir, exist_ok=True)\\n            model_path = os.path.join(args.output_dir, f\\\"model_epoch_{epoch+1}.pt\\\")\\n            torch.save(model.state_dict(), model_path)\\n            print(f\\\"ðŸ’¾ Saved model checkpoint to {model_path}\\\")' /kaggle/working/dendPLRNN/BPTT_TF/main.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:17:31.775838Z","iopub.execute_input":"2025-11-12T12:17:31.776676Z","iopub.status.idle":"2025-11-12T12:17:31.908446Z","shell.execute_reply.started":"2025-11-12T12:17:31.776627Z","shell.execute_reply":"2025-11-12T12:17:31.907576Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"!sed -i \"s|args.output_dir = .*|args.output_dir = os.path.join('results', 'ECG')|\" /kaggle/working/dendPLRNN/BPTT_TF/main.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:17:33.400228Z","iopub.execute_input":"2025-11-12T12:17:33.400589Z","iopub.status.idle":"2025-11-12T12:17:33.531432Z","shell.execute_reply.started":"2025-11-12T12:17:33.400548Z","shell.execute_reply":"2025-11-12T12:17:33.530335Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"!ls -lh /kaggle/working/dendPLRNN/BPTT_TF/results/ECG\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:17:40.920008Z","iopub.execute_input":"2025-11-12T12:17:40.920799Z","iopub.status.idle":"2025-11-12T12:17:41.053047Z","shell.execute_reply.started":"2025-11-12T12:17:40.920767Z","shell.execute_reply":"2025-11-12T12:17:41.052257Z"}},"outputs":[{"name":"stdout","text":"total 4.0K\ndrwxr-xr-x 3 root root 4.0K Nov 12 12:16 M30B50tau10T500\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"!find /kaggle/working/dendPLRNN/BPTT_TF/results/ECG/M30B50tau10T500 -type f -name \"*.pt\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:17:43.945151Z","iopub.execute_input":"2025-11-12T12:17:43.945849Z","iopub.status.idle":"2025-11-12T12:17:44.076040Z","shell.execute_reply.started":"2025-11-12T12:17:43.945819Z","shell.execute_reply":"2025-11-12T12:17:44.075177Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"%%bash\ncd /kaggle/working/dendPLRNN/BPTT_TF\n\nmkdir -p results/ECG/M30B50tau10T500/001\n\n# Copy your latest model checkpoint to expected filename\ncp results/ECG/model_epoch_10.pt results/ECG/M30B50tau10T500/001/model_10.pt\n\necho \"âœ… Model checkpoint moved to:\"\nls -lh results/ECG/M30B50tau10T500/001/\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:17:53.996730Z","iopub.execute_input":"2025-11-12T12:17:53.997427Z","iopub.status.idle":"2025-11-12T12:17:54.017118Z","shell.execute_reply.started":"2025-11-12T12:17:53.997398Z","shell.execute_reply":"2025-11-12T12:17:54.016314Z"}},"outputs":[{"name":"stdout","text":"âœ… Model checkpoint moved to:\ntotal 96K\n-rw-r--r-- 1 root root 87K Nov 12 12:16 events.out.tfevents.1762949764.ba668fcb6658\n-rw-r--r-- 1 root root 709 Nov 12 12:16 hypers.pkl\n-rw-r--r-- 1 root root 620 Nov 12 12:16 hypers.txt\n","output_type":"stream"},{"name":"stderr","text":"cp: cannot stat 'results/ECG/model_epoch_10.pt': No such file or directory\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"!sed -i '/for epoch in range(args.n_epochs):/a \\\n        # Save checkpoint at intervals\\n\\\n        if (epoch + 1) % args.save_step == 0:\\n\\\n            save_dir = os.path.join(args.output_dir, \"M30B50tau10T500\", \"001\")\\n\\\n            os.makedirs(save_dir, exist_ok=True)\\n\\\n            model_path = os.path.join(save_dir, f\"model_{epoch+1}.pt\")\\n\\\n            torch.save(model.state_dict(), model_path)\\n\\\n            print(f\"ðŸ’¾ Saved model checkpoint to {model_path}\")' \\\n/kaggle/working/dendPLRNN/BPTT_TF/main.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:18:01.053271Z","iopub.execute_input":"2025-11-12T12:18:01.053736Z","iopub.status.idle":"2025-11-12T12:18:01.185084Z","shell.execute_reply.started":"2025-11-12T12:18:01.053709Z","shell.execute_reply":"2025-11-12T12:18:01.184212Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"!grep -n \"save_step\" /kaggle/working/dendPLRNN/BPTT_TF/Experiments/Table1/ECG/ubermain.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:18:01.911283Z","iopub.execute_input":"2025-11-12T12:18:01.911628Z","iopub.status.idle":"2025-11-12T12:18:02.042362Z","shell.execute_reply.started":"2025-11-12T12:18:01.911600Z","shell.execute_reply":"2025-11-12T12:18:02.041203Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"!find /kaggle/working/dendPLRNN/BPTT_TF/results/ECG/M30B50tau10T500/001 -type f -name \"*.pt\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:18:08.415378Z","iopub.execute_input":"2025-11-12T12:18:08.415734Z","iopub.status.idle":"2025-11-12T12:18:08.545679Z","shell.execute_reply.started":"2025-11-12T12:18:08.415703Z","shell.execute_reply":"2025-11-12T12:18:08.544813Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"!rm -rf /kaggle/working/dendPLRNN/BPTT_TF/results/ECG\n!mkdir -p /kaggle/working/dendPLRNN/BPTT_TF/results/ECG\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:18:11.536333Z","iopub.execute_input":"2025-11-12T12:18:11.537271Z","iopub.status.idle":"2025-11-12T12:18:11.790134Z","shell.execute_reply.started":"2025-11-12T12:18:11.537234Z","shell.execute_reply":"2025-11-12T12:18:11.789266Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"!sed -i '/for epoch in range(args.n_epochs):/a \\\n        # === Custom save checkpoint every few epochs ===\\n\\\n        if (epoch + 1) % args.save_step == 0:\\n\\\n            import torch, os\\n\\\n            save_dir = os.path.join(args.output_dir, \"ECG\", \"M30B50tau10T500\", \"001\")\\n\\\n            os.makedirs(save_dir, exist_ok=True)\\n\\\n            model_path = os.path.join(save_dir, f\"model_{epoch+1}.pt\")\\n\\\n            torch.save(model.state_dict(), model_path)\\n\\\n            print(f\"ðŸ’¾ Saved model checkpoint to {model_path}\")' \\\n/kaggle/working/dendPLRNN/BPTT_TF/main.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:18:12.927679Z","iopub.execute_input":"2025-11-12T12:18:12.928004Z","iopub.status.idle":"2025-11-12T12:18:13.061093Z","shell.execute_reply.started":"2025-11-12T12:18:12.927977Z","shell.execute_reply":"2025-11-12T12:18:13.060091Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"!grep -n \"save_step\" /kaggle/working/dendPLRNN/BPTT_TF/Experiments/Table1/ECG/ubermain.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:18:16.391717Z","iopub.execute_input":"2025-11-12T12:18:16.392076Z","iopub.status.idle":"2025-11-12T12:18:16.521874Z","shell.execute_reply.started":"2025-11-12T12:18:16.392045Z","shell.execute_reply":"2025-11-12T12:18:16.520805Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"!find /kaggle/working/dendPLRNN/BPTT_TF/results/ECG/M30B50tau10T500/001 -type f -name \"*.pt\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:18:41.035775Z","iopub.execute_input":"2025-11-12T12:18:41.036664Z","iopub.status.idle":"2025-11-12T12:18:41.167442Z","shell.execute_reply.started":"2025-11-12T12:18:41.036631Z","shell.execute_reply":"2025-11-12T12:18:41.166630Z"}},"outputs":[{"name":"stdout","text":"find: â€˜/kaggle/working/dendPLRNN/BPTT_TF/results/ECG/M30B50tau10T500/001â€™: No such file or directory\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"!sed -i '/class PLRNNModel(nn.Module):/a \\\n    # âœ… Novelty 1: Add Dendritic Gating parameter (U matrix)\\n\\\n    def __init__(self, *args, **kwargs):\\n\\\n        super().__init__(*args, **kwargs)\\n\\\n        self.U = nn.Parameter(torch.randn(self.W.shape) * 0.1)  # Dendritic gating weights\\n\\\n\\n\\\n    def dendritic_gate(self, z):\\n\\\n        \\\"\\\"\\\"Applies dendritic gating mechanism.\\\"\\\"\\\"\\n\\\n        gate = torch.sigmoid(torch.matmul(z, self.U))  # Nonlinear dendritic control\\n\\\n        return gate * torch.relu(z)\\n' /kaggle/working/dendPLRNN/BPTT_TF/bptt/PLRNN_model.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:18:43.741894Z","iopub.execute_input":"2025-11-12T12:18:43.742519Z","iopub.status.idle":"2025-11-12T12:18:43.875737Z","shell.execute_reply.started":"2025-11-12T12:18:43.742489Z","shell.execute_reply":"2025-11-12T12:18:43.874688Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"!sed -i 's/torch.relu(z)/self.dendritic_gate(z)/g' /kaggle/working/dendPLRNN/BPTT_TF/bptt/PLRNN_model.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:18:44.986165Z","iopub.execute_input":"2025-11-12T12:18:44.987009Z","iopub.status.idle":"2025-11-12T12:18:45.117836Z","shell.execute_reply.started":"2025-11-12T12:18:44.986981Z","shell.execute_reply":"2025-11-12T12:18:45.116952Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"!sed -i '/loss = criterion(pred, target)/a \\\n        # âœ… Novelty 2: Add Hybrid Regularization (KLx + Cosine Smoothness)\\n\\\n        lambda_cos = 0.05  # smoothness coefficient\\n\\\n        if t > 0:\\n\\\n            z_prev = z_t_minus_1.detach()\\n\\\n            cos_sim = torch.nn.functional.cosine_similarity(z, z_prev, dim=-1).mean()\\n\\\n            loss += lambda_cos * (1 - cos_sim)\\n\\\n        \\n\\\n        # KLx regularization if available\\n\\\n        if hasattr(self, \"KLx\") and self.KLx is not None:\\n\\\n            lambda_klx = 0.01\\n\\\n            loss += lambda_klx * self.KLx\\n' \\\n/kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:18:47.631241Z","iopub.execute_input":"2025-11-12T12:18:47.631736Z","iopub.status.idle":"2025-11-12T12:18:47.767213Z","shell.execute_reply.started":"2025-11-12T12:18:47.631700Z","shell.execute_reply":"2025-11-12T12:18:47.766365Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"!sed -i '/optimizer = torch.optim.Adam/a \\\n        # Novelty 3: Manifoldâ€“Attractor Regularization + Cosine Scheduler\\n\\\n        lambda_mar = 0.01\\n\\\n        clip_val = 10.0\\n\\\n        def manifold_attractor_regularization(model):\\n\\\n            if not hasattr(model, \\\"A\\\"): return 0.0\\n\\\n            A = model.A; W = getattr(model, \\\"W\\\", None); h = getattr(model, \\\"h\\\", None)\\n\\\n            Mreg = int(A.shape[0] * 0.2)\\n\\\n            A_diag = torch.diag(A)\\n\\\n            mar_loss = 0.0\\n\\\n            for i in range(Mreg):\\n\\\n                mar_loss += (A_diag[i] - 1.0)**2\\n\\\n                if h is not None: mar_loss += h[i]**2\\n\\\n                if W is not None:\\n\\\n                    row = W[i, :]\\n\\\n                    mar_loss += torch.sum(row**2) - row[i]**2\\n\\\n            return mar_loss\\n\\\n        \\n\\\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.n_epochs)\\n' \\\n/kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:19:22.204395Z","iopub.execute_input":"2025-11-12T12:19:22.204751Z","iopub.status.idle":"2025-11-12T12:19:22.344942Z","shell.execute_reply.started":"2025-11-12T12:19:22.204725Z","shell.execute_reply":"2025-11-12T12:19:22.344023Z"}},"outputs":[],"execution_count":61},{"cell_type":"code","source":"!sed -i '/loss.backward()/i \\\n        #  Apply MAR loss before backprop\\n\\\n        mar_loss = manifold_attractor_regularization(model)\\n\\\n        loss = loss + lambda_mar * mar_loss\\n\\\n        for name, param in model.named_parameters():\\n\\\n            if \"z\" in name:\\n\\\n                param.data = torch.clamp(param.data, -clip_val, clip_val)\\n\\\n        scheduler.step()\\n' \\\n/kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:19:23.095190Z","iopub.execute_input":"2025-11-12T12:19:23.095523Z","iopub.status.idle":"2025-11-12T12:19:23.228538Z","shell.execute_reply.started":"2025-11-12T12:19:23.095492Z","shell.execute_reply":"2025-11-12T12:19:23.227418Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"!sed -i '/z = self.latent_step(z, \\*params)/a \\\n        # Novelty 4: Adaptive Latent Noise Injection\\n\\\n        if self.training:\\n\\\n            noise_std = getattr(self, \"noise_std\", 0.02)\\n\\\n            z = z + torch.randn_like(z) * noise_std\\n' \\\n/kaggle/working/dendPLRNN/BPTT_TF/bptt/PLRNN_model.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:19:25.119016Z","iopub.execute_input":"2025-11-12T12:19:25.119926Z","iopub.status.idle":"2025-11-12T12:19:25.252045Z","shell.execute_reply.started":"2025-11-12T12:19:25.119888Z","shell.execute_reply":"2025-11-12T12:19:25.250827Z"}},"outputs":[],"execution_count":63},{"cell_type":"code","source":"!sed -i '/lambda_klx = 0.01/a \\\n        # Novelty 5: Temporal Self-Distillation Loss\\n\\\n        if \\\"z\\\" in locals() and z.shape[0] > 1:\\n\\\n            distill_loss = torch.mean((z[1:] - z[:-1].detach()) ** 2)\\n\\\n            loss = loss + 0.05 * distill_loss\\n' \\\n/kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:19:25.484879Z","iopub.execute_input":"2025-11-12T12:19:25.485725Z","iopub.status.idle":"2025-11-12T12:19:25.617396Z","shell.execute_reply.started":"2025-11-12T12:19:25.485689Z","shell.execute_reply":"2025-11-12T12:19:25.616560Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"!grep -n \"Novelty\" /kaggle/working/dendPLRNN/BPTT_TF/bptt/*.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:19:25.946068Z","iopub.execute_input":"2025-11-12T12:19:25.946391Z","iopub.status.idle":"2025-11-12T12:19:26.077420Z","shell.execute_reply.started":"2025-11-12T12:19:25.946359Z","shell.execute_reply":"2025-11-12T12:19:26.076395Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/dendPLRNN/BPTT_TF/bptt/PLRNN_model.py:107:# Novelty 4: Adaptive Latent Noise Injection\n/kaggle/working/dendPLRNN/BPTT_TF/bptt/PLRNN_model.py:112:# Novelty 4: Adaptive Latent Noise Injection\n","output_type":"stream"}],"execution_count":65},{"cell_type":"code","source":"!grep -q \"Dendritic Gating parameter\" /kaggle/working/dendPLRNN/BPTT_TF/bptt/PLRNN_model.py || \\\nsed -i '/class PLRNNModel(nn.Module):/a \\\n    # âœ… Novelty 1: Add Dendritic Gating parameter (U matrix)\\n\\\n    def __init__(self, *args, **kwargs):\\n\\\n        super().__init__(*args, **kwargs)\\n\\\n        self.U = nn.Parameter(torch.randn(self.W.shape) * 0.1)  # Dendritic gating weights\\n\\\n\\n\\\n    def dendritic_gate(self, z):\\n\\\n        \\\"\\\"\\\"Applies dendritic gating mechanism.\\\"\\\"\\\"\\n\\\n        gate = torch.sigmoid(torch.matmul(z, self.U))  # Nonlinear dendritic control\\n\\\n        return gate * torch.relu(z)\\n' \\\n/kaggle/working/dendPLRNN/BPTT_TF/bptt/PLRNN_model.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:19:30.415984Z","iopub.execute_input":"2025-11-12T12:19:30.416315Z","iopub.status.idle":"2025-11-12T12:19:30.554530Z","shell.execute_reply.started":"2025-11-12T12:19:30.416289Z","shell.execute_reply":"2025-11-12T12:19:30.553449Z"}},"outputs":[],"execution_count":66},{"cell_type":"code","source":"!grep -q \"Hybrid Regularization\" /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py || \\\nsed -i '/loss = criterion(pred, target)/a \\\n        # âœ… Novelty 2: Add Hybrid Regularization (KLx + Cosine Smoothness)\\n\\\n        lambda_cos = 0.05  # smoothness coefficient\\n\\\n        if t > 0:\\n\\\n            z_prev = z_t_minus_1.detach()\\n\\\n            cos_sim = torch.nn.functional.cosine_similarity(z, z_prev, dim=-1).mean()\\n\\\n            loss += lambda_cos * (1 - cos_sim)\\n\\\n        if hasattr(self, \"KLx\") and self.KLx is not None:\\n\\\n            lambda_klx = 0.01\\n\\\n            loss += lambda_klx * self.KLx\\n' \\\n/kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:19:30.988224Z","iopub.execute_input":"2025-11-12T12:19:30.988569Z","iopub.status.idle":"2025-11-12T12:19:31.125689Z","shell.execute_reply.started":"2025-11-12T12:19:30.988533Z","shell.execute_reply":"2025-11-12T12:19:31.124801Z"}},"outputs":[],"execution_count":67},{"cell_type":"code","source":"!grep -q \"Manifoldâ€“Attractor\" /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py || \\\nsed -i '/optimizer = torch.optim.Adam/a \\\n        # âœ… Novelty 3: Manifoldâ€“Attractor Regularization + Cosine Scheduler\\n\\\n        lambda_mar = 0.01\\n\\\n        clip_val = 10.0\\n\\\n        def manifold_attractor_regularization(model):\\n\\\n            if not hasattr(model, \"A\"): return 0.0\\n\\\n            A = model.A; W = getattr(model, \"W\", None); h = getattr(model, \"h\", None)\\n\\\n            Mreg = int(A.shape[0] * 0.2)\\n\\\n            A_diag = torch.diag(A)\\n\\\n            mar_loss = 0.0\\n\\\n            for i in range(Mreg):\\n\\\n                mar_loss += (A_diag[i] - 1.0)**2\\n\\\n                if h is not None: mar_loss += h[i]**2\\n\\\n                if W is not None:\\n\\\n                    row = W[i, :]\\n\\\n                    mar_loss += torch.sum(row**2) - row[i]**2\\n\\\n            return mar_loss\\n\\\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.n_epochs)\\n' \\\n/kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:19:33.302624Z","iopub.execute_input":"2025-11-12T12:19:33.303300Z","iopub.status.idle":"2025-11-12T12:19:33.446203Z","shell.execute_reply.started":"2025-11-12T12:19:33.303272Z","shell.execute_reply":"2025-11-12T12:19:33.445086Z"}},"outputs":[],"execution_count":68},{"cell_type":"code","source":"!sed -i '/loss.backward()/i \\\n        # âœ… Apply MAR loss before backprop\\n\\\n        mar_loss = manifold_attractor_regularization(model)\\n\\\n        loss = loss + lambda_mar * mar_loss\\n\\\n        for name, param in model.named_parameters():\\n\\\n            if \"z\" in name:\\n\\\n                param.data = torch.clamp(param.data, -clip_val, clip_val)\\n\\\n        scheduler.step()\\n' \\\n/kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:19:34.051679Z","iopub.execute_input":"2025-11-12T12:19:34.051998Z","iopub.status.idle":"2025-11-12T12:19:34.184490Z","shell.execute_reply.started":"2025-11-12T12:19:34.051971Z","shell.execute_reply":"2025-11-12T12:19:34.183404Z"}},"outputs":[],"execution_count":69},{"cell_type":"code","source":"!grep -q \"Adaptive Latent Noise\" /kaggle/working/dendPLRNN/BPTT_TF/bptt/PLRNN_model.py || \\\nsed -i '/z = self.latent_step(z, \\*params)/a \\\n        # âœ… Novelty 4: Adaptive Latent Noise Injection\\n\\\n        if self.training:\\n\\\n            noise_std = getattr(self, \"noise_std\", 0.02)\\n\\\n            z = z + torch.randn_like(z) * noise_std\\n' \\\n/kaggle/working/dendPLRNN/BPTT_TF/bptt/PLRNN_model.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:19:36.371605Z","iopub.execute_input":"2025-11-12T12:19:36.371938Z","iopub.status.idle":"2025-11-12T12:19:36.504626Z","shell.execute_reply.started":"2025-11-12T12:19:36.371911Z","shell.execute_reply":"2025-11-12T12:19:36.503723Z"}},"outputs":[],"execution_count":70},{"cell_type":"code","source":"!grep -q \"Temporal Self-Distillation\" /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py || \\\nsed -i '/lambda_klx = 0.01/a \\\n        # âœ… Novelty 5: Temporal Self-Distillation Loss\\n\\\n        if \"z\" in locals() and z.shape[0] > 1:\\n\\\n            distill_loss = torch.mean((z[1:] - z[:-1].detach()) ** 2)\\n\\\n            loss = loss + 0.05 * distill_loss\\n' \\\n/kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:19:36.716997Z","iopub.execute_input":"2025-11-12T12:19:36.717310Z","iopub.status.idle":"2025-11-12T12:19:36.853061Z","shell.execute_reply.started":"2025-11-12T12:19:36.717279Z","shell.execute_reply":"2025-11-12T12:19:36.851964Z"}},"outputs":[],"execution_count":71},{"cell_type":"code","source":"!grep -n \"Novelty\" /kaggle/working/dendPLRNN/BPTT_TF/bptt/*.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:19:38.682433Z","iopub.execute_input":"2025-11-12T12:19:38.683314Z","iopub.status.idle":"2025-11-12T12:19:38.814136Z","shell.execute_reply.started":"2025-11-12T12:19:38.683268Z","shell.execute_reply":"2025-11-12T12:19:38.812949Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/dendPLRNN/BPTT_TF/bptt/PLRNN_model.py:107:# Novelty 4: Adaptive Latent Noise Injection\n/kaggle/working/dendPLRNN/BPTT_TF/bptt/PLRNN_model.py:112:# Novelty 4: Adaptive Latent Noise Injection\n","output_type":"stream"}],"execution_count":72},{"cell_type":"code","source":"!cp /kaggle/working/dendPLRNN/BPTT_TF/bptt/PLRNN_model.py /kaggle/working/PLRNN_model_backup.py\n!cp /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py /kaggle/working/bptt_algorithm_backup.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:19:42.110826Z","iopub.execute_input":"2025-11-12T12:19:42.111132Z","iopub.status.idle":"2025-11-12T12:19:42.365897Z","shell.execute_reply.started":"2025-11-12T12:19:42.111102Z","shell.execute_reply":"2025-11-12T12:19:42.364772Z"}},"outputs":[],"execution_count":73},{"cell_type":"code","source":"%%bash\n# --- Backup existing files ---\ncp /kaggle/working/dendPLRNN/BPTT_TF/bptt/PLRNN_model.py /kaggle/working/PLRNN_model_backup.py\ncp /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py /kaggle/working/bptt_algorithm_backup.py\n\n# --- Novelty 1: Dendritic Gating ---\ngrep -q \"Dendritic Gating parameter\" /kaggle/working/dendPLRNN/BPTT_TF/bptt/PLRNN_model.py || cat <<'EOF' >> /kaggle/working/dendPLRNN/BPTT_TF/bptt/PLRNN_model.py\n\n# âœ… Novelty 1: Dendritic Gating parameter (U matrix)\nclass DendriticGatingMixin:\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.U = nn.Parameter(torch.randn(self.W.shape) * 0.1)  # Dendritic gating weights\n\n    def dendritic_gate(self, z):\n        \"\"\"Applies dendritic gating mechanism.\"\"\"\n        gate = torch.sigmoid(torch.matmul(z, self.U))\n        return gate * torch.relu(z)\nEOF\n\n# --- Novelty 2: Hybrid Regularization ---\ngrep -q \"Hybrid Regularization\" /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py || cat <<'EOF' >> /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n\n# âœ… Novelty 2: Hybrid Regularization (KLx + Cosine Smoothness)\ndef hybrid_regularization(loss, z, z_prev=None, klx=None):\n    lambda_cos = 0.05\n    if z_prev is not None:\n        cos_sim = torch.nn.functional.cosine_similarity(z, z_prev, dim=-1).mean()\n        loss += lambda_cos * (1 - cos_sim)\n    if klx is not None:\n        lambda_klx = 0.01\n        loss += lambda_klx * klx\n    return loss\nEOF\n\n# --- Novelty 3: Manifoldâ€“Attractor Regularization + Cosine Scheduler ---\ngrep -q \"Manifoldâ€“Attractor\" /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py || cat <<'EOF' >> /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n\n# âœ… Novelty 3: Manifoldâ€“Attractor Regularization + Cosine Scheduler\ndef manifold_attractor_regularization(model, lambda_mar=0.01):\n    if not hasattr(model, \"A\"):\n        return 0.0\n    A = model.A\n    W = getattr(model, \"W\", None)\n    h = getattr(model, \"h\", None)\n    Mreg = int(A.shape[0] * 0.2)\n    A_diag = torch.diag(A)\n    mar_loss = 0.0\n    for i in range(Mreg):\n        mar_loss += (A_diag[i] - 1.0) ** 2\n        if h is not None:\n            mar_loss += h[i] ** 2\n        if W is not None:\n            row = W[i, :]\n            mar_loss += torch.sum(row**2) - row[i] ** 2\n    return lambda_mar * mar_loss\n\ndef build_cosine_scheduler(optimizer, n_epochs):\n    return torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs)\nEOF\n\n# --- Novelty 4: Adaptive Latent Noise Injection ---\ngrep -q \"Adaptive Latent Noise Injection\" /kaggle/working/dendPLRNN/BPTT_TF/bptt/PLRNN_model.py || cat <<'EOF' >> /kaggle/working/dendPLRNN/BPTT_TF/bptt/PLRNN_model.py\n\n# âœ… Novelty 4: Adaptive Latent Noise Injection\ndef inject_latent_noise(z, training=True, noise_std=0.02):\n    if training:\n        z = z + torch.randn_like(z) * noise_std\n    return z\nEOF\n\n# --- Novelty 5: Temporal Self-Distillation ---\ngrep -q \"Temporal Self-Distillation\" /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py || cat <<'EOF' >> /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n\n# âœ… Novelty 5: Temporal Self-Distillation Loss\ndef temporal_self_distillation(z, weight=0.05):\n    if z is not None and z.shape[0] > 1:\n        return weight * torch.mean((z[1:] - z[:-1].detach()) ** 2)\n    return 0.0\nEOF\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:19:45.079084Z","iopub.execute_input":"2025-11-12T12:19:45.079670Z","iopub.status.idle":"2025-11-12T12:19:45.112249Z","shell.execute_reply.started":"2025-11-12T12:19:45.079635Z","shell.execute_reply":"2025-11-12T12:19:45.111378Z"}},"outputs":[],"execution_count":74},{"cell_type":"code","source":"!grep -n \"Novelty\" /kaggle/working/dendPLRNN/BPTT_TF/bptt/*.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:26:52.862428Z","iopub.execute_input":"2025-11-12T12:26:52.863313Z","iopub.status.idle":"2025-11-12T12:26:52.994532Z","shell.execute_reply.started":"2025-11-12T12:26:52.863281Z","shell.execute_reply":"2025-11-12T12:26:52.993587Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py:202:# âœ… Novelty 2: Hybrid Regularization (KLx + Cosine Smoothness)\n/kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py:213:# âœ… Novelty 3: Manifoldâ€“Attractor Regularization + Cosine Scheduler\n/kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py:235:# âœ… Novelty 5: Temporal Self-Distillation Loss\n/kaggle/working/dendPLRNN/BPTT_TF/bptt/PLRNN_model.py:219:# âœ… Novelty 1: Dendritic Gating parameter (U matrix)\n","output_type":"stream"}],"execution_count":89},{"cell_type":"code","source":"!sed -i '113,130d' /kaggle/working/dendPLRNN/BPTT_TF/bptt/PLRNN_model.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:20:52.647745Z","iopub.execute_input":"2025-11-12T12:20:52.648270Z","iopub.status.idle":"2025-11-12T12:20:52.778790Z","shell.execute_reply.started":"2025-11-12T12:20:52.648243Z","shell.execute_reply":"2025-11-12T12:20:52.777831Z"}},"outputs":[],"execution_count":77},{"cell_type":"code","source":"!grep -n \"Novelty 4\" /kaggle/working/dendPLRNN/BPTT_TF/bptt/PLRNN_model.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:21:01.216658Z","iopub.execute_input":"2025-11-12T12:21:01.217392Z","iopub.status.idle":"2025-11-12T12:21:01.348921Z","shell.execute_reply.started":"2025-11-12T12:21:01.217360Z","shell.execute_reply":"2025-11-12T12:21:01.348114Z"}},"outputs":[{"name":"stdout","text":"107:# Novelty 4: Adaptive Latent Noise Injection\n112:# Novelty 4: Adaptive Latent Noise Injection\n","output_type":"stream"}],"execution_count":78},{"cell_type":"code","source":"!grep -n \"âœ… Novelty\" /kaggle/working/dendPLRNN/BPTT_TF/bptt/*.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:21:16.491105Z","iopub.execute_input":"2025-11-12T12:21:16.491942Z","iopub.status.idle":"2025-11-12T12:21:16.623902Z","shell.execute_reply.started":"2025-11-12T12:21:16.491913Z","shell.execute_reply":"2025-11-12T12:21:16.622820Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py:202:# âœ… Novelty 2: Hybrid Regularization (KLx + Cosine Smoothness)\n/kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py:213:# âœ… Novelty 3: Manifoldâ€“Attractor Regularization + Cosine Scheduler\n/kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py:235:# âœ… Novelty 5: Temporal Self-Distillation Loss\n/kaggle/working/dendPLRNN/BPTT_TF/bptt/PLRNN_model.py:250:# âœ… Novelty 1: Dendritic Gating parameter (U matrix)\n","output_type":"stream"}],"execution_count":79},{"cell_type":"code","source":"!sed -i '100,130d' /kaggle/working/dendPLRNN/BPTT_TF/bptt/PLRNN_model.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:24:42.701372Z","iopub.execute_input":"2025-11-12T12:24:42.701760Z","iopub.status.idle":"2025-11-12T12:24:42.832605Z","shell.execute_reply.started":"2025-11-12T12:24:42.701727Z","shell.execute_reply":"2025-11-12T12:24:42.831721Z"}},"outputs":[],"execution_count":81},{"cell_type":"code","source":"!sed -i '/class PLRNNModel/i \\\n# âœ… Novelty 4: Adaptive Latent Noise Injection\\n\\\nclass AdaptiveNoiseMixin:\\n\\\n    def inject_latent_noise(self, z, noise_scale=0.02):\\n\\\n        if self.training:\\n\\\n            eps = torch.randn_like(z) * noise_scale\\n\\\n            return z + eps\\n\\\n        return z\\n' /kaggle/working/dendPLRNN/BPTT_TF/bptt/PLRNN_model.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:27:57.010128Z","iopub.execute_input":"2025-11-12T12:27:57.011010Z","iopub.status.idle":"2025-11-12T12:27:57.142040Z","shell.execute_reply.started":"2025-11-12T12:27:57.010976Z","shell.execute_reply":"2025-11-12T12:27:57.141139Z"}},"outputs":[],"execution_count":90},{"cell_type":"code","source":"!grep -n \"âœ… Novelty\" /kaggle/working/dendPLRNN/BPTT_TF/bptt/*.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:27:58.853457Z","iopub.execute_input":"2025-11-12T12:27:58.854170Z","iopub.status.idle":"2025-11-12T12:27:58.983937Z","shell.execute_reply.started":"2025-11-12T12:27:58.854141Z","shell.execute_reply":"2025-11-12T12:27:58.983168Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py:202:# âœ… Novelty 2: Hybrid Regularization (KLx + Cosine Smoothness)\n/kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py:213:# âœ… Novelty 3: Manifoldâ€“Attractor Regularization + Cosine Scheduler\n/kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py:235:# âœ… Novelty 5: Temporal Self-Distillation Loss\n/kaggle/working/dendPLRNN/BPTT_TF/bptt/PLRNN_model.py:219:# âœ… Novelty 1: Dendritic Gating parameter (U matrix)\n","output_type":"stream"}],"execution_count":91},{"cell_type":"code","source":"!sed -n '90,120p' /kaggle/working/dendPLRNN/BPTT_TF/bptt/PLRNN_model.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:28:16.784625Z","iopub.execute_input":"2025-11-12T12:28:16.785403Z","iopub.status.idle":"2025-11-12T12:28:16.915125Z","shell.execute_reply.started":"2025-11-12T12:28:16.785376Z","shell.execute_reply":"2025-11-12T12:28:16.914196Z"}},"outputs":[{"name":"stdout","text":"\n        # initial state\n        if z0 is None:\n            z = tc.randn(size=(b, self.d_z), device=x.device)\n            z = self.teacher_force(z, x_[0], B_PI)\n        else:\n            z = z0\n\n        # stores whole latent state trajectory\n        Z = tc.empty(size=(T, b, self.d_z), device=x.device)\n\n    def teacher_force(self, z: tc.Tensor, x: tc.Tensor,\n                      B_PI: Optional[tc.Tensor] = None) -> tc.Tensor:\n        '''\n        Apply teacher forcing to the latent state vector z.\n        If B_PI is None, identity mapping is assumed the first\n        dx entries of z are teacher forced. If B_PI is not None,\n        z is estimated using the least-squares solution.\n        '''\n        if B_PI is not None:\n            z = x @ B_PI.t()\n        else:\n            z[:, :self.d_x] = x\n        return z\n\n\nclass Latent_Step(nn.Module):\n    def __init__(self, dz, clip_range=None, layer_norm=False):\n        super(Latent_Step, self).__init__()\n        self.clip_range = clip_range\n        #self.nonlinearity = nn.ReLU()\n","output_type":"stream"}],"execution_count":92},{"cell_type":"code","source":"!grep -n \"class \" /kaggle/working/dendPLRNN/BPTT_TF/bptt/PLRNN_model.py | head -n 10\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:29:26.361025Z","iopub.execute_input":"2025-11-12T12:29:26.361369Z","iopub.status.idle":"2025-11-12T12:29:26.492516Z","shell.execute_reply.started":"2025-11-12T12:29:26.361343Z","shell.execute_reply":"2025-11-12T12:29:26.491794Z"}},"outputs":[{"name":"stdout","text":"9:class PLRNN(nn.Module):\n116:class Latent_Step(nn.Module):\n170:class PLRNN_Step(Latent_Step):\n181:class PLRNN_Basis_Step(Latent_Step):\n201:class PLRNN_Clipping_Step(Latent_Step):\n220:class DendriticGatingMixin:\n","output_type":"stream"}],"execution_count":93},{"cell_type":"code","source":"!sed -i '/class PLRNN(nn.Module):/i \\\n# âœ… Novelty 4: Adaptive Latent Noise Injection\\n\\\nclass AdaptiveNoiseMixin:\\n\\\n    def inject_latent_noise(self, z, noise_scale=0.02):\\n\\\n        if self.training:\\n\\\n            eps = torch.randn_like(z) * noise_scale\\n\\\n            return z + eps\\n\\\n        return z\\n' /kaggle/working/dendPLRNN/BPTT_TF/bptt/PLRNN_model.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:29:54.447646Z","iopub.execute_input":"2025-11-12T12:29:54.448309Z","iopub.status.idle":"2025-11-12T12:29:54.579684Z","shell.execute_reply.started":"2025-11-12T12:29:54.448282Z","shell.execute_reply":"2025-11-12T12:29:54.578677Z"}},"outputs":[],"execution_count":94},{"cell_type":"code","source":"!grep -n \"âœ… Novelty\" /kaggle/working/dendPLRNN/BPTT_TF/bptt/*.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:29:58.125450Z","iopub.execute_input":"2025-11-12T12:29:58.126141Z","iopub.status.idle":"2025-11-12T12:29:58.255744Z","shell.execute_reply.started":"2025-11-12T12:29:58.126110Z","shell.execute_reply":"2025-11-12T12:29:58.254900Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py:202:# âœ… Novelty 2: Hybrid Regularization (KLx + Cosine Smoothness)\n/kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py:213:# âœ… Novelty 3: Manifoldâ€“Attractor Regularization + Cosine Scheduler\n/kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py:235:# âœ… Novelty 5: Temporal Self-Distillation Loss\n/kaggle/working/dendPLRNN/BPTT_TF/bptt/PLRNN_model.py:9:# âœ… Novelty 4: Adaptive Latent Noise Injection\n/kaggle/working/dendPLRNN/BPTT_TF/bptt/PLRNN_model.py:227:# âœ… Novelty 1: Dendritic Gating parameter (U matrix)\n","output_type":"stream"}],"execution_count":95},{"cell_type":"code","source":"!sed -i '90,140{s/^[[:space:]]*/        /}' /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:34:53.521575Z","iopub.execute_input":"2025-11-12T12:34:53.522484Z","iopub.status.idle":"2025-11-12T12:34:53.653803Z","shell.execute_reply.started":"2025-11-12T12:34:53.522431Z","shell.execute_reply":"2025-11-12T12:34:53.652870Z"}},"outputs":[],"execution_count":97},{"cell_type":"code","source":"!sed -n '90,140p' /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:35:02.812641Z","iopub.execute_input":"2025-11-12T12:35:02.813527Z","iopub.status.idle":"2025-11-12T12:35:02.943401Z","shell.execute_reply.started":"2025-11-12T12:35:02.813488Z","shell.execute_reply":"2025-11-12T12:35:02.942701Z"}},"outputs":[{"name":"stdout","text":"        T_start = timer()\n        \n        # sample random sequences every epoch\n        dataloader = self.data_set.get_rand_dataloader()\n        for idx, (inp, target) in enumerate(dataloader):\n        self.optimizer.zero_grad(set_to_none=True)\n        inp += tc.randn_like(inp) * self.noise_level\n        pred = self.model(inp, self.tau)\n        loss = self.compute_loss(pred, target)\n        \n        #  Apply MAR loss before backprop\n        mar_loss = manifold_attractor_regularization(model)\n        loss = loss + lambda_mar * mar_loss\n        for name, param in model.named_parameters():\n        if \"z\" in name:\n        param.data = torch.clamp(param.data, -clip_val, clip_val)\n        scheduler.step()\n        \n        #  Apply MAR loss before backprop\n        mar_loss = manifold_attractor_regularization(model)\n        loss = loss + lambda_mar * mar_loss\n        for name, param in model.named_parameters():\n        if \"z\" in name:\n        param.data = torch.clamp(param.data, -clip_val, clip_val)\n        scheduler.step()\n        \n        # âœ… Apply MAR loss before backprop\n        mar_loss = manifold_attractor_regularization(model)\n        loss = loss + lambda_mar * mar_loss\n        for name, param in model.named_parameters():\n        if \"z\" in name:\n        param.data = torch.clamp(param.data, -clip_val, clip_val)\n        scheduler.step()\n        \n        loss.backward()\n        if self.gradient_clipping >= 1:\n        nn.utils.clip_grad_norm_(parameters=self.model.parameters(),\n        max_norm=self.gc)\n        self.optimizer.step()\n        \n        self.scheduler.step()\n        \n        # timing\n        T_end = timer()\n        T_diff = T_end-T_start\n        cum_T += T_diff\n        cum_T_str = str(datetime.timedelta(seconds=cum_T)).split('.')[0]\n        \n        print(f\"Epoch {epoch} took {round(T_diff, 2)}s | Cumulative time (h:mm:ss):\" \n        f\" {cum_T_str} | Loss = {loss.item()}\")\n        \n","output_type":"stream"}],"execution_count":98},{"cell_type":"code","source":"!sed -n '85,105p' /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:36:09.544396Z","iopub.execute_input":"2025-11-12T12:36:09.545142Z","iopub.status.idle":"2025-11-12T12:36:09.674286Z","shell.execute_reply.started":"2025-11-12T12:36:09.545110Z","shell.execute_reply":"2025-11-12T12:36:09.673576Z"}},"outputs":[{"name":"stdout","text":"        for epoch in range(1, self.n_epochs + 1):\n            # enter training mode\n            self.model.train()\n\n            # measure time\n        T_start = timer()\n        \n        # sample random sequences every epoch\n        dataloader = self.data_set.get_rand_dataloader()\n        for idx, (inp, target) in enumerate(dataloader):\n        self.optimizer.zero_grad(set_to_none=True)\n        inp += tc.randn_like(inp) * self.noise_level\n        pred = self.model(inp, self.tau)\n        loss = self.compute_loss(pred, target)\n        \n        #  Apply MAR loss before backprop\n        mar_loss = manifold_attractor_regularization(model)\n        loss = loss + lambda_mar * mar_loss\n        for name, param in model.named_parameters():\n        if \"z\" in name:\n        param.data = torch.clamp(param.data, -clip_val, clip_val)\n","output_type":"stream"}],"execution_count":101},{"cell_type":"code","source":"!sed -i '95,105{s/^/    /}' /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:36:30.451574Z","iopub.execute_input":"2025-11-12T12:36:30.451950Z","iopub.status.idle":"2025-11-12T12:36:30.582440Z","shell.execute_reply.started":"2025-11-12T12:36:30.451918Z","shell.execute_reply":"2025-11-12T12:36:30.581425Z"}},"outputs":[],"execution_count":102},{"cell_type":"code","source":"!sed -n '90,110p' /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:36:43.356556Z","iopub.execute_input":"2025-11-12T12:36:43.357168Z","iopub.status.idle":"2025-11-12T12:36:43.486416Z","shell.execute_reply.started":"2025-11-12T12:36:43.357136Z","shell.execute_reply":"2025-11-12T12:36:43.485526Z"}},"outputs":[{"name":"stdout","text":"        T_start = timer()\n        \n        # sample random sequences every epoch\n        dataloader = self.data_set.get_rand_dataloader()\n        for idx, (inp, target) in enumerate(dataloader):\n            self.optimizer.zero_grad(set_to_none=True)\n            inp += tc.randn_like(inp) * self.noise_level\n            pred = self.model(inp, self.tau)\n            loss = self.compute_loss(pred, target)\n            \n            #  Apply MAR loss before backprop\n            mar_loss = manifold_attractor_regularization(model)\n            loss = loss + lambda_mar * mar_loss\n            for name, param in model.named_parameters():\n            if \"z\" in name:\n            param.data = torch.clamp(param.data, -clip_val, clip_val)\n        scheduler.step()\n        \n        #  Apply MAR loss before backprop\n        mar_loss = manifold_attractor_regularization(model)\n        loss = loss + lambda_mar * mar_loss\n","output_type":"stream"}],"execution_count":103},{"cell_type":"code","source":"!sed -n '95,115p' /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:37:40.609141Z","iopub.execute_input":"2025-11-12T12:37:40.609478Z","iopub.status.idle":"2025-11-12T12:37:40.740317Z","shell.execute_reply.started":"2025-11-12T12:37:40.609450Z","shell.execute_reply":"2025-11-12T12:37:40.739293Z"}},"outputs":[{"name":"stdout","text":"            self.optimizer.zero_grad(set_to_none=True)\n            inp += tc.randn_like(inp) * self.noise_level\n            pred = self.model(inp, self.tau)\n            loss = self.compute_loss(pred, target)\n            \n            #  Apply MAR loss before backprop\n            mar_loss = manifold_attractor_regularization(model)\n            loss = loss + lambda_mar * mar_loss\n            for name, param in model.named_parameters():\n            if \"z\" in name:\n            param.data = torch.clamp(param.data, -clip_val, clip_val)\n        scheduler.step()\n        \n        #  Apply MAR loss before backprop\n        mar_loss = manifold_attractor_regularization(model)\n        loss = loss + lambda_mar * mar_loss\n        for name, param in model.named_parameters():\n        if \"z\" in name:\n        param.data = torch.clamp(param.data, -clip_val, clip_val)\n        scheduler.step()\n        \n","output_type":"stream"}],"execution_count":106},{"cell_type":"code","source":"!sed -i '104,114{s/^/    /}' /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:38:01.036653Z","iopub.execute_input":"2025-11-12T12:38:01.037203Z","iopub.status.idle":"2025-11-12T12:38:01.166995Z","shell.execute_reply.started":"2025-11-12T12:38:01.037173Z","shell.execute_reply":"2025-11-12T12:38:01.166057Z"}},"outputs":[],"execution_count":107},{"cell_type":"code","source":"!sed -n '100,115p' /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:38:10.589558Z","iopub.execute_input":"2025-11-12T12:38:10.589866Z","iopub.status.idle":"2025-11-12T12:38:10.718102Z","shell.execute_reply.started":"2025-11-12T12:38:10.589839Z","shell.execute_reply":"2025-11-12T12:38:10.717340Z"}},"outputs":[{"name":"stdout","text":"            #  Apply MAR loss before backprop\n            mar_loss = manifold_attractor_regularization(model)\n            loss = loss + lambda_mar * mar_loss\n            for name, param in model.named_parameters():\n                if \"z\" in name:\n                param.data = torch.clamp(param.data, -clip_val, clip_val)\n            scheduler.step()\n            \n            #  Apply MAR loss before backprop\n            mar_loss = manifold_attractor_regularization(model)\n            loss = loss + lambda_mar * mar_loss\n            for name, param in model.named_parameters():\n            if \"z\" in name:\n            param.data = torch.clamp(param.data, -clip_val, clip_val)\n            scheduler.step()\n        \n","output_type":"stream"}],"execution_count":108},{"cell_type":"code","source":"!sed -i '105,115d' /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:39:13.752423Z","iopub.execute_input":"2025-11-12T12:39:13.753283Z","iopub.status.idle":"2025-11-12T12:39:13.881678Z","shell.execute_reply.started":"2025-11-12T12:39:13.753252Z","shell.execute_reply":"2025-11-12T12:39:13.880868Z"}},"outputs":[],"execution_count":109},{"cell_type":"code","source":"!sed -n '95,115p' /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:39:22.035819Z","iopub.execute_input":"2025-11-12T12:39:22.036142Z","iopub.status.idle":"2025-11-12T12:39:22.165858Z","shell.execute_reply.started":"2025-11-12T12:39:22.036107Z","shell.execute_reply":"2025-11-12T12:39:22.165124Z"}},"outputs":[{"name":"stdout","text":"            self.optimizer.zero_grad(set_to_none=True)\n            inp += tc.randn_like(inp) * self.noise_level\n            pred = self.model(inp, self.tau)\n            loss = self.compute_loss(pred, target)\n            \n            #  Apply MAR loss before backprop\n            mar_loss = manifold_attractor_regularization(model)\n            loss = loss + lambda_mar * mar_loss\n            for name, param in model.named_parameters():\n                if \"z\" in name:\n        # âœ… Apply MAR loss before backprop\n        mar_loss = manifold_attractor_regularization(model)\n        loss = loss + lambda_mar * mar_loss\n        for name, param in model.named_parameters():\n        if \"z\" in name:\n        param.data = torch.clamp(param.data, -clip_val, clip_val)\n        scheduler.step()\n        \n        loss.backward()\n        if self.gradient_clipping >= 1:\n        nn.utils.clip_grad_norm_(parameters=self.model.parameters(),\n","output_type":"stream"}],"execution_count":110},{"cell_type":"code","source":"cp /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py /kaggle/working/bptt_algorithm_DUPLICATE_BACKUP.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:41:45.509612Z","iopub.execute_input":"2025-11-12T12:41:45.509925Z","iopub.status.idle":"2025-11-12T12:41:45.639907Z","shell.execute_reply.started":"2025-11-12T12:41:45.509900Z","shell.execute_reply":"2025-11-12T12:41:45.638883Z"}},"outputs":[],"execution_count":111},{"cell_type":"code","source":"!sed -i '/Manifoldâ€“Attractor/d' /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n!sed -i '/CosineAnnealingLR/d' /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:42:29.101101Z","iopub.execute_input":"2025-11-12T12:42:29.101898Z","iopub.status.idle":"2025-11-12T12:42:29.349794Z","shell.execute_reply.started":"2025-11-12T12:42:29.101870Z","shell.execute_reply":"2025-11-12T12:42:29.348961Z"}},"outputs":[],"execution_count":113},{"cell_type":"code","source":"%%bash\ncp /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py /kaggle/working/bptt_algorithm_backup_DUPLICATES.py\ncp /kaggle/working/dendPLRNN/BPTT_TF/bptt/PLRNN_model.py /kaggle/working/PLRNN_model_backup_DUPLICATES.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:43:28.075509Z","iopub.execute_input":"2025-11-12T12:43:28.076320Z","iopub.status.idle":"2025-11-12T12:43:28.088736Z","shell.execute_reply.started":"2025-11-12T12:43:28.076289Z","shell.execute_reply":"2025-11-12T12:43:28.088123Z"}},"outputs":[],"execution_count":114},{"cell_type":"code","source":"%%bash\nsed -i '/Novelty/d' /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\nsed -i '/Novelty/d' /kaggle/working/dendPLRNN/BPTT_TF/bptt/PLRNN_model.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:43:41.371146Z","iopub.execute_input":"2025-11-12T12:43:41.371705Z","iopub.status.idle":"2025-11-12T12:43:41.384959Z","shell.execute_reply.started":"2025-11-12T12:43:41.371675Z","shell.execute_reply":"2025-11-12T12:43:41.384172Z"}},"outputs":[],"execution_count":116},{"cell_type":"code","source":"!grep -n \"âœ… Novelty\" /kaggle/working/dendPLRNN/BPTT_TF/bptt/*.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:43:43.605037Z","iopub.execute_input":"2025-11-12T12:43:43.605821Z","iopub.status.idle":"2025-11-12T12:43:43.733566Z","shell.execute_reply.started":"2025-11-12T12:43:43.605794Z","shell.execute_reply":"2025-11-12T12:43:43.732742Z"}},"outputs":[],"execution_count":117},{"cell_type":"code","source":"%%bash\n# --- Novelty 1: Dendritic Gating ---\ncat <<'EOF' >> /kaggle/working/dendPLRNN/BPTT_TF/bptt/PLRNN_model.py\n\n# âœ… Novelty 1: Dendritic Gating parameter (U matrix)\nclass DendriticGatingMixin:\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.U = nn.Parameter(torch.randn(self.W.shape) * 0.1)  # Dendritic gating weights\n\n    def dendritic_gate(self, z):\n        \"\"\"Applies dendritic gating mechanism.\"\"\"\n        gate = torch.sigmoid(torch.matmul(z, self.U))\n        return gate * torch.relu(z)\nEOF\n\n\n# --- Novelty 4: Adaptive Latent Noise Injection ---\ncat <<'EOF' >> /kaggle/working/dendPLRNN/BPTT_TF/bptt/PLRNN_model.py\n\n# âœ… Novelty 4: Adaptive Latent Noise Injection\ndef inject_latent_noise(z, training=True, noise_std=0.02):\n    \"\"\"\n    Adds Gaussian noise to latent representation during training.\n    Improves robustness and regularizes latent dynamics.\n    \"\"\"\n    if training:\n        z = z + torch.randn_like(z) * noise_std\n    return z\nEOF\n\n\n# --- Novelty 2, 3, 5 (in bptt_algorithm.py) ---\ncat <<'EOF' >> /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n\n# âœ… Novelty 2: Hybrid Regularization (KLx + Cosine Smoothness)\ndef hybrid_regularization(loss, z, z_prev=None, klx=None):\n    lambda_cos = 0.05\n    if z_prev is not None:\n        cos_sim = torch.nn.functional.cosine_similarity(z, z_prev, dim=-1).mean()\n        loss += lambda_cos * (1 - cos_sim)\n    if klx is not None:\n        lambda_klx = 0.01\n        loss += lambda_klx * klx\n    return loss\n\n\n# âœ… Novelty 3: Manifoldâ€“Attractor Regularization + Cosine Scheduler\ndef manifold_attractor_regularization(model, lambda_mar=0.01):\n    if not hasattr(model, \"A\"):\n        return 0.0\n    A = model.A\n    W = getattr(model, \"W\", None)\n    h = getattr(model, \"h\", None)\n    Mreg = int(A.shape[0] * 0.2)\n    A_diag = torch.diag(A)\n    mar_loss = 0.0\n    for i in range(Mreg):\n        mar_loss += (A_diag[i] - 1.0) ** 2\n        if h is not None:\n            mar_loss += h[i] ** 2\n        if W is not None:\n            row = W[i, :]\n            mar_loss += torch.sum(row**2) - row[i] ** 2\n    return lambda_mar * mar_loss\n\n\ndef build_cosine_scheduler(optimizer, n_epochs):\n    \"\"\"Builds a cosine annealing LR scheduler.\"\"\"\n    return torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs)\n\n\n# âœ… Novelty 5: Temporal Self-Distillation Loss\ndef temporal_self_distillation(z, weight=0.05):\n    \"\"\"\n    Encourages temporal smoothness in latent states by\n    penalizing abrupt changes across time steps.\n    \"\"\"\n    if z is not None and z.shape[0] > 1:\n        return weight * torch.mean((z[1:] - z[:-1].detach()) ** 2)\n    return 0.0\nEOF\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:44:33.241277Z","iopub.execute_input":"2025-11-12T12:44:33.241732Z","iopub.status.idle":"2025-11-12T12:44:33.258421Z","shell.execute_reply.started":"2025-11-12T12:44:33.241700Z","shell.execute_reply":"2025-11-12T12:44:33.257797Z"}},"outputs":[],"execution_count":118},{"cell_type":"code","source":"!grep -n \"âœ… Novelty\" /kaggle/working/dendPLRNN/BPTT_TF/bptt/*.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:44:44.333679Z","iopub.execute_input":"2025-11-12T12:44:44.334215Z","iopub.status.idle":"2025-11-12T12:44:44.463511Z","shell.execute_reply.started":"2025-11-12T12:44:44.334192Z","shell.execute_reply":"2025-11-12T12:44:44.462799Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py:226:# âœ… Novelty 2: Hybrid Regularization (KLx + Cosine Smoothness)\n/kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py:238:# âœ… Novelty 3: Manifoldâ€“Attractor Regularization + Cosine Scheduler\n/kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py:263:# âœ… Novelty 5: Temporal Self-Distillation Loss\n/kaggle/working/dendPLRNN/BPTT_TF/bptt/PLRNN_model.py:236:# âœ… Novelty 1: Dendritic Gating parameter (U matrix)\n/kaggle/working/dendPLRNN/BPTT_TF/bptt/PLRNN_model.py:247:# âœ… Novelty 4: Adaptive Latent Noise Injection\n","output_type":"stream"}],"execution_count":119},{"cell_type":"code","source":"!sed -n '95,115p' /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:48:33.850907Z","iopub.execute_input":"2025-11-12T12:48:33.851518Z","iopub.status.idle":"2025-11-12T12:48:33.980577Z","shell.execute_reply.started":"2025-11-12T12:48:33.851493Z","shell.execute_reply":"2025-11-12T12:48:33.979856Z"}},"outputs":[{"name":"stdout","text":"            self.optimizer.zero_grad(set_to_none=True)\n            inp += tc.randn_like(inp) * self.noise_level\n            pred = self.model(inp, self.tau)\n            loss = self.compute_loss(pred, target)\n            \n            #  Apply MAR loss before backprop\n            mar_loss = manifold_attractor_regularization(model)\n            loss = loss + lambda_mar * mar_loss\n            for name, param in model.named_parameters():\n                if \"z\" in name:\n        # âœ… Apply MAR loss before backprop\n        mar_loss = manifold_attractor_regularization(model)\n        loss = loss + lambda_mar * mar_loss\n        for name, param in model.named_parameters():\n        if \"z\" in name:\n        param.data = torch.clamp(param.data, -clip_val, clip_val)\n        scheduler.step()\n        \n        loss.backward()\n        if self.gradient_clipping >= 1:\n        nn.utils.clip_grad_norm_(parameters=self.model.parameters(),\n","output_type":"stream"}],"execution_count":122},{"cell_type":"code","source":"!grep -n \"Apply MAR loss\" /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:49:20.130361Z","iopub.execute_input":"2025-11-12T12:49:20.131180Z","iopub.status.idle":"2025-11-12T12:49:20.260578Z","shell.execute_reply.started":"2025-11-12T12:49:20.131152Z","shell.execute_reply":"2025-11-12T12:49:20.259714Z"}},"outputs":[{"name":"stdout","text":"100:            #  Apply MAR loss before backprop\n105:        # âœ… Apply MAR loss before backprop\n156:#  Apply MAR loss before backprop\n164:#  Apply MAR loss before backprop\n172:# âœ… Apply MAR loss before backprop\n","output_type":"stream"}],"execution_count":123},{"cell_type":"code","source":"%%bash\n# Remove all older MAR blocks (lines starting with \"#  Apply MAR loss\" before the final one)\nsed -i '/#  Apply MAR loss before backprop/{N;N;N;N;N;N;d;}' /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n\n# Ensure there is only one âœ… version remaining\ngrep -n \"Apply MAR loss\" /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py | cat\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:49:50.894074Z","iopub.execute_input":"2025-11-12T12:49:50.894911Z","iopub.status.idle":"2025-11-12T12:49:50.908688Z","shell.execute_reply.started":"2025-11-12T12:49:50.894878Z","shell.execute_reply":"2025-11-12T12:49:50.907957Z"}},"outputs":[{"name":"stdout","text":"151:# âœ… Apply MAR loss before backprop\n","output_type":"stream"}],"execution_count":124},{"cell_type":"code","source":"!sed -n '170,185p' /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:50:07.885454Z","iopub.execute_input":"2025-11-12T12:50:07.886190Z","iopub.status.idle":"2025-11-12T12:50:08.014345Z","shell.execute_reply.started":"2025-11-12T12:50:07.886167Z","shell.execute_reply":"2025-11-12T12:50:08.013605Z"}},"outputs":[{"name":"stdout","text":"def hybrid_regularization(loss, z, z_prev=None, klx=None):\n    lambda_cos = 0.05\n    if z_prev is not None:\n        cos_sim = torch.nn.functional.cosine_similarity(z, z_prev, dim=-1).mean()\n        loss += lambda_cos * (1 - cos_sim)\n    if klx is not None:\n        lambda_klx = 0.01\n        loss += lambda_klx * klx\n    return loss\n\ndef manifold_attractor_regularization(model, lambda_mar=0.01):\n    if not hasattr(model, \"A\"):\n        return 0.0\n    A = model.A\n    W = getattr(model, \"W\", None)\n    h = getattr(model, \"h\", None)\n","output_type":"stream"}],"execution_count":125},{"cell_type":"code","source":"!grep -R \"âœ… Novelty\" /kaggle/working/dendPLRNN/BPTT_TF/bptt/ | sort\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:51:20.182653Z","iopub.execute_input":"2025-11-12T12:51:20.183397Z","iopub.status.idle":"2025-11-12T12:51:20.315115Z","shell.execute_reply.started":"2025-11-12T12:51:20.183369Z","shell.execute_reply":"2025-11-12T12:51:20.314342Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py:# âœ… Novelty 2: Hybrid Regularization (KLx + Cosine Smoothness)\n/kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py:# âœ… Novelty 3: Manifoldâ€“Attractor Regularization + Cosine Scheduler\n/kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py:# âœ… Novelty 5: Temporal Self-Distillation Loss\n/kaggle/working/dendPLRNN/BPTT_TF/bptt/PLRNN_model.py:# âœ… Novelty 1: Dendritic Gating parameter (U matrix)\n/kaggle/working/dendPLRNN/BPTT_TF/bptt/PLRNN_model.py:# âœ… Novelty 4: Adaptive Latent Noise Injection\n","output_type":"stream"}],"execution_count":126},{"cell_type":"code","source":"%%bash\n# Remove all repeated novelty definitions except the last\nfor tag in \"Novelty 1\" \"Novelty 2\" \"Novelty 3\" \"Novelty 4\" \"Novelty 5\"; do\n  awk -v tag=\"$tag\" '!seen[$0]++ || $0 !~ tag' /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py > tmp && mv tmp /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n  awk -v tag=\"$tag\" '!seen[$0]++ || $0 !~ tag' /kaggle/working/dendPLRNN/BPTT_TF/bptt/PLRNN_model.py > tmp && mv tmp /kaggle/working/dendPLRNN/BPTT_TF/bptt/PLRNN_model.py\ndone\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:51:31.901809Z","iopub.execute_input":"2025-11-12T12:51:31.902647Z","iopub.status.idle":"2025-11-12T12:51:31.948945Z","shell.execute_reply.started":"2025-11-12T12:51:31.902619Z","shell.execute_reply":"2025-11-12T12:51:31.948217Z"}},"outputs":[],"execution_count":127},{"cell_type":"code","source":"!python -m py_compile /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n!python -m py_compile /kaggle/working/dendPLRNN/BPTT_TF/bptt/PLRNN_model.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:51:41.607322Z","iopub.execute_input":"2025-11-12T12:51:41.608065Z","iopub.status.idle":"2025-11-12T12:51:42.004041Z","shell.execute_reply.started":"2025-11-12T12:51:41.608039Z","shell.execute_reply":"2025-11-12T12:51:42.003268Z"}},"outputs":[{"name":"stdout","text":"Sorry: IndentationError: expected an indented block after 'for' statement on line 101 (bptt_algorithm.py, line 102)Sorry: IndentationError: unexpected indent (PLRNN_model.py, line 9)","output_type":"stream"}],"execution_count":128},{"cell_type":"code","source":"%%bash\n# Remove extra indentation (normalize to 0 spaces before def/class)\nsed -i 's/^    def dendritic_gate/def dendritic_gate/' /kaggle/working/dendPLRNN/BPTT_TF/bptt/PLRNN_model.py\nsed -i 's/^    class DendriticGatingMixin/class DendriticGatingMixin/' /kaggle/working/dendPLRNN/BPTT_TF/bptt/PLRNN_model.py\nsed -i 's/^    def inject_latent_noise/def inject_latent_noise/' /kaggle/working/dendPLRNN/BPTT_TF/bptt/PLRNN_model.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:52:18.560407Z","iopub.execute_input":"2025-11-12T12:52:18.561283Z","iopub.status.idle":"2025-11-12T12:52:18.576063Z","shell.execute_reply.started":"2025-11-12T12:52:18.561248Z","shell.execute_reply":"2025-11-12T12:52:18.575480Z"}},"outputs":[],"execution_count":129},{"cell_type":"code","source":"!sed -n '1,30p' /kaggle/working/dendPLRNN/BPTT_TF/bptt/PLRNN_model.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:52:26.437715Z","iopub.execute_input":"2025-11-12T12:52:26.438416Z","iopub.status.idle":"2025-11-12T12:52:26.567770Z","shell.execute_reply.started":"2025-11-12T12:52:26.438393Z","shell.execute_reply":"2025-11-12T12:52:26.566997Z"}},"outputs":[{"name":"stdout","text":"from typing import Optional, Tuple\nfrom bptt.dataset import GeneralDataset\nimport torch.nn as nn\nimport torch as tc\nimport math\nfrom torch.linalg import pinv\n\n\n class AdaptiveNoiseMixin:\n     def inject_latent_noise(self, z, noise_scale=0.02):\n         if self.training:\n             eps = torch.randn_like(z) * noise_scale\n             return z + eps\n         return z\n\nclass PLRNN(nn.Module):\n    \"\"\"\n    Piece-wise Linear Recurrent Neural Network (Durstewitz 2017)\n\n    Args:\n        dim_x: Dimension of the observations\n        dim_z: Dimension of the latent states (number of hidden neurons)\n        n_bases: Number of bases to use in the BE-PLRNN\n        clip_range: latent state clipping value\n        latent_model: Name of the latent model to use. Has to be in LATENT_MODELS\n        mean_centering: Use mean centering\n    \"\"\"\n\n    LATENT_MODELS = ['PLRNN', 'clipped-PLRNN', 'dendr-PLRNN']\n\n","output_type":"stream"}],"execution_count":130},{"cell_type":"code","source":"%%bash\n# Fix missing indentation inside MAR loop (ensure 4 spaces after for)\nsed -i '/for i in range(Mreg):/{n; s/^/    /}' /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:53:00.799749Z","iopub.execute_input":"2025-11-12T12:53:00.800601Z","iopub.status.idle":"2025-11-12T12:53:00.813245Z","shell.execute_reply.started":"2025-11-12T12:53:00.800557Z","shell.execute_reply":"2025-11-12T12:53:00.812413Z"}},"outputs":[],"execution_count":131},{"cell_type":"code","source":"!sed -n '90,120p' /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:53:09.563909Z","iopub.execute_input":"2025-11-12T12:53:09.564749Z","iopub.status.idle":"2025-11-12T12:53:09.694581Z","shell.execute_reply.started":"2025-11-12T12:53:09.564722Z","shell.execute_reply":"2025-11-12T12:53:09.693653Z"}},"outputs":[{"name":"stdout","text":"        T_start = timer()\n        \n        # sample random sequences every epoch\n        dataloader = self.data_set.get_rand_dataloader()\n        for idx, (inp, target) in enumerate(dataloader):\n            self.optimizer.zero_grad(set_to_none=True)\n            inp += tc.randn_like(inp) * self.noise_level\n            pred = self.model(inp, self.tau)\n            loss = self.compute_loss(pred, target)\n            \n        loss = loss + lambda_mar * mar_loss\n        for name, param in model.named_parameters():\n        if \"z\" in name:\n        param.data = torch.clamp(param.data, -clip_val, clip_val)\n        scheduler.step()\n        \n        loss.backward()\n        if self.gradient_clipping >= 1:\n        nn.utils.clip_grad_norm_(parameters=self.model.parameters(),\n        max_norm=self.gc)\n        self.optimizer.step()\n        \n        self.scheduler.step()\n        \n        # timing\n        T_end = timer()\n        T_diff = T_end-T_start\n        cum_T += T_diff\n        cum_T_str = str(datetime.timedelta(seconds=cum_T)).split('.')[0]\n        \n        print(f\"Epoch {epoch} took {round(T_diff, 2)}s | Cumulative time (h:mm:ss):\" \n","output_type":"stream"}],"execution_count":132},{"cell_type":"code","source":"%%bash\n# Fix the MAR + scheduler section indentation\nsed -i '/loss = loss + lambda_mar \\* mar_loss/{N;N;N;N;N;N;d;}' /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n\n# Append correctly formatted MAR + scheduler block\ncat <<'EOF' >> /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n\n        # âœ… Apply MAR loss before backprop\n        mar_loss = manifold_attractor_regularization(model)\n        loss = loss + lambda_mar * mar_loss\n        for name, param in model.named_parameters():\n            if \"z\" in name:\n                param.data = torch.clamp(param.data, -clip_val, clip_val)\n\n        # Step optimizer and scheduler\n        loss.backward()\n        if self.gradient_clipping >= 1:\n            nn.utils.clip_grad_norm_(parameters=self.model.parameters(), max_norm=self.gc)\n        self.optimizer.step()\n        scheduler.step()\nEOF\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:54:48.716667Z","iopub.execute_input":"2025-11-12T12:54:48.717528Z","iopub.status.idle":"2025-11-12T12:54:48.731105Z","shell.execute_reply.started":"2025-11-12T12:54:48.717486Z","shell.execute_reply":"2025-11-12T12:54:48.730272Z"}},"outputs":[],"execution_count":133},{"cell_type":"code","source":"!grep -n \"Apply MAR loss\" /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n!sed -n '90,120p' /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:55:25.652089Z","iopub.execute_input":"2025-11-12T12:55:25.652687Z","iopub.status.idle":"2025-11-12T12:55:25.902903Z","shell.execute_reply.started":"2025-11-12T12:55:25.652663Z","shell.execute_reply":"2025-11-12T12:55:25.902163Z"}},"outputs":[{"name":"stdout","text":"144:# âœ… Apply MAR loss before backprop\n238:        # âœ… Apply MAR loss before backprop\n        T_start = timer()\n        \n        # sample random sequences every epoch\n        dataloader = self.data_set.get_rand_dataloader()\n        for idx, (inp, target) in enumerate(dataloader):\n            self.optimizer.zero_grad(set_to_none=True)\n            inp += tc.randn_like(inp) * self.noise_level\n            pred = self.model(inp, self.tau)\n            loss = self.compute_loss(pred, target)\n            \n        if self.gradient_clipping >= 1:\n        nn.utils.clip_grad_norm_(parameters=self.model.parameters(),\n        max_norm=self.gc)\n        self.optimizer.step()\n        \n        self.scheduler.step()\n        \n        # timing\n        T_end = timer()\n        T_diff = T_end-T_start\n        cum_T += T_diff\n        cum_T_str = str(datetime.timedelta(seconds=cum_T)).split('.')[0]\n        \n        print(f\"Epoch {epoch} took {round(T_diff, 2)}s | Cumulative time (h:mm:ss):\" \n        f\" {cum_T_str} | Loss = {loss.item()}\")\n        \n        \n\n            if epoch % self.save_step == 0:\n                self.saver.epoch_save(self.model, epoch)\n\n","output_type":"stream"}],"execution_count":134},{"cell_type":"code","source":"%%bash\nsed -i '/Apply MAR loss/d' /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:56:57.289133Z","iopub.execute_input":"2025-11-12T12:56:57.289447Z","iopub.status.idle":"2025-11-12T12:56:57.301092Z","shell.execute_reply.started":"2025-11-12T12:56:57.289419Z","shell.execute_reply":"2025-11-12T12:56:57.300407Z"}},"outputs":[],"execution_count":135},{"cell_type":"code","source":"%%bash\n# Insert the correct MAR block after compute_loss line\nsed -i '/loss = self.compute_loss(pred, target)/a \\\n        # âœ… Apply MAR loss before backprop\\n\\\n        mar_loss = manifold_attractor_regularization(model)\\n\\\n        loss = loss + lambda_mar * mar_loss\\n\\\n        for name, param in model.named_parameters():\\n\\\n            if \"z\" in name:\\n\\\n                param.data = torch.clamp(param.data, -clip_val, clip_val)\\n\\\n        \\n\\\n        loss.backward()\\n\\\n        if self.gradient_clipping >= 1:\\n\\\n            nn.utils.clip_grad_norm_(parameters=self.model.parameters(), max_norm=self.gc)\\n\\\n        self.optimizer.step()\\n\\\n        scheduler.step()' \\\n/kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:57:16.626666Z","iopub.execute_input":"2025-11-12T12:57:16.627381Z","iopub.status.idle":"2025-11-12T12:57:16.638425Z","shell.execute_reply.started":"2025-11-12T12:57:16.627359Z","shell.execute_reply":"2025-11-12T12:57:16.637620Z"}},"outputs":[],"execution_count":136},{"cell_type":"code","source":"!grep -n \"Apply MAR loss\" /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n!sed -n '120,160p' /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:57:27.740789Z","iopub.execute_input":"2025-11-12T12:57:27.741527Z","iopub.status.idle":"2025-11-12T12:57:27.995250Z","shell.execute_reply.started":"2025-11-12T12:57:27.741504Z","shell.execute_reply":"2025-11-12T12:57:27.994526Z"}},"outputs":[{"name":"stdout","text":"99:        # âœ… Apply MAR loss before backprop\n164:        # âœ… Apply MAR loss before backprop\n\n        scheduler.step()\n            \n        if self.gradient_clipping >= 1:\n        nn.utils.clip_grad_norm_(parameters=self.model.parameters(),\n        max_norm=self.gc)\n        self.optimizer.step()\n        \n        self.scheduler.step()\n        \n        # timing\n        T_end = timer()\n        T_diff = T_end-T_start\n        cum_T += T_diff\n        cum_T_str = str(datetime.timedelta(seconds=cum_T)).split('.')[0]\n        \n        print(f\"Epoch {epoch} took {round(T_diff, 2)}s | Cumulative time (h:mm:ss):\" \n        f\" {cum_T_str} | Loss = {loss.item()}\")\n        \n        \n\n            if epoch % self.save_step == 0:\n                self.saver.epoch_save(self.model, epoch)\n\n\n    def estimate_gc_norm(self):\n        '''\n        Estimate gradient clipping value as suggested by\n        Pascanu, 2012: On the difficulty of training Recurrent Neural Networks.\n        https://arxiv.org/abs/1211.5063\n        Tracks gradient norms across 10 epochs of training and \n        computes the mean. GC clipping value is then set to 5 times\n        that value.\n        '''\n        print(\"Estimating Gradient Clipping Value ...\")\n        params = deepcopy(self.model.state_dict())\n        N_samples = 0\n        running_g_norm = 0.\n        for e in range(15):\n            dataloader = self.data_set.get_rand_dataloader()\n            for _, (inp, target) in enumerate(dataloader):\n","output_type":"stream"}],"execution_count":137},{"cell_type":"code","source":"%%bash\n# Remove all duplicated MAR blocks\nsed -i '/Apply MAR loss before backprop/,+10d' /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T13:00:03.129942Z","iopub.execute_input":"2025-11-12T13:00:03.130267Z","iopub.status.idle":"2025-11-12T13:00:03.141579Z","shell.execute_reply.started":"2025-11-12T13:00:03.130237Z","shell.execute_reply":"2025-11-12T13:00:03.140939Z"}},"outputs":[],"execution_count":139},{"cell_type":"code","source":"%%bash\nsed -i '/loss = self.compute_loss(pred, target)/a \\\n        # âœ… Apply MAR loss before backprop\\n\\\n        mar_loss = manifold_attractor_regularization(model)\\n\\\n        loss = loss + lambda_mar * mar_loss\\n\\\n        for name, param in model.named_parameters():\\n\\\n            if \"z\" in name:\\n\\\n                param.data = torch.clamp(param.data, -clip_val, clip_val)\\n\\\n        \\n\\\n        # Backpropagation and optimization step\\n\\\n        loss.backward()\\n\\\n        if self.gradient_clipping >= 1:\\n\\\n            nn.utils.clip_grad_norm_(parameters=self.model.parameters(), max_norm=self.gc)\\n\\\n        self.optimizer.step()\\n\\\n        scheduler.step()' \\\n/kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T13:00:11.867161Z","iopub.execute_input":"2025-11-12T13:00:11.867450Z","iopub.status.idle":"2025-11-12T13:00:11.879253Z","shell.execute_reply.started":"2025-11-12T13:00:11.867427Z","shell.execute_reply":"2025-11-12T13:00:11.878518Z"}},"outputs":[],"execution_count":140},{"cell_type":"code","source":"!grep -n \"Apply MAR loss\" /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n!sed -n '80,160p' /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T13:00:24.377032Z","iopub.execute_input":"2025-11-12T13:00:24.377783Z","iopub.status.idle":"2025-11-12T13:00:24.631236Z","shell.execute_reply.started":"2025-11-12T13:00:24.377760Z","shell.execute_reply":"2025-11-12T13:00:24.630454Z"}},"outputs":[{"name":"stdout","text":"99:        # âœ… Apply MAR loss before backprop\n178:        # âœ… Apply MAR loss before backprop\n        if self.gradient_clipping == 1:\n            self.estimate_gc_norm()\n        else:\n            self.gc = self.gradient_clipping\n\n        for epoch in range(1, self.n_epochs + 1):\n            # enter training mode\n            self.model.train()\n\n            # measure time\n        T_start = timer()\n        \n        # sample random sequences every epoch\n        dataloader = self.data_set.get_rand_dataloader()\n        for idx, (inp, target) in enumerate(dataloader):\n            self.optimizer.zero_grad(set_to_none=True)\n            inp += tc.randn_like(inp) * self.noise_level\n            pred = self.model(inp, self.tau)\n            loss = self.compute_loss(pred, target)\n        # âœ… Apply MAR loss before backprop\n\n        mar_loss = manifold_attractor_regularization(model)\n\n        loss = loss + lambda_mar * mar_loss\n\n        for name, param in model.named_parameters():\n\n            if \"z\" in name:\n\n                param.data = torch.clamp(param.data, -clip_val, clip_val)\n\n        \n\n        # Backpropagation and optimization step\n\n        loss.backward()\n\n        if self.gradient_clipping >= 1:\n\n            nn.utils.clip_grad_norm_(parameters=self.model.parameters(), max_norm=self.gc)\n\n        self.optimizer.step()\n\n        scheduler.step()\n\n        \n\n        loss.backward()\n\n        if self.gradient_clipping >= 1:\n\n            nn.utils.clip_grad_norm_(parameters=self.model.parameters(), max_norm=self.gc)\n\n        self.optimizer.step()\n\n        scheduler.step()\n            \n        if self.gradient_clipping >= 1:\n        nn.utils.clip_grad_norm_(parameters=self.model.parameters(),\n        max_norm=self.gc)\n        self.optimizer.step()\n        \n        self.scheduler.step()\n        \n        # timing\n        T_end = timer()\n        T_diff = T_end-T_start\n        cum_T += T_diff\n        cum_T_str = str(datetime.timedelta(seconds=cum_T)).split('.')[0]\n        \n        print(f\"Epoch {epoch} took {round(T_diff, 2)}s | Cumulative time (h:mm:ss):\" \n        f\" {cum_T_str} | Loss = {loss.item()}\")\n        \n        \n\n            if epoch % self.save_step == 0:\n                self.saver.epoch_save(self.model, epoch)\n\n\n    def estimate_gc_norm(self):\n        '''\n","output_type":"stream"}],"execution_count":141},{"cell_type":"code","source":"%%bash\n# Delete all duplicated Apply MAR loss and optimizer/scheduler lines\nsed -i '/Apply MAR loss before backprop/,+20d' /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\nsed -i '/loss.backward()/,+5d' /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T13:02:43.054644Z","iopub.execute_input":"2025-11-12T13:02:43.055491Z","iopub.status.idle":"2025-11-12T13:02:43.068696Z","shell.execute_reply.started":"2025-11-12T13:02:43.055444Z","shell.execute_reply":"2025-11-12T13:02:43.068081Z"}},"outputs":[],"execution_count":142},{"cell_type":"code","source":"%%bash\nsed -i '/loss = self.compute_loss(pred, target)/a \\\n        # âœ… Apply MAR loss before backprop\\n\\\n        mar_loss = manifold_attractor_regularization(model)\\n\\\n        loss = loss + lambda_mar * mar_loss\\n\\\n        for name, param in model.named_parameters():\\n\\\n            if \"z\" in name:\\n\\\n                param.data = torch.clamp(param.data, -clip_val, clip_val)\\n\\\n        \\n\\\n        # Backpropagation and optimization step\\n\\\n        loss.backward()\\n\\\n        if self.gradient_clipping >= 1:\\n\\\n            nn.utils.clip_grad_norm_(parameters=self.model.parameters(), max_norm=self.gc)\\n\\\n        self.optimizer.step()\\n\\\n        scheduler.step()' \\\n/kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T13:02:55.808763Z","iopub.execute_input":"2025-11-12T13:02:55.809285Z","iopub.status.idle":"2025-11-12T13:02:55.820076Z","shell.execute_reply.started":"2025-11-12T13:02:55.809262Z","shell.execute_reply":"2025-11-12T13:02:55.819307Z"}},"outputs":[],"execution_count":143},{"cell_type":"code","source":"!grep -n \"Apply MAR loss\" /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n!sed -n '80,160p' /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T13:03:05.918171Z","iopub.execute_input":"2025-11-12T13:03:05.918411Z","iopub.status.idle":"2025-11-12T13:03:06.170173Z","shell.execute_reply.started":"2025-11-12T13:03:05.918396Z","shell.execute_reply":"2025-11-12T13:03:06.169436Z"}},"outputs":[{"name":"stdout","text":"99:        # âœ… Apply MAR loss before backprop\n176:        # âœ… Apply MAR loss before backprop\n        if self.gradient_clipping == 1:\n            self.estimate_gc_norm()\n        else:\n            self.gc = self.gradient_clipping\n\n        for epoch in range(1, self.n_epochs + 1):\n            # enter training mode\n            self.model.train()\n\n            # measure time\n        T_start = timer()\n        \n        # sample random sequences every epoch\n        dataloader = self.data_set.get_rand_dataloader()\n        for idx, (inp, target) in enumerate(dataloader):\n            self.optimizer.zero_grad(set_to_none=True)\n            inp += tc.randn_like(inp) * self.noise_level\n            pred = self.model(inp, self.tau)\n            loss = self.compute_loss(pred, target)\n        # âœ… Apply MAR loss before backprop\n\n        mar_loss = manifold_attractor_regularization(model)\n\n        loss = loss + lambda_mar * mar_loss\n\n        for name, param in model.named_parameters():\n\n            if \"z\" in name:\n\n                param.data = torch.clamp(param.data, -clip_val, clip_val)\n\n        \n\n        # Backpropagation and optimization step\n\n        loss.backward()\n\n        if self.gradient_clipping >= 1:\n\n            nn.utils.clip_grad_norm_(parameters=self.model.parameters(), max_norm=self.gc)\n\n        self.optimizer.step()\n\n        scheduler.step()\n\n        self.optimizer.step()\n\n        scheduler.step()\n\n        \n\n        self.optimizer.step()\n\n        scheduler.step()\n            \n        if self.gradient_clipping >= 1:\n        nn.utils.clip_grad_norm_(parameters=self.model.parameters(),\n        max_norm=self.gc)\n        self.optimizer.step()\n        \n        self.scheduler.step()\n        \n        # timing\n        T_end = timer()\n        T_diff = T_end-T_start\n        cum_T += T_diff\n        cum_T_str = str(datetime.timedelta(seconds=cum_T)).split('.')[0]\n        \n        print(f\"Epoch {epoch} took {round(T_diff, 2)}s | Cumulative time (h:mm:ss):\" \n        f\" {cum_T_str} | Loss = {loss.item()}\")\n        \n        \n\n            if epoch % self.save_step == 0:\n                self.saver.epoch_save(self.model, epoch)\n\n\n    def estimate_gc_norm(self):\n        '''\n        Estimate gradient clipping value as suggested by\n        Pascanu, 2012: On the difficulty of training Recurrent Neural Networks.\n","output_type":"stream"}],"execution_count":144},{"cell_type":"code","source":"%%bash\nsed -i '90,190d' /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\necho \"âœ… Duplicate MAR and optimizer/scheduler sections removed cleanly.\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T13:04:00.871206Z","iopub.execute_input":"2025-11-12T13:04:00.871534Z","iopub.status.idle":"2025-11-12T13:04:00.882481Z","shell.execute_reply.started":"2025-11-12T13:04:00.871504Z","shell.execute_reply":"2025-11-12T13:04:00.881815Z"}},"outputs":[{"name":"stdout","text":"âœ… Duplicate MAR and optimizer/scheduler sections removed cleanly.\n","output_type":"stream"}],"execution_count":145},{"cell_type":"code","source":"%%bash\nsed -i '/loss = self.compute_loss(pred, target)/a \\\n        # âœ… Apply MAR loss before backprop\\n\\\n        mar_loss = manifold_attractor_regularization(model)\\n\\\n        loss = loss + lambda_mar * mar_loss\\n\\\n        for name, param in model.named_parameters():\\n\\\n            if \"z\" in name:\\n\\\n                param.data = torch.clamp(param.data, -clip_val, clip_val)\\n\\\n        \\n\\\n        # Backpropagation and optimization step\\n\\\n        loss.backward()\\n\\\n        if self.gradient_clipping >= 1:\\n\\\n            nn.utils.clip_grad_norm_(parameters=self.model.parameters(), max_norm=self.gc)\\n\\\n        self.optimizer.step()\\n\\\n        scheduler.step()' \\\n/kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T13:04:11.108611Z","iopub.execute_input":"2025-11-12T13:04:11.109214Z","iopub.status.idle":"2025-11-12T13:04:11.120151Z","shell.execute_reply.started":"2025-11-12T13:04:11.109194Z","shell.execute_reply":"2025-11-12T13:04:11.119357Z"}},"outputs":[],"execution_count":146},{"cell_type":"code","source":"!grep -n \"Apply MAR loss\" /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n!sed -n '80,160p' /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py | head -n 40\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T13:04:21.969011Z","iopub.execute_input":"2025-11-12T13:04:21.969297Z","iopub.status.idle":"2025-11-12T13:04:22.220462Z","shell.execute_reply.started":"2025-11-12T13:04:21.969277Z","shell.execute_reply":"2025-11-12T13:04:22.219758Z"}},"outputs":[{"name":"stdout","text":"        if self.gradient_clipping == 1:\n            self.estimate_gc_norm()\n        else:\n            self.gc = self.gradient_clipping\n\n        for epoch in range(1, self.n_epochs + 1):\n            # enter training mode\n            self.model.train()\n\n            # measure time\n\n        loss.backward()\n\n        if self.gradient_clipping >= 1:\n\n            nn.utils.clip_grad_norm_(parameters=self.model.parameters(), max_norm=self.gc)\n\n        self.optimizer.step()\n\n        scheduler.step()\n\n        self.optimizer.step()\n\n        scheduler.step()\n\n        \n\n        self.optimizer.step()\n\n        scheduler.step()\n\n\n\n         mar_loss = manifold_attractor_regularization(model)\n                if e > 5:\n                    running_g_norm += nn.utils.clip_grad_norm_(parameters=self.model.parameters(),\n                                                            max_norm=1e10)\n                    N_samples += 1\n                self.optimizer.step()\n        \n","output_type":"stream"}],"execution_count":147},{"cell_type":"code","source":"%%bash\nsed -i '90,190d' /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\necho \"âœ… Corrupted section (lines 90â€“190) removed.\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T13:05:17.361120Z","iopub.execute_input":"2025-11-12T13:05:17.361919Z","iopub.status.idle":"2025-11-12T13:05:17.372845Z","shell.execute_reply.started":"2025-11-12T13:05:17.361889Z","shell.execute_reply":"2025-11-12T13:05:17.372288Z"}},"outputs":[{"name":"stdout","text":"âœ… Corrupted section (lines 90â€“190) removed.\n","output_type":"stream"}],"execution_count":148},{"cell_type":"code","source":"%%bash\ncat <<'EOF' >> /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n\n        # === Training Loop (Cleaned & Rebuilt) ===\n        for epoch in range(1, self.n_epochs + 1):\n            self.model.train()\n            T_start = timer()\n\n            dataloader = self.data_set.get_rand_dataloader()\n            for idx, (inp, target) in enumerate(dataloader):\n                self.optimizer.zero_grad(set_to_none=True)\n                inp += tc.randn_like(inp) * self.noise_level\n                pred = self.model(inp, self.tau)\n                loss = self.compute_loss(pred, target)\n\n                # âœ… Apply MAR loss before backprop\n                mar_loss = manifold_attractor_regularization(model)\n                loss = loss + lambda_mar * mar_loss\n                for name, param in model.named_parameters():\n                    if \"z\" in name:\n                        param.data = torch.clamp(param.data, -clip_val, clip_val)\n\n                # Backpropagation and optimization step\n                loss.backward()\n                if self.gradient_clipping >= 1:\n                    nn.utils.clip_grad_norm_(parameters=self.model.parameters(), max_norm=self.gc)\n                self.optimizer.step()\n                scheduler.step()\n\n            # timing\n            T_end = timer()\n            T_diff = T_end - T_start\n            cum_T += T_diff\n            cum_T_str = str(datetime.timedelta(seconds=cum_T)).split('.')[0]\n\n            print(f\"Epoch {epoch} took {round(T_diff, 2)}s | Cumulative time (h:mm:ss): {cum_T_str} | Loss = {loss.item()}\")\n\n            if epoch % self.save_step == 0:\n                self.saver.epoch_save(self.model, epoch)\nEOF\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T13:05:29.657853Z","iopub.execute_input":"2025-11-12T13:05:29.658128Z","iopub.status.idle":"2025-11-12T13:05:29.669389Z","shell.execute_reply.started":"2025-11-12T13:05:29.658106Z","shell.execute_reply":"2025-11-12T13:05:29.668817Z"}},"outputs":[],"execution_count":149},{"cell_type":"code","source":"!grep -n \"Apply MAR loss\" /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n!sed -n '80,200p' /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py | head -n 40\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T13:05:38.897403Z","iopub.execute_input":"2025-11-12T13:05:38.898127Z","iopub.status.idle":"2025-11-12T13:05:39.151196Z","shell.execute_reply.started":"2025-11-12T13:05:38.898102Z","shell.execute_reply":"2025-11-12T13:05:39.150240Z"}},"outputs":[{"name":"stdout","text":"125:                # âœ… Apply MAR loss before backprop\n        if self.gradient_clipping == 1:\n            self.estimate_gc_norm()\n        else:\n            self.gc = self.gradient_clipping\n\n        for epoch in range(1, self.n_epochs + 1):\n            # enter training mode\n            self.model.train()\n\n            # measure time\ndef build_cosine_scheduler(optimizer, n_epochs):\n    \"\"\"Builds a cosine annealing LR scheduler.\"\"\"\n    return torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs)\n\n\n# âœ… Novelty 5: Temporal Self-Distillation Loss\ndef temporal_self_distillation(z, weight=0.05):\n    \"\"\"\n    Encourages temporal smoothness in latent states by\n    penalizing abrupt changes across time steps.\n    \"\"\"\n    if z is not None and z.shape[0] > 1:\n        return weight * torch.mean((z[1:] - z[:-1].detach()) ** 2)\n    return 0.0\n\n        mar_loss = manifold_attractor_regularization(model)\n        loss = loss + lambda_mar * mar_loss\n        for name, param in model.named_parameters():\n            if \"z\" in name:\n                param.data = torch.clamp(param.data, -clip_val, clip_val)\n\n        # Step optimizer and scheduler\n\n        # === Training Loop (Cleaned & Rebuilt) ===\n        for epoch in range(1, self.n_epochs + 1):\n            self.model.train()\n            T_start = timer()\n\n            dataloader = self.data_set.get_rand_dataloader()\n            for idx, (inp, target) in enumerate(dataloader):\n","output_type":"stream"}],"execution_count":150},{"cell_type":"code","source":"%%bash\nsed -i '120,140d' /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\necho \"âœ… Removed duplicate MAR block above training loop.\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T13:07:21.200128Z","iopub.execute_input":"2025-11-12T13:07:21.200551Z","iopub.status.idle":"2025-11-12T13:07:21.212493Z","shell.execute_reply.started":"2025-11-12T13:07:21.200517Z","shell.execute_reply":"2025-11-12T13:07:21.211858Z"}},"outputs":[{"name":"stdout","text":"âœ… Removed duplicate MAR block above training loop.\n","output_type":"stream"}],"execution_count":152},{"cell_type":"code","source":"!grep -n \"mar_loss\" /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py | head -n 5\n!grep -n \"for epoch\" /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py | head -n 3\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T13:07:43.557230Z","iopub.execute_input":"2025-11-12T13:07:43.557819Z","iopub.status.idle":"2025-11-12T13:07:43.810885Z","shell.execute_reply.started":"2025-11-12T13:07:43.557793Z","shell.execute_reply":"2025-11-12T13:07:43.810204Z"}},"outputs":[{"name":"stdout","text":"105:        mar_loss = manifold_attractor_regularization(model)\n106:        loss = loss + lambda_mar * mar_loss\n85:        for epoch in range(1, self.n_epochs + 1):\n114:        for epoch in range(1, self.n_epochs + 1):\n","output_type":"stream"}],"execution_count":153},{"cell_type":"code","source":"%%bash\nsed -i '100,110d' /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\necho \"âœ… Removed stray mar_loss block causing indentation error (lines 100â€“110).\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T13:08:27.304412Z","iopub.execute_input":"2025-11-12T13:08:27.304753Z","iopub.status.idle":"2025-11-12T13:08:27.315831Z","shell.execute_reply.started":"2025-11-12T13:08:27.304722Z","shell.execute_reply":"2025-11-12T13:08:27.315131Z"}},"outputs":[{"name":"stdout","text":"âœ… Removed stray mar_loss block causing indentation error (lines 100â€“110).\n","output_type":"stream"}],"execution_count":154},{"cell_type":"code","source":"!grep -n \"mar_loss\" /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py | head -n 5\n!grep -n \"for epoch\" /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py | head -n 3\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T13:08:40.166030Z","iopub.execute_input":"2025-11-12T13:08:40.166799Z","iopub.status.idle":"2025-11-12T13:08:40.420069Z","shell.execute_reply.started":"2025-11-12T13:08:40.166775Z","shell.execute_reply":"2025-11-12T13:08:40.419354Z"}},"outputs":[{"name":"stdout","text":"85:        for epoch in range(1, self.n_epochs + 1):\n103:        for epoch in range(1, self.n_epochs + 1):\n","output_type":"stream"}],"execution_count":155},{"cell_type":"code","source":"%%bash\nsed -i '80,100d' /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\necho \"âœ… Removed duplicate early training loop (lines 80â€“100).\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T13:09:10.243457Z","iopub.execute_input":"2025-11-12T13:09:10.244181Z","iopub.status.idle":"2025-11-12T13:09:10.255418Z","shell.execute_reply.started":"2025-11-12T13:09:10.244151Z","shell.execute_reply":"2025-11-12T13:09:10.254827Z"}},"outputs":[{"name":"stdout","text":"âœ… Removed duplicate early training loop (lines 80â€“100).\n","output_type":"stream"}],"execution_count":156},{"cell_type":"code","source":"!grep -n \"for epoch\" /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T13:09:18.347315Z","iopub.execute_input":"2025-11-12T13:09:18.348072Z","iopub.status.idle":"2025-11-12T13:09:18.477005Z","shell.execute_reply.started":"2025-11-12T13:09:18.348048Z","shell.execute_reply":"2025-11-12T13:09:18.476089Z"}},"outputs":[{"name":"stdout","text":"82:        for epoch in range(1, self.n_epochs + 1):\n","output_type":"stream"}],"execution_count":157},{"cell_type":"code","source":"!grep -n \"mar_loss\" /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n!sed -n '100,160p' /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py | head -n 40\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T13:09:32.307055Z","iopub.execute_input":"2025-11-12T13:09:32.307350Z","iopub.status.idle":"2025-11-12T13:09:32.559618Z","shell.execute_reply.started":"2025-11-12T13:09:32.307324Z","shell.execute_reply":"2025-11-12T13:09:32.558797Z"}},"outputs":[],"execution_count":158},{"cell_type":"code","source":"%%bash\nsed -i '/loss = self.compute_loss(pred, target)/a \\\n            # âœ… Apply MAR loss before backprop\\n\\\n            mar_loss = manifold_attractor_regularization(model)\\n\\\n            loss = loss + lambda_mar * mar_loss\\n\\\n            for name, param in model.named_parameters():\\n\\\n                if \"z\" in name:\\n\\\n                    param.data = torch.clamp(param.data, -clip_val, clip_val)\\n' \\\n/kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n\necho \"âœ… Reinserted MAR loss block properly after loss computation.\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T13:10:09.417024Z","iopub.execute_input":"2025-11-12T13:10:09.417755Z","iopub.status.idle":"2025-11-12T13:10:09.429621Z","shell.execute_reply.started":"2025-11-12T13:10:09.417724Z","shell.execute_reply":"2025-11-12T13:10:09.428833Z"}},"outputs":[{"name":"stdout","text":"âœ… Reinserted MAR loss block properly after loss computation.\n","output_type":"stream"}],"execution_count":159},{"cell_type":"code","source":"%%bash\nset -euo pipefail\nROOT=/kaggle/working/dendPLRNN/BPTT_TF\nBP_DIR=/kaggle/working/bptt_backups\nmkdir -p \"$BP_DIR\"\n\necho \"1) Backing up original files...\"\ncp \"$ROOT/bptt/bptt_algorithm.py\" \"$BP_DIR/bptt_algorithm.py.bak.$(date +%s)\"\ncp \"$ROOT/bptt/PLRNN_model.py\" \"$BP_DIR/PLRNN_model.py.bak.$(date +%s)\"\necho \"   backups -> $BP_DIR\"\n\n# ---------------------------------------------------------------------\n# 2) Clean any old novelties\n# ---------------------------------------------------------------------\necho \"2) Cleaning previously injected novelty blocks...\"\n\nsed -i '/Apply Novelties/d' \"$ROOT/bptt/bptt_algorithm.py\" || true\nsed -i '/âœ… Novelty/d' \"$ROOT/bptt/bptt_algorithm.py\" || true\nsed -i '/âœ… Novelty/d' \"$ROOT/bptt/PLRNN_model.py\" || true\n\n# Remove helper definitions by name\nfor pat in hybrid_regularization manifold_attractor_regularization build_cosine_scheduler temporal_self_distillation inject_latent_noise; do\n    sed -i \"/^def $pat/,/^$/d\" \"$ROOT/bptt/bptt_algorithm.py\" || true\ndone\n\nsed -i '/^class DendriticGatingMixin/,/^$/d' \"$ROOT/bptt/PLRNN_model.py\" || true\nsed -i '/manifold_attractor_regularization/d' \"$ROOT/bptt/bptt_algorithm.py\" || true\nsed -i '/Apply MAR loss/d' \"$ROOT/bptt/bptt_algorithm.py\" || true\necho \"   Clean pass done.\"\n\n# ---------------------------------------------------------------------\n# 3) Append three safe novelties\n# ---------------------------------------------------------------------\necho \"3) Appending minimal novelty definitions...\"\n\n# === PLRNN_model.py novelties ===\ncat >> \"$ROOT/bptt/PLRNN_model.py\" <<'PYCODE'\n# --- NOVELTY 1: Dendritic Gating (minimal mixin) ---\nclass DendriticGatingMixin:\n    def __init__(self, *args, **kwargs):\n        try:\n            super().__init__(*args, **kwargs)\n        except Exception:\n            pass\n        self.U = None\n\n    def ensure_U(self):\n        import torch as tc, torch.nn as nn\n        if self.U is None and hasattr(self, \"W\"):\n            self.U = nn.Parameter(tc.randn_like(self.W) * 0.05)\n\n    def dendritic_gate(self, z):\n        import torch as tc\n        self.ensure_U()\n        if self.U is None:\n            return z\n        gate = tc.sigmoid(tc.matmul(z, self.U))\n        return gate * tc.relu(z)\n\n# --- NOVELTY 2: Adaptive Latent Noise ---\ndef inject_latent_noise(z, training=True, noise_std=0.02):\n    import torch as tc\n    if training:\n        return z + tc.randn_like(z) * noise_std\n    return z\nPYCODE\n\n# === bptt_algorithm.py novelties ===\ncat >> \"$ROOT/bptt/bptt_algorithm.py\" <<'PYCODE'\n# --- NOVELTY 3: Manifoldâ€“Attractor Regularization (minimal) ---\ndef manifold_attractor_regularization(model, lambda_mar=0.01):\n    import torch as tc\n    if not hasattr(model, \"A\"):\n        return tc.tensor(0.0, device=next(model.parameters()).device)\n    A = getattr(model, \"A\")\n    W = getattr(model, \"W\", None)\n    h = getattr(model, \"h\", None)\n    Mreg = max(1, int(A.shape[0] * 0.2))\n    A_diag = tc.diag(A)\n    mar_loss = tc.tensor(0.0, device=A.device)\n    for i in range(Mreg):\n        mar_loss += (A_diag[i] - 1.0)**2\n        if h is not None:\n            mar_loss += h[i]**2\n        if W is not None:\n            row = W[i, :]\n            mar_loss += (tc.sum(row**2) - row[i]**2)\n    return lambda_mar * mar_loss\nPYCODE\n\necho \"   Novelties appended cleanly.\"\n\n# ---------------------------------------------------------------------\n# 4) Syntax check\n# ---------------------------------------------------------------------\necho \"4) Running syntax validation...\"\npython -m py_compile \"$ROOT/bptt/bptt_algorithm.py\"\npython -m py_compile \"$ROOT/bptt/PLRNN_model.py\"\necho \"   âœ… Syntax OK.\"\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T13:18:53.565762Z","iopub.execute_input":"2025-11-12T13:18:53.566523Z","iopub.status.idle":"2025-11-12T13:18:53.711433Z","shell.execute_reply.started":"2025-11-12T13:18:53.566482Z","shell.execute_reply":"2025-11-12T13:18:53.710420Z"}},"outputs":[{"name":"stdout","text":"1) Backing up original files...\n   backups -> /kaggle/working/bptt_backups\n2) Cleaning previously injected novelty blocks...\n   Clean pass done.\n3) Appending minimal novelty definitions...\n   Novelties appended cleanly.\n4) Running syntax validation...\n","output_type":"stream"},{"name":"stderr","text":"Sorry: IndentationError: expected an indented block after 'for' statement on line 87 (bptt_algorithm.py, line 88)","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_90/4194781006.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'set -euo pipefail\\nROOT=/kaggle/working/dendPLRNN/BPTT_TF\\nBP_DIR=/kaggle/working/bptt_backups\\nmkdir -p \"$BP_DIR\"\\n\\necho \"1) Backing up original files...\"\\ncp \"$ROOT/bptt/bptt_algorithm.py\" \"$BP_DIR/bptt_algorithm.py.bak.$(date +%s)\"\\ncp \"$ROOT/bptt/PLRNN_model.py\" \"$BP_DIR/PLRNN_model.py.bak.$(date +%s)\"\\necho \"   backups -> $BP_DIR\"\\n\\n# ---------------------------------------------------------------------\\n# 2) Clean any old novelties\\n# ---------------------------------------------------------------------\\necho \"2) Cleaning previously injected novelty blocks...\"\\n\\nsed -i \\'/Apply Novelties/d\\' \"$ROOT/bptt/bptt_algorithm.py\" || true\\nsed -i \\'/âœ… Novelty/d\\' \"$ROOT/bptt/bptt_algorithm.py\" || true\\nsed -i \\'/âœ… Novelty/d\\' \"$ROOT/bptt/PLRNN_model.py\" || true\\n\\n# Remove helper definitions by name\\nfor pat in hybrid_regularization manifold_attractor_regularization build_cosine_scheduler temporal_self_distillation inject_latent_noise; do\\n    sed -i \"/^def $pat/,/^$/d\" \"$ROOT/bptt/bptt_algorithm.py\" || true\\ndone\\n\\nsed -i \\'/^class DendriticGatingMixin/,/^$/d\\' \"$ROOT/bptt/PLRNN_model.py\" || true\\nsed -i \\'/manifold_attractor_regularization/d\\' \"$ROOT/bptt/bptt_algorithm.py\" || true\\nsed -i \\'/Apply MAR loss/d\\' \"$ROOT/bptt/bptt_algorithm.py\" || true\\necho \"   Clean pass done.\"\\n\\n# ---------------------------------------------------------------------\\n# 3) Append three safe novelties\\n# ---------------------------------------------------------------------\\necho \"3) Appending minimal novelty definitions...\"\\n\\n# === PLRNN_model.py novelties ===\\ncat >> \"$ROOT/bptt/PLRNN_model.py\" <<\\'PYCODE\\'\\n# --- NOVELTY 1: Dendritic Gating (minimal mixin) ---\\nclass DendriticGatingMixin:\\n    def __init__(self, *args, **kwargs):\\n        try:\\n            super().__init__(*args, **kwargs)\\n        except Exception:\\n            pass\\n        self.U = None\\n\\n    def ensure_U(self):\\n        import torch as tc, torch.nn as nn\\n        if self.U is None and hasattr(self, \"W\"):\\n            self.U = nn.Parameter(tc.randn_like(self.W) * 0.05)\\n\\n    def dendritic_gate(self, z):\\n        import torch as tc\\n        self.ensure_U()\\n        if self.U is None:\\n            return z\\n        gate = tc.sigmoid(tc.matmul(z, self.U))\\n        return gate * tc.relu(z)\\n\\n# --- NOVELTY 2: Adaptive Latent Noise ---\\ndef inject_latent_noise(z, training=True, noise_std=0.02):\\n    import torch as tc\\n    if training:\\n        return z + tc.randn_like(z) * noise_std\\n    return z\\nPYCODE\\n\\n# === bptt_algorithm.py novelties ===\\ncat >> \"$ROOT/bptt/bptt_algorithm.py\" <<\\'PYCODE\\'\\n# --- NOVELTY 3: Manifoldâ€“Attractor Regularization (minimal) ---\\ndef manifold_attractor_regularization(model, lambda_mar=0.01):\\n    import torch as tc\\n    if not hasattr(model, \"A\"):\\n        return tc.tensor(0.0, device=next(model.parameters()).device)\\n    A = getattr(model, \"A\")\\n    W = getattr(model, \"W\", None)\\n    h = getattr(model, \"h\", None)\\n    Mreg = max(1, int(A.shape[0] * 0.2))\\n    A_diag = tc.diag(A)\\n    mar_loss = tc.tensor(0.0, device=A.device)\\n    for i in range(Mreg):\\n        mar_loss += (A_diag[i] - 1.0)**2\\n        if h is not None:\\n            mar_loss += h[i]**2\\n        if W is not None:\\n            row = W[i, :]\\n            mar_loss += (tc.sum(row**2) - row[i]**2)\\n    return lambda_mar * mar_loss\\nPYCODE\\n\\necho \"   Novelties appended cleanly.\"\\n\\n# ---------------------------------------------------------------------\\n# 4) Syntax check\\n# ---------------------------------------------------------------------\\necho \"4) Running syntax validation...\"\\npython -m py_compile \"$ROOT/bptt/bptt_algorithm.py\"\\npython -m py_compile \"$ROOT/bptt/PLRNN_model.py\"\\necho \"   âœ… Syntax OK.\"\\n\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2471\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2472\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2473\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2474\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<decorator-gen-103>\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'set -euo pipefail\\nROOT=/kaggle/working/dendPLRNN/BPTT_TF\\nBP_DIR=/kaggle/working/bptt_backups\\nmkdir -p \"$BP_DIR\"\\n\\necho \"1) Backing up original files...\"\\ncp \"$ROOT/bptt/bptt_algorithm.py\" \"$BP_DIR/bptt_algorithm.py.bak.$(date +%s)\"\\ncp \"$ROOT/bptt/PLRNN_model.py\" \"$BP_DIR/PLRNN_model.py.bak.$(date +%s)\"\\necho \"   backups -> $BP_DIR\"\\n\\n# ---------------------------------------------------------------------\\n# 2) Clean any old novelties\\n# ---------------------------------------------------------------------\\necho \"2) Cleaning previously injected novelty blocks...\"\\n\\nsed -i \\'/Apply Novelties/d\\' \"$ROOT/bptt/bptt_algorithm.py\" || true\\nsed -i \\'/\\xe2\\x9c\\x85 Novelty/d\\' \"$ROOT/bptt/bptt_algorithm.py\" || true\\nsed -i \\'/\\xe2\\x9c\\x85 Novelty/d\\' \"$ROOT/bptt/PLRNN_model.py\" || true\\n\\n# Remove helper definitions by name\\nfor pat in hybrid_regularization manifold_attractor_regularization build_cosine_scheduler temporal_self_distillation inject_latent_noise; do\\n    sed -i \"/^def $pat/,/^$/d\" \"$ROOT/bptt/bptt_algorithm.py\" || true\\ndone\\n\\nsed -i \\'/^class DendriticGatingMixin/,/^$/d\\' \"$ROOT/bptt/PLRNN_model.py\" || true\\nsed -i \\'/manifold_attractor_regularization/d\\' \"$ROOT/bptt/bptt_algorithm.py\" || true\\nsed -i \\'/Apply MAR loss/d\\' \"$ROOT/bptt/bptt_algorithm.py\" || true\\necho \"   Clean pass done.\"\\n\\n# ---------------------------------------------------------------------\\n# 3) Append three safe novelties\\n# ---------------------------------------------------------------------\\necho \"3) Appending minimal novelty definitions...\"\\n\\n# === PLRNN_model.py novelties ===\\ncat >> \"$ROOT/bptt/PLRNN_model.py\" <<\\'PYCODE\\'\\n# --- NOVELTY 1: Dendritic Gating (minimal mixin) ---\\nclass DendriticGatingMixin:\\n    def __init__(self, *args, **kwargs):\\n        try:\\n            super().__init__(*args, **kwargs)\\n        except Exception:\\n            pass\\n        self.U = None\\n\\n    def ensure_U(self):\\n        import torch as tc, torch.nn as nn\\n        if self.U is None and hasattr(self, \"W\"):\\n            self.U = nn.Parameter(tc.randn_like(self.W) * 0.05)\\n\\n    def dendritic_gate(self, z):\\n        import torch as tc\\n        self.ensure_U()\\n        if self.U is None:\\n            return z\\n        gate = tc.sigmoid(tc.matmul(z, self.U))\\n        return gate * tc.relu(z)\\n\\n# --- NOVELTY 2: Adaptive Latent Noise ---\\ndef inject_latent_noise(z, training=True, noise_std=0.02):\\n    import torch as tc\\n    if training:\\n        return z + tc.randn_like(z) * noise_std\\n    return z\\nPYCODE\\n\\n# === bptt_algorithm.py novelties ===\\ncat >> \"$ROOT/bptt/bptt_algorithm.py\" <<\\'PYCODE\\'\\n# --- NOVELTY 3: Manifold\\xe2\\x80\\x93Attractor Regularization (minimal) ---\\ndef manifold_attractor_regularization(model, lambda_mar=0.01):\\n    import torch as tc\\n    if not hasattr(model, \"A\"):\\n        return tc.tensor(0.0, device=next(model.parameters()).device)\\n    A = getattr(model, \"A\")\\n    W = getattr(model, \"W\", None)\\n    h = getattr(model, \"h\", None)\\n    Mreg = max(1, int(A.shape[0] * 0.2))\\n    A_diag = tc.diag(A)\\n    mar_loss = tc.tensor(0.0, device=A.device)\\n    for i in range(Mreg):\\n        mar_loss += (A_diag[i] - 1.0)**2\\n        if h is not None:\\n            mar_loss += h[i]**2\\n        if W is not None:\\n            row = W[i, :]\\n            mar_loss += (tc.sum(row**2) - row[i]**2)\\n    return lambda_mar * mar_loss\\nPYCODE\\n\\necho \"   Novelties appended cleanly.\"\\n\\n# ---------------------------------------------------------------------\\n# 4) Syntax check\\n# ---------------------------------------------------------------------\\necho \"4) Running syntax validation...\"\\npython -m py_compile \"$ROOT/bptt/bptt_algorithm.py\"\\npython -m py_compile \"$ROOT/bptt/PLRNN_model.py\"\\necho \"   \\xe2\\x9c\\x85 Syntax OK.\"\\n\\n'' returned non-zero exit status 1."],"ename":"CalledProcessError","evalue":"Command 'b'set -euo pipefail\\nROOT=/kaggle/working/dendPLRNN/BPTT_TF\\nBP_DIR=/kaggle/working/bptt_backups\\nmkdir -p \"$BP_DIR\"\\n\\necho \"1) Backing up original files...\"\\ncp \"$ROOT/bptt/bptt_algorithm.py\" \"$BP_DIR/bptt_algorithm.py.bak.$(date +%s)\"\\ncp \"$ROOT/bptt/PLRNN_model.py\" \"$BP_DIR/PLRNN_model.py.bak.$(date +%s)\"\\necho \"   backups -> $BP_DIR\"\\n\\n# ---------------------------------------------------------------------\\n# 2) Clean any old novelties\\n# ---------------------------------------------------------------------\\necho \"2) Cleaning previously injected novelty blocks...\"\\n\\nsed -i \\'/Apply Novelties/d\\' \"$ROOT/bptt/bptt_algorithm.py\" || true\\nsed -i \\'/\\xe2\\x9c\\x85 Novelty/d\\' \"$ROOT/bptt/bptt_algorithm.py\" || true\\nsed -i \\'/\\xe2\\x9c\\x85 Novelty/d\\' \"$ROOT/bptt/PLRNN_model.py\" || true\\n\\n# Remove helper definitions by name\\nfor pat in hybrid_regularization manifold_attractor_regularization build_cosine_scheduler temporal_self_distillation inject_latent_noise; do\\n    sed -i \"/^def $pat/,/^$/d\" \"$ROOT/bptt/bptt_algorithm.py\" || true\\ndone\\n\\nsed -i \\'/^class DendriticGatingMixin/,/^$/d\\' \"$ROOT/bptt/PLRNN_model.py\" || true\\nsed -i \\'/manifold_attractor_regularization/d\\' \"$ROOT/bptt/bptt_algorithm.py\" || true\\nsed -i \\'/Apply MAR loss/d\\' \"$ROOT/bptt/bptt_algorithm.py\" || true\\necho \"   Clean pass done.\"\\n\\n# ---------------------------------------------------------------------\\n# 3) Append three safe novelties\\n# ---------------------------------------------------------------------\\necho \"3) Appending minimal novelty definitions...\"\\n\\n# === PLRNN_model.py novelties ===\\ncat >> \"$ROOT/bptt/PLRNN_model.py\" <<\\'PYCODE\\'\\n# --- NOVELTY 1: Dendritic Gating (minimal mixin) ---\\nclass DendriticGatingMixin:\\n    def __init__(self, *args, **kwargs):\\n        try:\\n            super().__init__(*args, **kwargs)\\n        except Exception:\\n            pass\\n        self.U = None\\n\\n    def ensure_U(self):\\n        import torch as tc, torch.nn as nn\\n        if self.U is None and hasattr(self, \"W\"):\\n            self.U = nn.Parameter(tc.randn_like(self.W) * 0.05)\\n\\n    def dendritic_gate(self, z):\\n        import torch as tc\\n        self.ensure_U()\\n        if self.U is None:\\n            return z\\n        gate = tc.sigmoid(tc.matmul(z, self.U))\\n        return gate * tc.relu(z)\\n\\n# --- NOVELTY 2: Adaptive Latent Noise ---\\ndef inject_latent_noise(z, training=True, noise_std=0.02):\\n    import torch as tc\\n    if training:\\n        return z + tc.randn_like(z) * noise_std\\n    return z\\nPYCODE\\n\\n# === bptt_algorithm.py novelties ===\\ncat >> \"$ROOT/bptt/bptt_algorithm.py\" <<\\'PYCODE\\'\\n# --- NOVELTY 3: Manifold\\xe2\\x80\\x93Attractor Regularization (minimal) ---\\ndef manifold_attractor_regularization(model, lambda_mar=0.01):\\n    import torch as tc\\n    if not hasattr(model, \"A\"):\\n        return tc.tensor(0.0, device=next(model.parameters()).device)\\n    A = getattr(model, \"A\")\\n    W = getattr(model, \"W\", None)\\n    h = getattr(model, \"h\", None)\\n    Mreg = max(1, int(A.shape[0] * 0.2))\\n    A_diag = tc.diag(A)\\n    mar_loss = tc.tensor(0.0, device=A.device)\\n    for i in range(Mreg):\\n        mar_loss += (A_diag[i] - 1.0)**2\\n        if h is not None:\\n            mar_loss += h[i]**2\\n        if W is not None:\\n            row = W[i, :]\\n            mar_loss += (tc.sum(row**2) - row[i]**2)\\n    return lambda_mar * mar_loss\\nPYCODE\\n\\necho \"   Novelties appended cleanly.\"\\n\\n# ---------------------------------------------------------------------\\n# 4) Syntax check\\n# ---------------------------------------------------------------------\\necho \"4) Running syntax validation...\"\\npython -m py_compile \"$ROOT/bptt/bptt_algorithm.py\"\\npython -m py_compile \"$ROOT/bptt/PLRNN_model.py\"\\necho \"   \\xe2\\x9c\\x85 Syntax OK.\"\\n\\n'' returned non-zero exit status 1.","output_type":"error"}],"execution_count":161},{"cell_type":"code","source":"%%bash\nset -euo pipefail\n\nROOT=/kaggle/working/dendPLRNN/BPTT_TF\nBP_DIR=/kaggle/working/bptt_backups\n\necho \"ROOT = $ROOT\"\necho \"BACKUP DIR = $BP_DIR\"\n\n# 1) Find latest backups and restore them (if present)\nLATEST_BPTT_BACKUP=$(ls -1t \"$BP_DIR\"/bptt_algorithm.py.bak.* 2>/dev/null | head -n1 || true)\nLATEST_PLRNN_BACKUP=$(ls -1t \"$BP_DIR\"/PLRNN_model.py.bak.* 2>/dev/null | head -n1 || true)\n\nif [ -n \"$LATEST_BPTT_BACKUP\" ]; then\n  echo \"Restoring bptt_algorithm.py from backup: $LATEST_BPTT_BACKUP\"\n  cp -f \"$LATEST_BPTT_BACKUP\" \"$ROOT/bptt/bptt_algorithm.py\"\nelse\n  echo \"No bptt_algorithm.py backup found in $BP_DIR â€” skipping restore\"\nfi\n\nif [ -n \"$LATEST_PLRNN_BACKUP\" ]; then\n  echo \"Restoring PLRNN_model.py from backup: $LATEST_PLRNN_BACKUP\"\n  cp -f \"$LATEST_PLRNN_BACKUP\" \"$ROOT/bptt/PLRNN_model.py\"\nelse\n  echo \"No PLRNN_model.py backup found in $BP_DIR â€” skipping restore\"\nfi\n\n# 2) Quick sanity dump of the area where earlier error was reported (lines ~60-110)\necho\necho \"---- Showing lines 60..120 of bptt_algorithm.py (for manual inspection) ----\"\nsed -n '60,120p' \"$ROOT/bptt/bptt_algorithm.py\" || true\necho \"-----------------------------------------------------------------------\"\necho\n\n# 3) Try compiling restored files\necho \"Trying to compile restored files...\"\npython -m py_compile \"$ROOT/bptt/bptt_algorithm.py\" || {\n  echo \"ERROR: bptt_algorithm.py did not compile. Showing first 200 lines for debugging:\"\n  sed -n '1,200p' \"$ROOT/bptt/bptt_algorithm.py\"\n  echo \"Aborting â€” please inspect file above (broken indentation likely present).\"\n  exit 1\n}\npython -m py_compile \"$ROOT/bptt/PLRNN_model.py\" || {\n  echo \"ERROR: PLRNN_model.py did not compile. Showing first 200 lines for debugging:\"\n  sed -n '1,200p' \"$ROOT/bptt/PLRNN_model.py\"\n  echo \"Aborting â€” please inspect file above (broken indentation likely present).\"\n  exit 1\n}\necho \"Compilation OK for restored files.\"\n\n# 4) Append safe novelties only if not already present\necho \"Inserting canonical novelties only if missing...\"\n\n# Dendritic gating + inject_latent_noise into PLRNN_model.py\ngrep -q \"class DendriticGatingMixin\" \"$ROOT/bptt/PLRNN_model.py\" || cat >> \"$ROOT/bptt/PLRNN_model.py\" <<'PYCODE'\n\n# --- NOVELTY 1: Dendritic Gating (minimal mixin) ---\nclass DendriticGatingMixin:\n    def __init__(self, *args, **kwargs):\n        try:\n            super().__init__(*args, **kwargs)\n        except Exception:\n            pass\n        self.U = None\n\n    def ensure_U(self):\n        import torch as tc, torch.nn as nn\n        if self.U is None and hasattr(self, \"W\"):\n            self.U = nn.Parameter(tc.randn_like(self.W) * 0.05)\n\n    def dendritic_gate(self, z):\n        import torch as tc\n        self.ensure_U()\n        if self.U is None:\n            return z\n        gate = tc.sigmoid(tc.matmul(z, self.U))\n        return gate * tc.relu(z)\n\n# --- NOVELTY 2: Adaptive Latent Noise ---\ndef inject_latent_noise(z, training=True, noise_std=0.02):\n    import torch as tc\n    if training:\n        return z + tc.randn_like(z) * noise_std\n    return z\nPYCODE\n\n# Manifold-attractor regularization into bptt_algorithm.py\ngrep -q \"def manifold_attractor_regularization\" \"$ROOT/bptt/bptt_algorithm.py\" || cat >> \"$ROOT/bptt/bptt_algorithm.py\" <<'PYCODE'\n\n# --- NOVELTY 3: Manifoldâ€“Attractor Regularization (minimal) ---\ndef manifold_attractor_regularization(model, lambda_mar=0.01):\n    import torch as tc\n    # defensive: if model has no params, return zero tensor on CPU\n    try:\n        device = next(model.parameters()).device\n    except Exception:\n        device = tc.device(\"cpu\")\n    if not hasattr(model, \"A\"):\n        return tc.tensor(0.0, device=device)\n    A = getattr(model, \"A\")\n    W = getattr(model, \"W\", None)\n    h = getattr(model, \"h\", None)\n    Mreg = max(1, int(A.shape[0] * 0.2))\n    A_diag = tc.diag(A)\n    mar_loss = tc.tensor(0.0, device=A.device)\n    for i in range(Mreg):\n        mar_loss = mar_loss + (A_diag[i] - 1.0)**2\n        if h is not None:\n            mar_loss = mar_loss + h[i]**2\n        if W is not None:\n            row = W[i, :]\n            mar_loss = mar_loss + (tc.sum(row**2) - row[i]**2)\n    return lambda_mar * mar_loss\nPYCODE\n\necho \"Insertion done.\"\n\n# 5) Re-run py_compile to be safe\necho \"Re-checking syntax for both files...\"\npython -m py_compile \"$ROOT/bptt/bptt_algorithm.py\" || {\n  echo \"ERROR after insertion: bptt_algorithm.py failed to compile. Showing lines 1..220:\"\n  sed -n '1,220p' \"$ROOT/bptt/bptt_algorithm.py\"\n  exit 1\n}\npython -m py_compile \"$ROOT/bptt/PLRNN_model.py\" || {\n  echo \"ERROR after insertion: PLRNN_model.py failed to compile. Showing lines 1..220:\"\n  sed -n '1,220p' \"$ROOT/bptt/PLRNN_model.py\"\n  exit 1\n}\necho \"All files compile OK now.\"\n\necho \"DONE. If you still see indentation errors, run these two commands to inspect the problematic regions:\"\necho \"  sed -n '1,220p' $ROOT/bptt/bptt_algorithm.py | sed -n '1,220p'\"\necho \"  sed -n '1,220p' $ROOT/bptt/PLRNN_model.py | sed -n '1,220p'\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T13:21:41.216818Z","iopub.execute_input":"2025-11-12T13:21:41.217592Z","iopub.status.idle":"2025-11-12T13:21:41.329388Z","shell.execute_reply.started":"2025-11-12T13:21:41.217566Z","shell.execute_reply":"2025-11-12T13:21:41.328271Z"}},"outputs":[{"name":"stdout","text":"ROOT = /kaggle/working/dendPLRNN/BPTT_TF\nBACKUP DIR = /kaggle/working/bptt_backups\nRestoring bptt_algorithm.py from backup: /kaggle/working/bptt_backups/bptt_algorithm.py.bak.1762953533\nRestoring PLRNN_model.py from backup: /kaggle/working/bptt_backups/PLRNN_model.py.bak.1762953533\n\n---- Showing lines 60..120 of bptt_algorithm.py (for manual inspection) ----\n        self.model.to(self.device)\n        self.data_set.to(self.device)\n        self.regularizer.to(self.device)\n\n    def compute_loss(self, pred: tc.Tensor, target: tc.Tensor) -> tc.Tensor:\n        '''\n        Compute Loss w/ optional MAR loss.\n        '''\n        loss = .0\n        loss += self.loss_fn(pred, target)\n\n        if self.use_reg:\n            lat_model_parameters = self.model.latent_model.get_latent_parameters()\n            loss += self.regularizer.loss(lat_model_parameters)\n\n        return loss\n\n    def train(self):\n        cum_T = 0.\n\n\n        # === Training Loop (Cleaned & Rebuilt) ===\n        for epoch in range(1, self.n_epochs + 1):\n            self.model.train()\n            T_start = timer()\n\n            dataloader = self.data_set.get_rand_dataloader()\n            for idx, (inp, target) in enumerate(dataloader):\n            T_diff = T_end - T_start\n            cum_T += T_diff\n            cum_T_str = str(datetime.timedelta(seconds=cum_T)).split('.')[0]\n\n            print(f\"Epoch {epoch} took {round(T_diff, 2)}s | Cumulative time (h:mm:ss): {cum_T_str} | Loss = {loss.item()}\")\n\n            if epoch % self.save_step == 0:\n                self.saver.epoch_save(self.model, epoch)\n-----------------------------------------------------------------------\n\nTrying to compile restored files...\nERROR: bptt_algorithm.py did not compile. Showing first 200 lines for debugging:\nfrom copy import deepcopy\nimport torch as tc\nfrom torch import optim\nfrom torch import nn\nfrom bptt import models\nfrom bptt import regularization\nfrom bptt import saving\nfrom bptt.dataset import GeneralDataset\nfrom tensorboardX import SummaryWriter\nfrom argparse import Namespace\nfrom timeit import default_timer as timer\nimport datetime\nfrom bptt.tau_estimation import estimate_forcing_interval\n\nclass BPTT:\n    def __init__(self, args: Namespace, data_set: GeneralDataset,\n                 writer: SummaryWriter, save_path: str, device: tc.device):\n        # dataset, model, device, regularizer\n        self.device = device\n        self.data_set = data_set\n        self.model = models.Model(args, data_set)\n        self.regularizer = regularization.Regularizer(args)\n        self.to_device()\n\n        # estimate forcing interval\n        if args.estimate_forcing:\n            print(f\"Estimating forcing interval...\")\n            tau_ac = estimate_forcing_interval(data_set.data.cpu().numpy(),\n                                               False, mode=\"ACORR\")[0]\n            tau_mi = estimate_forcing_interval(data_set.data.cpu().numpy(),\n                                               False, mode=\"MI\")[0]\n            mn = min(tau_ac, tau_mi)\n            print(f\"Estimated forcing interval: min(AC {tau_ac}, MI {tau_mi}) ---> {mn}\")\n            self.tau = mn\n        else:\n            print(f\"Forcing interval set by user: {args.teacher_forcing_interval}\")\n            self.tau = args.teacher_forcing_interval\n\n        # optimizer\n        self.optimizer = optim.RAdam(self.model.parameters(), args.learning_rate)\n        \n        # others\n        self.n_epochs = args.n_epochs\n        self.gradient_clipping = args.gradient_clipping\n        self.writer = writer\n        self.use_reg = args.use_reg\n        self.saver = saving.Saver(writer, save_path, args, self.data_set, self.regularizer)\n        self.save_step = args.save_step\n        self.loss_fn = nn.MSELoss()\n        self.noise_level = args.noise_level\n\n        # scheduler\n        e = args.n_epochs\n        self.scheduler = optim.lr_scheduler.MultiStepLR(self.optimizer, [int(0.1*e), int(0.8*e), int(0.9*e)], 0.1)\n\n    def to_device(self) -> None:\n        '''\n        Moves members to computing device.\n        '''\n        self.model.to(self.device)\n        self.data_set.to(self.device)\n        self.regularizer.to(self.device)\n\n    def compute_loss(self, pred: tc.Tensor, target: tc.Tensor) -> tc.Tensor:\n        '''\n        Compute Loss w/ optional MAR loss.\n        '''\n        loss = .0\n        loss += self.loss_fn(pred, target)\n\n        if self.use_reg:\n            lat_model_parameters = self.model.latent_model.get_latent_parameters()\n            loss += self.regularizer.loss(lat_model_parameters)\n\n        return loss\n\n    def train(self):\n        cum_T = 0.\n\n\n        # === Training Loop (Cleaned & Rebuilt) ===\n        for epoch in range(1, self.n_epochs + 1):\n            self.model.train()\n            T_start = timer()\n\n            dataloader = self.data_set.get_rand_dataloader()\n            for idx, (inp, target) in enumerate(dataloader):\n            T_diff = T_end - T_start\n            cum_T += T_diff\n            cum_T_str = str(datetime.timedelta(seconds=cum_T)).split('.')[0]\n\n            print(f\"Epoch {epoch} took {round(T_diff, 2)}s | Cumulative time (h:mm:ss): {cum_T_str} | Loss = {loss.item()}\")\n\n            if epoch % self.save_step == 0:\n                self.saver.epoch_save(self.model, epoch)\nAborting â€” please inspect file above (broken indentation likely present).\n","output_type":"stream"},{"name":"stderr","text":"Sorry: IndentationError: expected an indented block after 'for' statement on line 87 (bptt_algorithm.py, line 88)","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_90/470870364.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'set -euo pipefail\\n\\nROOT=/kaggle/working/dendPLRNN/BPTT_TF\\nBP_DIR=/kaggle/working/bptt_backups\\n\\necho \"ROOT = $ROOT\"\\necho \"BACKUP DIR = $BP_DIR\"\\n\\n# 1) Find latest backups and restore them (if present)\\nLATEST_BPTT_BACKUP=$(ls -1t \"$BP_DIR\"/bptt_algorithm.py.bak.* 2>/dev/null | head -n1 || true)\\nLATEST_PLRNN_BACKUP=$(ls -1t \"$BP_DIR\"/PLRNN_model.py.bak.* 2>/dev/null | head -n1 || true)\\n\\nif [ -n \"$LATEST_BPTT_BACKUP\" ]; then\\n  echo \"Restoring bptt_algorithm.py from backup: $LATEST_BPTT_BACKUP\"\\n  cp -f \"$LATEST_BPTT_BACKUP\" \"$ROOT/bptt/bptt_algorithm.py\"\\nelse\\n  echo \"No bptt_algorithm.py backup found in $BP_DIR â€” skipping restore\"\\nfi\\n\\nif [ -n \"$LATEST_PLRNN_BACKUP\" ]; then\\n  echo \"Restoring PLRNN_model.py from backup: $LATEST_PLRNN_BACKUP\"\\n  cp -f \"$LATEST_PLRNN_BACKUP\" \"$ROOT/bptt/PLRNN_model.py\"\\nelse\\n  echo \"No PLRNN_model.py backup found in $BP_DIR â€” skipping restore\"\\nfi\\n\\n# 2) Quick sanity dump of the area where earlier error was reported (lines ~60-110)\\necho\\necho \"---- Showing lines 60..120 of bptt_algorithm.py (for manual inspection) ----\"\\nsed -n \\'60,120p\\' \"$ROOT/bptt/bptt_algorithm.py\" || true\\necho \"-----------------------------------------------------------------------\"\\necho\\n\\n# 3) Try compiling restored files\\necho \"Trying to compile restored files...\"\\npython -m py_compile \"$ROOT/bptt/bptt_algorithm.py\" || {\\n  echo \"ERROR: bptt_algorithm.py did not compile. Showing first 200 lines for debugging:\"\\n  sed -n \\'1,200p\\' \"$ROOT/bptt/bptt_algorithm.py\"\\n  echo \"Aborting â€” please inspect file above (broken indentation likely present).\"\\n  exit 1\\n}\\npython -m py_compile \"$ROOT/bptt/PLRNN_model.py\" || {\\n  echo \"ERROR: PLRNN_model.py did not compile. Showing first 200 lines for debugging:\"\\n  sed -n \\'1,200p\\' \"$ROOT/bptt/PLRNN_model.py\"\\n  echo \"Aborting â€” please inspect file above (broken indentation likely present).\"\\n  exit 1\\n}\\necho \"Compilation OK for restored files.\"\\n\\n# 4) Append safe novelties only if not already present\\necho \"Inserting canonical novelties only if missing...\"\\n\\n# Dendritic gating + inject_latent_noise into PLRNN_model.py\\ngrep -q \"class DendriticGatingMixin\" \"$ROOT/bptt/PLRNN_model.py\" || cat >> \"$ROOT/bptt/PLRNN_model.py\" <<\\'PYCODE\\'\\n\\n# --- NOVELTY 1: Dendritic Gating (minimal mixin) ---\\nclass DendriticGatingMixin:\\n    def __init__(self, *args, **kwargs):\\n        try:\\n            super().__init__(*args, **kwargs)\\n        except Exception:\\n            pass\\n        self.U = None\\n\\n    def ensure_U(self):\\n        import torch as tc, torch.nn as nn\\n        if self.U is None and hasattr(self, \"W\"):\\n            self.U = nn.Parameter(tc.randn_like(self.W) * 0.05)\\n\\n    def dendritic_gate(self, z):\\n        import torch as tc\\n        self.ensure_U()\\n        if self.U is None:\\n            return z\\n        gate = tc.sigmoid(tc.matmul(z, self.U))\\n        return gate * tc.relu(z)\\n\\n# --- NOVELTY 2: Adaptive Latent Noise ---\\ndef inject_latent_noise(z, training=True, noise_std=0.02):\\n    import torch as tc\\n    if training:\\n        return z + tc.randn_like(z) * noise_std\\n    return z\\nPYCODE\\n\\n# Manifold-attractor regularization into bptt_algorithm.py\\ngrep -q \"def manifold_attractor_regularization\" \"$ROOT/bptt/bptt_algorithm.py\" || cat >> \"$ROOT/bptt/bptt_algorithm.py\" <<\\'PYCODE\\'\\n\\n# --- NOVELTY 3: Manifoldâ€“Attractor Regularization (minimal) ---\\ndef manifold_attractor_regularization(model, lambda_mar=0.01):\\n    import torch as tc\\n    # defensive: if model has no params, return zero tensor on CPU\\n    try:\\n        device = next(model.parameters()).device\\n    except Exception:\\n        device = tc.device(\"cpu\")\\n    if not hasattr(model, \"A\"):\\n        return tc.tensor(0.0, device=device)\\n    A = getattr(model, \"A\")\\n    W = getattr(model, \"W\", None)\\n    h = getattr(model, \"h\", None)\\n    Mreg = max(1, int(A.shape[0] * 0.2))\\n    A_diag = tc.diag(A)\\n    mar_loss = tc.tensor(0.0, device=A.device)\\n    for i in range(Mreg):\\n        mar_loss = mar_loss + (A_diag[i] - 1.0)**2\\n        if h is not None:\\n            mar_loss = mar_loss + h[i]**2\\n        if W is not None:\\n            row = W[i, :]\\n            mar_loss = mar_loss + (tc.sum(row**2) - row[i]**2)\\n    return lambda_mar * mar_loss\\nPYCODE\\n\\necho \"Insertion done.\"\\n\\n# 5) Re-run py_compile to be safe\\necho \"Re-checking syntax for both files...\"\\npython -m py_compile \"$ROOT/bptt/bptt_algorithm.py\" || {\\n  echo \"ERROR after insertion: bptt_algorithm.py failed to compile. Showing lines 1..220:\"\\n  sed -n \\'1,220p\\' \"$ROOT/bptt/bptt_algorithm.py\"\\n  exit 1\\n}\\npython -m py_compile \"$ROOT/bptt/PLRNN_model.py\" || {\\n  echo \"ERROR after insertion: PLRNN_model.py failed to compile. Showing lines 1..220:\"\\n  sed -n \\'1,220p\\' \"$ROOT/bptt/PLRNN_model.py\"\\n  exit 1\\n}\\necho \"All files compile OK now.\"\\n\\necho \"DONE. If you still see indentation errors, run these two commands to inspect the problematic regions:\"\\necho \"  sed -n \\'1,220p\\' $ROOT/bptt/bptt_algorithm.py | sed -n \\'1,220p\\'\"\\necho \"  sed -n \\'1,220p\\' $ROOT/bptt/PLRNN_model.py | sed -n \\'1,220p\\'\"\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2471\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2472\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2473\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2474\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<decorator-gen-103>\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'set -euo pipefail\\n\\nROOT=/kaggle/working/dendPLRNN/BPTT_TF\\nBP_DIR=/kaggle/working/bptt_backups\\n\\necho \"ROOT = $ROOT\"\\necho \"BACKUP DIR = $BP_DIR\"\\n\\n# 1) Find latest backups and restore them (if present)\\nLATEST_BPTT_BACKUP=$(ls -1t \"$BP_DIR\"/bptt_algorithm.py.bak.* 2>/dev/null | head -n1 || true)\\nLATEST_PLRNN_BACKUP=$(ls -1t \"$BP_DIR\"/PLRNN_model.py.bak.* 2>/dev/null | head -n1 || true)\\n\\nif [ -n \"$LATEST_BPTT_BACKUP\" ]; then\\n  echo \"Restoring bptt_algorithm.py from backup: $LATEST_BPTT_BACKUP\"\\n  cp -f \"$LATEST_BPTT_BACKUP\" \"$ROOT/bptt/bptt_algorithm.py\"\\nelse\\n  echo \"No bptt_algorithm.py backup found in $BP_DIR \\xe2\\x80\\x94 skipping restore\"\\nfi\\n\\nif [ -n \"$LATEST_PLRNN_BACKUP\" ]; then\\n  echo \"Restoring PLRNN_model.py from backup: $LATEST_PLRNN_BACKUP\"\\n  cp -f \"$LATEST_PLRNN_BACKUP\" \"$ROOT/bptt/PLRNN_model.py\"\\nelse\\n  echo \"No PLRNN_model.py backup found in $BP_DIR \\xe2\\x80\\x94 skipping restore\"\\nfi\\n\\n# 2) Quick sanity dump of the area where earlier error was reported (lines ~60-110)\\necho\\necho \"---- Showing lines 60..120 of bptt_algorithm.py (for manual inspection) ----\"\\nsed -n \\'60,120p\\' \"$ROOT/bptt/bptt_algorithm.py\" || true\\necho \"-----------------------------------------------------------------------\"\\necho\\n\\n# 3) Try compiling restored files\\necho \"Trying to compile restored files...\"\\npython -m py_compile \"$ROOT/bptt/bptt_algorithm.py\" || {\\n  echo \"ERROR: bptt_algorithm.py did not compile. Showing first 200 lines for debugging:\"\\n  sed -n \\'1,200p\\' \"$ROOT/bptt/bptt_algorithm.py\"\\n  echo \"Aborting \\xe2\\x80\\x94 please inspect file above (broken indentation likely present).\"\\n  exit 1\\n}\\npython -m py_compile \"$ROOT/bptt/PLRNN_model.py\" || {\\n  echo \"ERROR: PLRNN_model.py did not compile. Showing first 200 lines for debugging:\"\\n  sed -n \\'1,200p\\' \"$ROOT/bptt/PLRNN_model.py\"\\n  echo \"Aborting \\xe2\\x80\\x94 please inspect file above (broken indentation likely present).\"\\n  exit 1\\n}\\necho \"Compilation OK for restored files.\"\\n\\n# 4) Append safe novelties only if not already present\\necho \"Inserting canonical novelties only if missing...\"\\n\\n# Dendritic gating + inject_latent_noise into PLRNN_model.py\\ngrep -q \"class DendriticGatingMixin\" \"$ROOT/bptt/PLRNN_model.py\" || cat >> \"$ROOT/bptt/PLRNN_model.py\" <<\\'PYCODE\\'\\n\\n# --- NOVELTY 1: Dendritic Gating (minimal mixin) ---\\nclass DendriticGatingMixin:\\n    def __init__(self, *args, **kwargs):\\n        try:\\n            super().__init__(*args, **kwargs)\\n        except Exception:\\n            pass\\n        self.U = None\\n\\n    def ensure_U(self):\\n        import torch as tc, torch.nn as nn\\n        if self.U is None and hasattr(self, \"W\"):\\n            self.U = nn.Parameter(tc.randn_like(self.W) * 0.05)\\n\\n    def dendritic_gate(self, z):\\n        import torch as tc\\n        self.ensure_U()\\n        if self.U is None:\\n            return z\\n        gate = tc.sigmoid(tc.matmul(z, self.U))\\n        return gate * tc.relu(z)\\n\\n# --- NOVELTY 2: Adaptive Latent Noise ---\\ndef inject_latent_noise(z, training=True, noise_std=0.02):\\n    import torch as tc\\n    if training:\\n        return z + tc.randn_like(z) * noise_std\\n    return z\\nPYCODE\\n\\n# Manifold-attractor regularization into bptt_algorithm.py\\ngrep -q \"def manifold_attractor_regularization\" \"$ROOT/bptt/bptt_algorithm.py\" || cat >> \"$ROOT/bptt/bptt_algorithm.py\" <<\\'PYCODE\\'\\n\\n# --- NOVELTY 3: Manifold\\xe2\\x80\\x93Attractor Regularization (minimal) ---\\ndef manifold_attractor_regularization(model, lambda_mar=0.01):\\n    import torch as tc\\n    # defensive: if model has no params, return zero tensor on CPU\\n    try:\\n        device = next(model.parameters()).device\\n    except Exception:\\n        device = tc.device(\"cpu\")\\n    if not hasattr(model, \"A\"):\\n        return tc.tensor(0.0, device=device)\\n    A = getattr(model, \"A\")\\n    W = getattr(model, \"W\", None)\\n    h = getattr(model, \"h\", None)\\n    Mreg = max(1, int(A.shape[0] * 0.2))\\n    A_diag = tc.diag(A)\\n    mar_loss = tc.tensor(0.0, device=A.device)\\n    for i in range(Mreg):\\n        mar_loss = mar_loss + (A_diag[i] - 1.0)**2\\n        if h is not None:\\n            mar_loss = mar_loss + h[i]**2\\n        if W is not None:\\n            row = W[i, :]\\n            mar_loss = mar_loss + (tc.sum(row**2) - row[i]**2)\\n    return lambda_mar * mar_loss\\nPYCODE\\n\\necho \"Insertion done.\"\\n\\n# 5) Re-run py_compile to be safe\\necho \"Re-checking syntax for both files...\"\\npython -m py_compile \"$ROOT/bptt/bptt_algorithm.py\" || {\\n  echo \"ERROR after insertion: bptt_algorithm.py failed to compile. Showing lines 1..220:\"\\n  sed -n \\'1,220p\\' \"$ROOT/bptt/bptt_algorithm.py\"\\n  exit 1\\n}\\npython -m py_compile \"$ROOT/bptt/PLRNN_model.py\" || {\\n  echo \"ERROR after insertion: PLRNN_model.py failed to compile. Showing lines 1..220:\"\\n  sed -n \\'1,220p\\' \"$ROOT/bptt/PLRNN_model.py\"\\n  exit 1\\n}\\necho \"All files compile OK now.\"\\n\\necho \"DONE. If you still see indentation errors, run these two commands to inspect the problematic regions:\"\\necho \"  sed -n \\'1,220p\\' $ROOT/bptt/bptt_algorithm.py | sed -n \\'1,220p\\'\"\\necho \"  sed -n \\'1,220p\\' $ROOT/bptt/PLRNN_model.py | sed -n \\'1,220p\\'\"\\n'' returned non-zero exit status 1."],"ename":"CalledProcessError","evalue":"Command 'b'set -euo pipefail\\n\\nROOT=/kaggle/working/dendPLRNN/BPTT_TF\\nBP_DIR=/kaggle/working/bptt_backups\\n\\necho \"ROOT = $ROOT\"\\necho \"BACKUP DIR = $BP_DIR\"\\n\\n# 1) Find latest backups and restore them (if present)\\nLATEST_BPTT_BACKUP=$(ls -1t \"$BP_DIR\"/bptt_algorithm.py.bak.* 2>/dev/null | head -n1 || true)\\nLATEST_PLRNN_BACKUP=$(ls -1t \"$BP_DIR\"/PLRNN_model.py.bak.* 2>/dev/null | head -n1 || true)\\n\\nif [ -n \"$LATEST_BPTT_BACKUP\" ]; then\\n  echo \"Restoring bptt_algorithm.py from backup: $LATEST_BPTT_BACKUP\"\\n  cp -f \"$LATEST_BPTT_BACKUP\" \"$ROOT/bptt/bptt_algorithm.py\"\\nelse\\n  echo \"No bptt_algorithm.py backup found in $BP_DIR \\xe2\\x80\\x94 skipping restore\"\\nfi\\n\\nif [ -n \"$LATEST_PLRNN_BACKUP\" ]; then\\n  echo \"Restoring PLRNN_model.py from backup: $LATEST_PLRNN_BACKUP\"\\n  cp -f \"$LATEST_PLRNN_BACKUP\" \"$ROOT/bptt/PLRNN_model.py\"\\nelse\\n  echo \"No PLRNN_model.py backup found in $BP_DIR \\xe2\\x80\\x94 skipping restore\"\\nfi\\n\\n# 2) Quick sanity dump of the area where earlier error was reported (lines ~60-110)\\necho\\necho \"---- Showing lines 60..120 of bptt_algorithm.py (for manual inspection) ----\"\\nsed -n \\'60,120p\\' \"$ROOT/bptt/bptt_algorithm.py\" || true\\necho \"-----------------------------------------------------------------------\"\\necho\\n\\n# 3) Try compiling restored files\\necho \"Trying to compile restored files...\"\\npython -m py_compile \"$ROOT/bptt/bptt_algorithm.py\" || {\\n  echo \"ERROR: bptt_algorithm.py did not compile. Showing first 200 lines for debugging:\"\\n  sed -n \\'1,200p\\' \"$ROOT/bptt/bptt_algorithm.py\"\\n  echo \"Aborting \\xe2\\x80\\x94 please inspect file above (broken indentation likely present).\"\\n  exit 1\\n}\\npython -m py_compile \"$ROOT/bptt/PLRNN_model.py\" || {\\n  echo \"ERROR: PLRNN_model.py did not compile. Showing first 200 lines for debugging:\"\\n  sed -n \\'1,200p\\' \"$ROOT/bptt/PLRNN_model.py\"\\n  echo \"Aborting \\xe2\\x80\\x94 please inspect file above (broken indentation likely present).\"\\n  exit 1\\n}\\necho \"Compilation OK for restored files.\"\\n\\n# 4) Append safe novelties only if not already present\\necho \"Inserting canonical novelties only if missing...\"\\n\\n# Dendritic gating + inject_latent_noise into PLRNN_model.py\\ngrep -q \"class DendriticGatingMixin\" \"$ROOT/bptt/PLRNN_model.py\" || cat >> \"$ROOT/bptt/PLRNN_model.py\" <<\\'PYCODE\\'\\n\\n# --- NOVELTY 1: Dendritic Gating (minimal mixin) ---\\nclass DendriticGatingMixin:\\n    def __init__(self, *args, **kwargs):\\n        try:\\n            super().__init__(*args, **kwargs)\\n        except Exception:\\n            pass\\n        self.U = None\\n\\n    def ensure_U(self):\\n        import torch as tc, torch.nn as nn\\n        if self.U is None and hasattr(self, \"W\"):\\n            self.U = nn.Parameter(tc.randn_like(self.W) * 0.05)\\n\\n    def dendritic_gate(self, z):\\n        import torch as tc\\n        self.ensure_U()\\n        if self.U is None:\\n            return z\\n        gate = tc.sigmoid(tc.matmul(z, self.U))\\n        return gate * tc.relu(z)\\n\\n# --- NOVELTY 2: Adaptive Latent Noise ---\\ndef inject_latent_noise(z, training=True, noise_std=0.02):\\n    import torch as tc\\n    if training:\\n        return z + tc.randn_like(z) * noise_std\\n    return z\\nPYCODE\\n\\n# Manifold-attractor regularization into bptt_algorithm.py\\ngrep -q \"def manifold_attractor_regularization\" \"$ROOT/bptt/bptt_algorithm.py\" || cat >> \"$ROOT/bptt/bptt_algorithm.py\" <<\\'PYCODE\\'\\n\\n# --- NOVELTY 3: Manifold\\xe2\\x80\\x93Attractor Regularization (minimal) ---\\ndef manifold_attractor_regularization(model, lambda_mar=0.01):\\n    import torch as tc\\n    # defensive: if model has no params, return zero tensor on CPU\\n    try:\\n        device = next(model.parameters()).device\\n    except Exception:\\n        device = tc.device(\"cpu\")\\n    if not hasattr(model, \"A\"):\\n        return tc.tensor(0.0, device=device)\\n    A = getattr(model, \"A\")\\n    W = getattr(model, \"W\", None)\\n    h = getattr(model, \"h\", None)\\n    Mreg = max(1, int(A.shape[0] * 0.2))\\n    A_diag = tc.diag(A)\\n    mar_loss = tc.tensor(0.0, device=A.device)\\n    for i in range(Mreg):\\n        mar_loss = mar_loss + (A_diag[i] - 1.0)**2\\n        if h is not None:\\n            mar_loss = mar_loss + h[i]**2\\n        if W is not None:\\n            row = W[i, :]\\n            mar_loss = mar_loss + (tc.sum(row**2) - row[i]**2)\\n    return lambda_mar * mar_loss\\nPYCODE\\n\\necho \"Insertion done.\"\\n\\n# 5) Re-run py_compile to be safe\\necho \"Re-checking syntax for both files...\"\\npython -m py_compile \"$ROOT/bptt/bptt_algorithm.py\" || {\\n  echo \"ERROR after insertion: bptt_algorithm.py failed to compile. Showing lines 1..220:\"\\n  sed -n \\'1,220p\\' \"$ROOT/bptt/bptt_algorithm.py\"\\n  exit 1\\n}\\npython -m py_compile \"$ROOT/bptt/PLRNN_model.py\" || {\\n  echo \"ERROR after insertion: PLRNN_model.py failed to compile. Showing lines 1..220:\"\\n  sed -n \\'1,220p\\' \"$ROOT/bptt/PLRNN_model.py\"\\n  exit 1\\n}\\necho \"All files compile OK now.\"\\n\\necho \"DONE. If you still see indentation errors, run these two commands to inspect the problematic regions:\"\\necho \"  sed -n \\'1,220p\\' $ROOT/bptt/bptt_algorithm.py | sed -n \\'1,220p\\'\"\\necho \"  sed -n \\'1,220p\\' $ROOT/bptt/PLRNN_model.py | sed -n \\'1,220p\\'\"\\n'' returned non-zero exit status 1.","output_type":"error"}],"execution_count":162},{"cell_type":"code","source":"# ---------------------------------------------------------------------\n# 5) Run 10-epoch training\n# ---------------------------------------------------------------------\necho \"5) Starting 10-epoch training...\"\ncd \"$ROOT\"\nPYTHONPATH=\"$ROOT:$ROOT/..\" python Experiments/Table1/ECG/ubermain.py --n_epochs 10 > training_log.txt 2>&1 || true\n\ntail -n 30 training_log.txt || true\n\nLATEST_MODEL=$(find results -type f -name \"*.pt\" | sort | tail -n 1 || true)\nif [ -f \"$LATEST_MODEL\" ]; then\n  cp \"$LATEST_MODEL\" /kaggle/working/final_trained_model.pt\n  echo \"ðŸ’¾ Model copied to /kaggle/working/final_trained_model.pt\"\nelse\n  echo \"âš ï¸ No .pt checkpoint found â€” check training_log.txt\"\nfi\n\necho \"ðŸŽ¯ Script done.\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!grep -n \"mar_loss\" /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T13:10:19.103056Z","iopub.execute_input":"2025-11-12T13:10:19.103537Z","iopub.status.idle":"2025-11-12T13:10:19.231724Z","shell.execute_reply.started":"2025-11-12T13:10:19.103508Z","shell.execute_reply":"2025-11-12T13:10:19.230892Z"}},"outputs":[],"execution_count":160},{"cell_type":"code","source":"!python -m py_compile /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T13:05:59.842653Z","iopub.execute_input":"2025-11-12T13:05:59.842953Z","iopub.status.idle":"2025-11-12T13:06:00.045812Z","shell.execute_reply.started":"2025-11-12T13:05:59.842925Z","shell.execute_reply":"2025-11-12T13:06:00.044915Z"}},"outputs":[{"name":"stdout","text":"Sorry: IndentationError: unexpected indent (bptt_algorithm.py, line 105)","output_type":"stream"}],"execution_count":151},{"cell_type":"code","source":"!python -m py_compile /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:58:14.197667Z","iopub.execute_input":"2025-11-12T12:58:14.198449Z","iopub.status.idle":"2025-11-12T12:58:14.400288Z","shell.execute_reply.started":"2025-11-12T12:58:14.198416Z","shell.execute_reply":"2025-11-12T12:58:14.399507Z"}},"outputs":[{"name":"stdout","text":"Sorry: IndentationError: expected an indented block after 'if' statement on line 123 (bptt_algorithm.py, line 124)","output_type":"stream"}],"execution_count":138},{"cell_type":"code","source":"%%bash\ncd /kaggle/working/dendPLRNN/BPTT_TF\n\n# Run training with novelties enabled for 10 epochs\necho \"ðŸš€ Starting 10-epoch training with all 5 novelties...\"\nPYTHONPATH=\"/kaggle/working/dendPLRNN/BPTT_TF:/kaggle/working/dendPLRNN\" \\\npython Experiments/Table1/ECG/ubermain.py > training_log.txt 2>&1\n\n# Extract summary for quick review\necho \"âœ… Training completed. Showing last 20 lines of training log:\"\ntail -n 20 training_log.txt\n\n# Save trained model checkpoint for download\nLATEST_MODEL=$(find results/ECG -type f -name \"*.pt\" | sort | tail -n 1)\nif [ -f \"$LATEST_MODEL\" ]; then\n  cp \"$LATEST_MODEL\" /kaggle/working/final_trained_model.pt\n  echo \"ðŸ’¾ Model saved to /kaggle/working/final_trained_model.pt\"\nelse\n  echo \"âš ï¸ No model file found â€” please verify training checkpoint saving.\"\nfi\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:47:02.239194Z","iopub.execute_input":"2025-11-12T12:47:02.239539Z","iopub.status.idle":"2025-11-12T12:47:21.374432Z","shell.execute_reply.started":"2025-11-12T12:47:02.239509Z","shell.execute_reply":"2025-11-12T12:47:21.373660Z"}},"outputs":[{"name":"stdout","text":"ðŸš€ Starting 10-epoch training with all 5 novelties...\nâœ… Training completed. Showing last 20 lines of training log:\n'use_gpu' flag is set.\nWill distribute tasks to GPUs automatically.\nThere are not enough GPU Resources available to spawn 20 processes. Reducing number of parallel runs to 1\nTraceback (most recent call last):\n  File \"/kaggle/working/dendPLRNN/BPTT_TF/main.py\", line 5, in <module>\n    from bptt import bptt_algorithm\n  File \"/kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\", line 106\n    mar_loss = manifold_attractor_regularization(model)\nIndentationError: expected an indented block after 'if' statement on line 104\nâš ï¸ No model file found â€” please verify training checkpoint saving.\n","output_type":"stream"}],"execution_count":120},{"cell_type":"code","source":"import re\nimport matplotlib.pyplot as plt\n\n# Read log file\nlog_path = \"/kaggle/working/dendPLRNN/BPTT_TF/training_log.txt\"\nwith open(log_path, \"r\") as f:\n    text = f.read()\n\n# Extract epoch, loss, and metrics\nepochs = [int(x) for x in re.findall(r\"Epoch (\\d+)\", text)]\nlosses = [float(x) for x in re.findall(r\"Loss = ([0-9.]+)\", text)]\nmse1 = [float(x) for x in re.findall(r\"MSE-1 ([0-9.]+)\", text)]\npse = [float(x) for x in re.findall(r\"PSE ([0-9.]+)\", text)]\nklx = [float(x) for x in re.findall(r\"KLx ([0-9.]+)\", text)]\n\n# Plot training metrics\nplt.figure(figsize=(12, 8))\nplt.subplot(2, 2, 1)\nplt.plot(epochs, losses, label=\"Training Loss\")\nplt.title(\"Training Loss\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.legend()\n\nplt.subplot(2, 2, 2)\nplt.plot(epochs, mse1, label=\"MSE-1\")\nplt.title(\"MSE-1\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Error\"); plt.legend()\n\nplt.subplot(2, 2, 3)\nplt.plot(epochs, pse, label=\"PSE\", color=\"orange\")\nplt.title(\"Prediction Smoothness Error (PSE)\"); plt.xlabel(\"Epoch\"); plt.legend()\n\nplt.subplot(2, 2, 4)\nplt.plot(epochs, klx, label=\"KLx-GMM\", color=\"red\")\nplt.title(\"KLx-GMM Divergence\"); plt.xlabel(\"Epoch\"); plt.legend()\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:47:21.375732Z","iopub.execute_input":"2025-11-12T12:47:21.376060Z","iopub.status.idle":"2025-11-12T12:47:21.945063Z","shell.execute_reply.started":"2025-11-12T12:47:21.376040Z","shell.execute_reply":"2025-11-12T12:47:21.944392Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1200x800 with 4 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":121},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink('/kaggle/working/final_trained_model.pt')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%cd /kaggle/working/dendPLRNN/BPTT_TF\n!PYTHONPATH=\"/kaggle/working/dendPLRNN/BPTT_TF:/kaggle/working/dendPLRNN\" \\\npython Experiments/Table1/ECG/ubermain.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:36:52.185792Z","iopub.execute_input":"2025-11-12T12:36:52.186605Z","iopub.status.idle":"2025-11-12T12:37:11.504099Z","shell.execute_reply.started":"2025-11-12T12:36:52.186572Z","shell.execute_reply":"2025-11-12T12:37:11.503356Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/dendPLRNN/BPTT_TF\n'use_gpu' flag is set.\nWill distribute tasks to GPUs automatically.\nThere are not enough GPU Resources available to spawn 20 processes. Reducing number of parallel runs to 1\nTraceback (most recent call last):\n  File \"/kaggle/working/dendPLRNN/BPTT_TF/main.py\", line 5, in <module>\n    from bptt import bptt_algorithm\n  File \"/kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\", line 104\n    if \"z\" in name:\n    ^\nIndentationError: expected an indented block after 'for' statement on line 103\n","output_type":"stream"}],"execution_count":105},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%bash\nsed -i '/loss = criterion(pred, target)/a \\\n        # === Apply Novelties ===\\n\\\n        mar_loss = manifold_attractor_regularization(self.model, lambda_mar=0.01)\\n\\\n        distill_loss = temporal_self_distillation(z, weight=0.05)\\n\\\n        loss = hybrid_regularization(loss, z, z_prev=None, klx=getattr(self, \\\"KLx\\\", None))\\n\\\n        loss = loss + mar_loss + distill_loss\\n\\\n        # Inject adaptive latent noise\\n\\\n        z = inject_latent_noise(z, training=self.model.training, noise_std=0.02)\\n' \\\n/kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%bash\nsed -i '/optimizer = torch.optim.Adam/a \\\n    # âœ… Attach cosine scheduler (Novelty 3)\\n\\\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.n_epochs)\\n' \\\n/kaggle/working/dendPLRNN/BPTT_TF/main.py\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%bash\nsed -i '/for epoch in range(args.n_epochs):/a \\\n        scheduler.step()\\n\\\n        print(f\\\"[Scheduler] Current LR: {scheduler.get_last_lr()[0]:.6f}\\\")\\n' \\\n/kaggle/working/dendPLRNN/BPTT_TF/main.py\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%bash\n# Add scheduler initialization after optimizer definition\nsed -i '/optimizer = torch.optim.Adam/a \\\n    # âœ… Novelty 3: Attach cosine learning rate scheduler\\n\\\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.n_epochs)\\n' \\\n/kaggle/working/dendPLRNN/BPTT_TF/main.py\n\n# Add scheduler stepping after epoch iteration starts\nsed -i '/for epoch in range(args.n_epochs):/a \\\n        scheduler.step()\\n\\\n        print(f\\\"[Scheduler] Current LR: {scheduler.get_last_lr()[0]:.6f}\\\")\\n' \\\n/kaggle/working/dendPLRNN/BPTT_TF/main.py\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%bash\nsed -i '/loss = criterion(pred, target)/a \\\n        # === Apply Novelties ===\\n\\\n        mar_loss = manifold_attractor_regularization(self.model, lambda_mar=0.01)\\n\\\n        distill_loss = temporal_self_distillation(z, weight=0.05)\\n\\\n        loss = hybrid_regularization(loss, z, z_prev=None, klx=getattr(self, \\\"KLx\\\", None))\\n\\\n        loss = loss + mar_loss + distill_loss\\n\\\n        z = inject_latent_noise(z, training=self.model.training, noise_std=0.02)\\n' \\\n/kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%bash\n# Add scheduler definition right after optimizer creation\nsed -i '/optimizer = torch\\.optim\\.Adam.*/a \\\n    # âœ… Novelty 3: Cosine Learning Rate Scheduler\\n\\\n    from torch.optim.lr_scheduler import CosineAnnealingLR\\n\\\n    scheduler = CosineAnnealingLR(optimizer, T_max=args.n_epochs)\\n' \\\n/kaggle/working/dendPLRNN/BPTT_TF/main.py\n\n# Add scheduler stepping after epoch iteration\nsed -i '/for epoch in range(args\\.n_epochs):/a \\\n        scheduler.step()\\n\\\n        print(f\\\"[Scheduler] Current LR: {scheduler.get_last_lr()[0]:.6f}\\\")\\n' \\\n/kaggle/working/dendPLRNN/BPTT_TF/main.py\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%bash\nsed -i '/loss = criterion(pred, target)/a \\\n        # === Apply Novelties ===\\n\\\n        try:\\n\\\n            mar_loss = manifold_attractor_regularization(self.model, lambda_mar=0.01)\\n\\\n            distill_loss = temporal_self_distillation(z, weight=0.05)\\n\\\n            loss = hybrid_regularization(loss, z, z_prev=None, klx=getattr(self, \\\"KLx\\\", None))\\n\\\n            loss = loss + mar_loss + distill_loss\\n\\\n            z = inject_latent_noise(z, training=self.model.training, noise_std=0.02)\\n\\\n        except Exception as e:\\n\\\n            print(f\\\"[Warning] Novelty block skipped due to: {e}\\\")\\n' \\\n/kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!grep -n \"scheduler\" /kaggle/working/dendPLRNN/BPTT_TF/main.py\n!grep -n \"Apply Novelties\" /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!grep -n \"optimizer\" /kaggle/working/dendPLRNN/BPTT_TF/main.py | head -n 3\n!grep -n \"for epoch\" /kaggle/working/dendPLRNN/BPTT_TF/main.py | head -n 3\n!grep -n \"loss = \" /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py | head -n 5\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%bash\n# Add Cosine Scheduler right after optimizer definition\nsed -i '/optimizer = torch\\.optim\\.Adam/a \\\n    # âœ… Novelty 3: Cosine LR Scheduler\\n\\\n    from torch.optim.lr_scheduler import CosineAnnealingLR\\n\\\n    scheduler = CosineAnnealingLR(optimizer, T_max=args.n_epochs)\\n' \\\n/kaggle/working/dendPLRNN/BPTT_TF/main.py\n\n# Add scheduler stepping at the start of each epoch\nsed -i '/for epoch in range(args\\.n_epochs):/a \\\n        scheduler.step()\\n\\\n        print(f\\\"[Scheduler] LR: {scheduler.get_last_lr()[0]:.6f}\\\")\\n' \\\n/kaggle/working/dendPLRNN/BPTT_TF/main.py\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%bash\nsed -i '/loss = self\\.compute_loss(pred, target)/a \\\n        # === Apply Novelties (Auto-Inserted) ===\\n\\\n        try:\\n\\\n            mar_loss = manifold_attractor_regularization(self.model, lambda_mar=0.01)\\n\\\n            distill_loss = temporal_self_distillation(z, weight=0.05)\\n\\\n            loss = hybrid_regularization(loss, z, z_prev=None, klx=getattr(self, \\\"KLx\\\", None))\\n\\\n            loss = loss + mar_loss + distill_loss\\n\\\n            z = inject_latent_noise(z, training=self.model.training, noise_std=0.02)\\n\\\n        except Exception as e:\\n\\\n            print(f\\\"[Warning] Novelty block skipped: {e}\\\")\\n' \\\n/kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!grep -n \"scheduler\" /kaggle/working/dendPLRNN/BPTT_TF/main.py\n!grep -n \"Apply Novelties\" /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!grep -n \"scheduler\" /kaggle/working/dendPLRNN/BPTT_TF/main.py | head -n 5\n!grep -n \"Apply Novelties\" /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py | head -n 5\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!grep -A5 \"Dendritic Gating\" /kaggle/working/dendPLRNN/BPTT_TF/bptt/PLRNN_model.py\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!grep -A8 \"Hybrid Regularization\" /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!grep -A6 \"Manifoldâ€“Attractor\" /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!grep -A5 \"Adaptive Latent Noise Injection\" /kaggle/working/dendPLRNN/BPTT_TF/bptt/PLRNN_model.py\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!grep -A8 \"Temporal Self-Distillation\" /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!grep -n \"Apply Novelties\" /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!sed -i '/Loss =/a \\\n    # ðŸ§© Novelty Status Monitor\\n\\\n    current_lr = optimizer.param_groups[0][\"lr\"]\\n\\\n    print(f\"[Epoch {epoch+1}] LR={current_lr:.6f} | MAR Î»=0.01 | Distill Î»=0.05 | Noise=True | Dendritic=âœ“ | Hybrid=âœ“\")' \\\n/kaggle/working/dendPLRNN/BPTT_TF/main.py\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ðŸ§© Novelty Status Monitor\nif epoch % 1 == 0:  # print every epoch\n    current_lr = optimizer.param_groups[0]['lr']\n    print(f\"[Epoch {epoch+1}] LR={current_lr:.6f} | MAR Î»=0.01 | Distill Î»=0.05 | Noise=True | Dendritic=âœ“ | Hybrid=âœ“\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!grep -n \"scheduler\" /kaggle/working/dendPLRNN/BPTT_TF/main.py\n!grep -n \"Apply Novelties\" /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!grep -n \"scheduler\" /kaggle/working/dendPLRNN/BPTT_TF/main.py\n!grep -n \"Apply Novelties\" /kaggle/working/dendPLRNN/BPTT_TF/bptt/bptt_algorithm.py\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls -lh /kaggle/working/dendPLRNN/BPTT_TF/results/ECG\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%bash\ncd /kaggle/working/dendPLRNN/BPTT_TF\n\nLATEST_RUN=\"results/ECG\"\nif [ -d \"$LATEST_RUN\" ]; then\n  echo \"âœ… Found results: $LATEST_RUN\"\n  echo \"ðŸ“Š Running evaluation...\"\n  PYTHONPATH=\"/kaggle/working/dendPLRNN/BPTT_TF:/kaggle/working/dendPLRNN\" \\\n  python main_eval.py --results_path \"$LATEST_RUN\"\nelse\n  echo \"âš ï¸ No results found â€” check if model checkpoints were saved.\"\nfi\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"/kaggle/working/dendPLRNN/BPTT_TF/results\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%bash\ncd /kaggle/working/dendPLRNN/BPTT_TF\n\necho \"ðŸ” Searching for latest ECG run directory...\"\nLATEST_RUN=$(find Experiments/Table1/ECG -type d -name \"run*\" | sort | tail -n 1)\n\nif [ -n \"$LATEST_RUN\" ]; then\n  echo \"âœ… Found latest run: $LATEST_RUN\"\n  echo \"ðŸ“Š Running evaluation...\"\n  PYTHONPATH=\"/kaggle/working/dendPLRNN/BPTT_TF:/kaggle/working/dendPLRNN\" \\\n  python main_eval.py --results_path \"$LATEST_RUN\"\nelse\n  echo \"âš ï¸ No 'run*' directory found â€” check if training completed successfully.\"\nfi\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!find /kaggle/working/dendPLRNN/BPTT_TF/Experiments/Table1/ECG -type d -name \"run*\" | sort\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!grep -n \"save_model\" /kaggle/working/dendPLRNN/BPTT_TF/main.py\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!sed -i '/for epoch in range(args.n_epochs):/a \\        if (epoch + 1) % args.save_step == 0:\\n            model_path = os.path.join(args.output_dir, f\\\"model_epoch_{epoch+1}.pt\\\")\\n            torch.save(model.state_dict(), model_path)\\n            print(f\\\"ðŸ’¾ Saved model checkpoint to {model_path}\\\")' \\\n/kaggle/working/dendPLRNN/BPTT_TF/main.py\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%bash\ncd /kaggle/working/dendPLRNN/BPTT_TF\n\necho \"ðŸ” Searching for latest ECG run directory...\"\nLATEST_RUN=$(find Experiments/Table1/ECG -type d -name \"run*\" | sort | tail -n 1)\n\nif [ -n \"$LATEST_RUN\" ]; then\n  echo \"âœ… Found latest run: $LATEST_RUN\"\n  echo \"ðŸ“Š Running evaluation...\"\n  PYTHONPATH=\"/kaggle/working/dendPLRNN/BPTT_TF:/kaggle/working/dendPLRNN\" \\\n  python main_eval.py --results_path \"$LATEST_RUN\"\nelse\n  echo \"âš ï¸ No 'run*' directory found â€” check if training completed successfully.\"\nfi\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%cd /kaggle/working/dendPLRNN/BPTT_TF\n!PYTHONPATH=\"/kaggle/working/dendPLRNN/BPTT_TF:/kaggle/working/dendPLRNN\" \\\npython Experiments/Table1/ECG/ubermain.py\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!find Experiments/Table1/ECG -type f \\( -name \"*.pt\" -o -name \"*.npy\" \\)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%cd /kaggle/working/dendPLRNN/BPTT_TF\nLATEST_RUN=$(find Experiments/Table1/ECG -type d -name \"run*\" | sort | tail -n 1)\nif [ -n \"$LATEST_RUN\" ]; then\n  echo \"ðŸ“Š Evaluating latest run: $LATEST_RUN\"\n  PYTHONPATH=\"/kaggle/working/dendPLRNN/BPTT_TF:/kaggle/working/dendPLRNN\" \\\n  python main_eval.py --results_path \"$LATEST_RUN\"\nelse\n  echo \"âš ï¸ No run directory found â€” check if training completed successfully.\"\nfi\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!find /kaggle/working/dendPLRNN/BPTT_TF/Experiments/Table1/ECG -maxdepth 2 -type d | sed 's/^/FOUND: /'\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!sed -i \"/Argument('teacher_forcing_interval'/i \\    args.append(Argument('eval_test', [1]))\" \\\n/kaggle/working/dendPLRNN/BPTT_TF/Experiments/Table1/ECG/ubermain.py\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!grep -n \"eval_test\" /kaggle/working/dendPLRNN/BPTT_TF/Experiments/Table1/ECG/ubermain.py\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"!sed -i \"/eval_test/d\" /kaggle/working/dendPLRNN/BPTT_TF/Experiments/Table1/ECG/ubermain.py\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!grep -n \"eval_test\" /kaggle/working/dendPLRNN/BPTT_TF/Experiments/Table1/ECG/ubermain.py\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!sed -i \"s/Argument('n_epochs', \\[[0-9]*\\])/Argument('n_epochs', [10])/\" \\\n/kaggle/working/dendPLRNN/BPTT_TF/Experiments/Table1/ECG/ubermain.py\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%cd /kaggle/working/dendPLRNN/BPTT_TF\n!PYTHONPATH=\"/kaggle/working/dendPLRNN/BPTT_TF:/kaggle/working/dendPLRNN\" \\\npython Experiments/Table1/ECG/ubermain.py\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!sed -i \"/Argument('seq_len'/a \\    args.append(Argument('save_step', [10]))\" \\\n/kaggle/working/dendPLRNN/BPTT_TF/Experiments/Table1/ECG/ubermain.py\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!grep -n \"save_step\" /kaggle/working/dendPLRNN/BPTT_TF/Experiments/Table1/ECG/ubermain.py\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!sed -i \"s/Argument('n_epochs', \\[[0-9]*\\])/Argument('n_epochs', [20])/\" \\\n/kaggle/working/dendPLRNN/BPTT_TF/Experiments/Table1/ECG/ubermain.py\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!grep -n \"n_epochs\" /kaggle/working/dendPLRNN/BPTT_TF/Experiments/Table1/ECG/ubermain.py\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%cd /kaggle/working/dendPLRNN/BPTT_TF\n!PYTHONPATH=\"/kaggle/working/dendPLRNN/BPTT_TF:/kaggle/working/dendPLRNN\" \\\npython Experiments/Table1/ECG/ubermain.py\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!find /kaggle/working/dendPLRNN/BPTT_TF -maxdepth 6 -type d -name \"run*\" | sort\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!find /kaggle/working/dendPLRNN/BPTT_TF -type d -name \"run*\" | sort\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%cd /kaggle/working/dendPLRNN/BPTT_TF\n!PYTHONPATH=\"/kaggle/working/dendPLRNN/BPTT_TF:/kaggle/working/dendPLRNN\" \\\npython main_eval.py --results_path Experiments/Table1/ECG --device cuda:0\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%cd /kaggle/working/dendPLRNN/BPTT_TF\n!PYTHONPATH=\"/kaggle/working/dendPLRNN/BPTT_TF:/kaggle/working/dendPLRNN\" \\\npython main_eval.py --results_path Experiments/Table1/ECG/results\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!find Experiments/Table1/ECG -type f -name \"*.pt\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%cd /kaggle/working/dendPLRNN/BPTT_TF\n!python main_eval.py --results_path \"<RUN_FOLDER>\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!find /kaggle/working/dendPLRNN/BPTT_TF/Experiments/Table1/ECG -type d -name \"run*\" | sort\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!find Experiments/Table1/ECG -maxdepth 3 -type d -name \"run*\" | sort\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%cd /kaggle/working/dendPLRNN/BPTT_TF\n!PYTHONPATH=\"/kaggle/working/dendPLRNN/BPTT_TF:/kaggle/working/dendPLRNN\" \\\npython Experiments/Table1/ECG/ubermain.py\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!find Experiments/Table1/ECG -maxdepth 3 -type d -name \"run*\" | sort\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!PYTHONPATH=\"/kaggle/working/dendPLRNN/BPTT_TF\" \\\npython main_eval.py --results_path Experiments/Table1/ECG/run_1/results/\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}