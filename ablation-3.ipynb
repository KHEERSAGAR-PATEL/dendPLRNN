{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Cell 1: Environment Setup & Reset ---\n\n!rm -rf /kaggle/working/dendPLRNN\n!git clone --depth 1 https://github.com/DurstewitzLab/dendPLRNN.git /kaggle/working/dendPLRNN\nprint(\"✅ Repository cloned.\")\n\n# Install dependencies\n!pip install numpy scipy scikit-learn matplotlib tensorboardX\n!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\nprint(\"✅ Dependencies installed.\")\n\n# Ensure Python can find project modules\nimport sys\nsys.path.insert(0, \"/kaggle/working/dendPLRNN/BPTT_TF\")\nsys.path.insert(0, \"/kaggle/working/dendPLRNN\")\nprint(\"✅ PYTHONPATH ready.\")\n\n# Change to the correct directory\n%cd /kaggle/working/dendPLRNN/BPTT_TF\nprint(f\"✅ Current directory: $(pwd)\")\n\n# Fix known import and experiment issues\n!sed -i \"s/from pandas.core.indexes import numeric/from pandas.api.types import is_numeric_dtype as numeric/\" main_eval.py\n!sed -i \"s/Argument('n_epochs', \\[[0-9]*\\])/Argument('n_epochs', [10])/\" \\\n/kaggle/working/dendPLRNN/BPTT_TF/Experiments/Table1/ECG/ubermain.py\nprint(\"✅ Pandas and Epoch fixes applied.\")\n\n!nvidia-smi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:37:41.187022Z","iopub.execute_input":"2025-11-12T19:37:41.187212Z","iopub.status.idle":"2025-11-12T19:39:05.476532Z","shell.execute_reply.started":"2025-11-12T19:37:41.187195Z","shell.execute_reply":"2025-11-12T19:39:05.475714Z"}},"outputs":[{"name":"stdout","text":"Cloning into '/kaggle/working/dendPLRNN'...\nremote: Enumerating objects: 137, done.\u001b[K\nremote: Counting objects: 100% (137/137), done.\u001b[K\nremote: Compressing objects: 100% (117/117), done.\u001b[K\nremote: Total 137 (delta 27), reused 122 (delta 19), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (137/137), 60.51 MiB | 38.53 MiB/s, done.\nResolving deltas: 100% (27/27), done.\n✅ Repository cloned.\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.3)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.2)\nCollecting tensorboardX\n  Downloading tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.3.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (6.33.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\nDownloading tensorboardx-2.6.4-py3-none-any.whl (87 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: tensorboardX\nSuccessfully installed tensorboardX-2.6.4\nLooking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu113\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\nRequirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.20.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.15.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.10.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n✅ Dependencies installed.\n✅ PYTHONPATH ready.\n/kaggle/working/dendPLRNN/BPTT_TF\n✅ Current directory: $(pwd)\n✅ Pandas and Epoch fixes applied.\nWed Nov 12 19:39:05 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n| N/A   34C    P0             27W /  250W |       0MiB /  16384MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# --- Cell 2 (Ablate Noise): Installs MAR + TSD ONLY ---\n\nimport os\nimport textwrap\n\n# --- 1. Define the \"Novelty\" Module Code ---\n# This code is identical to your original\nnovelty_code = \"\"\"\n\\\"\\\"\\\"\nMinimal, defensive novelty utilities for PLRNN training.\nKeeps logic isolated to avoid repeated in-place edits of big files.\n\\\"\\\"\\\"\n\nimport torch as tc\nimport torch.nn as nn\n\ndef inject_latent_noise(x, training=True, noise_std=0.02):\n    \\\"\\\"\\\"Add Gaussian noise to input tensor x if training is True.\\\"\\\"\\\"\n    if not training:\n        return x\n    try:\n        return x + tc.randn_like(x) * float(noise_std)\n    except Exception:\n        return x\n\ndef manifold_attractor_regularization(model, lambda_mar=0.01, frac=0.2):\n    \\\"\\\"\\\"\n    Minimal manifold-attractor regularizer.\n    - If model has attribute 'A' (tensor or parameter), compute small penalty.\n    - Returns a scalar tensor on same device; returns 0.0 tensor if not applicable.\n    \\\"\\\"\\\"\n    # safe guards\n    try:\n        if not hasattr(model, \"A\"):\n            return tc.tensor(0.0, device=next(model.parameters()).device)\n        A = getattr(model, \"A\")\n        # convert to tensor if Parameter\n        if isinstance(A, nn.Parameter):\n            A = A.data\n        A = tc.as_tensor(A)\n        device = A.device\n        W = getattr(model, \"W\", None)\n        h = getattr(model, \"h\", None)\n        M = A.shape[0]\n        Mreg = max(1, int(M * float(frac)))\n        A_diag = tc.diag(A)\n        mar_loss = tc.tensor(0.0, device=device)\n        for i in range(Mreg):\n            mar_loss = mar_loss + (A_diag[i] - 1.0) ** 2\n            if h is not None:\n                try:\n                    mar_loss = mar_loss + (tc.as_tensor(h[i]) ** 2)\n                except Exception:\n                    pass\n            if W is not None:\n                try:\n                    row = tc.as_tensor(W[i, :])\n                    mar_loss = mar_loss + (tc.sum(row ** 2) - row[i] ** 2)\n                except Exception:\n                    pass\n        return float(lambda_mar) * mar_loss\n    except Exception:\n        # On any unexpected issue, return zero tensor safely\n        try:\n            return tc.tensor(0.0, device=next(model.parameters()).device)\n        except Exception:\n            return tc.tensor(0.0)\n\ndef temporal_self_distillation_from_model(model, weight=0.05):\n    \\\"\\\"\\\"\n    Try to extract a latent trajectory from the model and compute MSE between adjacent timesteps.\n    Looks for common attributes (Z, last_z, z_hist). If none found, returns 0.\n    \\\"\\\"\\\"\n    try:\n        z = None\n        for attr in (\"Z\", \"z_hist\", \"last_z\", \"Z_hist\"):\n            if hasattr(model, attr):\n                z = getattr(model, attr)\n                break\n        # if z is a list or tuple, coerce to tensor if possible\n        if z is None:\n            return tc.tensor(0.0, device=next(model.parameters()).device)\n        \n        # Ensure z is a tensor\n        if not isinstance(z, tc.Tensor):\n            # Try to convert list of tensors, e.g., from a .cpu() loop\n            if isinstance(z, (list, tuple)) and len(z) > 1 and isinstance(z[0], tc.Tensor):\n                 zt = tc.stack(z)\n            else:\n                 zt = tc.as_tensor(z)\n        else:\n            zt = z\n\n        if zt.dim() < 2 or zt.shape[0] < 2:\n            return tc.tensor(0.0, device=zt.device)\n        diff = zt[1:] - zt[:-1].detach()\n        return float(weight) * tc.mean(diff ** 2)\n    except Exception:\n        try:\n            return tc.tensor(0.0, device=next(model.parameters()).device)\n        except Exception:\n            return tc.tensor(0.0)\n\ndef init_dendritic_if_possible(model, init_scale=0.05):\n    \\\"\\\"\\\"\n    If model has W and supports register_parameter, create and register 'U' parameter.\n    Safe no-op if not possible.\n    \\\"\\\"\\\"\n    try:\n        if hasattr(model, \"W\") and hasattr(model, \"register_parameter\") and not hasattr(model, \"U\"):\n            W = getattr(model, \"W\")\n            # create a parameter of same shape as W\n            U = nn.Parameter(tc.randn_like(tc.as_tensor(W)) * float(init_scale))\n            model.register_parameter(\"U\", U)\n            return True\n    except Exception:\n        pass\n    return False\n\ndef apply_dendritic_gate(z, model):\n    \\\"\\\"\\\"\n    Apply gating if model has parameter U; else return z unchanged.\n    \\\"\\\"\\\"\n    try:\n        if hasattr(model, \"U\"):\n            U = getattr(model, \"U\")\n            # Apply dendritic-style gating\n            return tc.sigmoid(tc.matmul(z, U)) * tc.relu(z)\n    except Exception:\n        pass\n    # Fallback to standard relu if no U or if error\n    return tc.relu(z)\n\"\"\"\n\n# --- 2. Define the Code to Inject (Noise ABLATED) ---\n# This block is modified to comment out the Noise section.\ninsertion_block = textwrap.dedent(\"\"\"\n    # --- novelties (MAR + TSD) ---\n    try:\n        # manifold-attractor regularization (safe)\n        mar = novelties.manifold_attractor_regularization(self.model, lambda_mar=0.01)\n        if isinstance(mar, float) or (hasattr(mar, \"item\") and callable(getattr(mar, \"item\"))):\n            loss = loss + (mar if isinstance(mar, float) else mar)\n    except Exception:\n        pass\n    try:\n        # temporal self-distillation (if model exposes latent trajectory)\n        tsd = novelties.temporal_self_distillation_from_model(self.model, weight=0.05)\n        if hasattr(tsd, \"item\"):\n            loss = loss + tsd\n    except Exception:\n        pass\n\n    # --- NOISE ABLATED ---\n    # try:\n    #     # add an extra small noise via helper (this is optional and safe)\n    #     if \"inp\" in locals():\n    #         inp = novelties.inject_latent_noise(inp, training=self.model.training if hasattr(self.model, \"training\") else True, noise_level=getattr(self, \"noise_level\", 0.02))\n    # except Exception:\n    #     pass\n\"\"\")\n\n# Define file paths\nnovelties_file = \"bptt/novelties.py\"\nalgo_file = \"bptt/bptt_algorithm.py\"\n\n# --- 3. Write the new novelties.py file ---\ntry:\n    with open(novelties_file, \"w\") as f:\n        f.write(novelty_code)\n    print(f\"✅ Successfully created {novelties_file}\")\n\n    # --- 4. Patch bptt_algorithm.py ---\n    \n    # Add the import at the top\n    with open(algo_file, \"r\") as f:\n        content = f.read()\n    \n    if \"from bptt import novelties\" not in content:\n        content = \"from bptt import novelties\\n\" + content\n        with open(algo_file, \"w\") as f:\n            f.write(content)\n        print(f\"✅ Added import to {algo_file}\")\n    \n    # Insert the loss block\n    with open(algo_file, \"r\") as f:\n        lines = f.readlines()\n\n    new_lines = []\n    inserted = False\n    target_line = \"loss = self.compute_loss(pred, target)\"\n\n    for line in lines:\n        new_lines.append(line)\n        # Check if this is the target line and we haven't inserted yet\n        if target_line in line and not inserted:\n            # Find the indentation of the target line\n            indentation = line[:len(line) - len(line.lstrip())]\n            # Add the insertion block with the same indentation\n            indented_insertion = \"\\n\".join([f\"{indentation}{l}\" for l in insertion_block.splitlines() if l])\n            new_lines.append(indented_insertion + \"\\n\")\n            inserted = True\n            print(f\"✅ Successfully inserted (Ablated-Noise) block into {algo_file}\")\n\n    if not inserted:\n        print(f\"⚠️ WARNING: Could not find target line '{target_line}' in {algo_file}. Patch failed.\")\n    else:\n        # Write the modified content back\n        with open(algo_file, \"w\") as f:\n            f.writelines(new_lines)\n        print(\"✅ File patching complete.\")\n\nexcept Exception as e:\n    print(f\"❌ An error occurred during file modification: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:39:05.478788Z","iopub.execute_input":"2025-11-12T19:39:05.479348Z","iopub.status.idle":"2025-11-12T19:39:05.492107Z","shell.execute_reply.started":"2025-11-12T19:39:05.479318Z","shell.execute_reply":"2025-11-12T19:39:05.491390Z"}},"outputs":[{"name":"stdout","text":"✅ Successfully created bptt/novelties.py\n✅ Added import to bptt/bptt_algorithm.py\n✅ Successfully inserted (Ablated-Noise) block into bptt/bptt_algorithm.py\n✅ File patching complete.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# --- Cell 3: Syntax Check and Run Training ---\n\n!echo \"Running python syntax check...\"\n!python -m py_compile bptt/novelties.py\n!python -m py_compile bptt/bptt_algorithm.py\n!echo \"✅ Syntax OK.\"\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:39:05.493153Z","iopub.execute_input":"2025-11-12T19:39:05.493415Z","iopub.status.idle":"2025-11-12T19:39:06.123581Z","shell.execute_reply.started":"2025-11-12T19:39:05.493399Z","shell.execute_reply":"2025-11-12T19:39:06.122792Z"}},"outputs":[{"name":"stdout","text":"Running python syntax check...\n✅ Syntax OK.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# --- GPU Diagnostic Cell ---\nimport torch\n\nprint(f\"PyTorch Version: {torch.__version__}\")\nprint(\"---\")\nprint(f\"Is CUDA (GPU) available? ==> {torch.cuda.is_available()}\")\nprint(\"---\")\n\nif torch.cuda.is_available():\n    print(f\"Current GPU Name: {torch.cuda.get_device_name(0)}\")\nelse:\n    print(\"WARNING: PyTorch cannot find the GPU.\")\n    print(\"Please go to 'Settings' on the right and ensure 'Accelerator' is set to 'GPU'.\")\n\nprint(\"\\n--- nvidia-smi (Hardware Check) ---\")\n!nvidia-smi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:39:06.124915Z","iopub.execute_input":"2025-11-12T19:39:06.125183Z","iopub.status.idle":"2025-11-12T19:39:09.900289Z","shell.execute_reply.started":"2025-11-12T19:39:06.125148Z","shell.execute_reply":"2025-11-12T19:39:09.899355Z"}},"outputs":[{"name":"stdout","text":"PyTorch Version: 2.6.0+cu124\n---\nIs CUDA (GPU) available? ==> True\n---\nCurrent GPU Name: Tesla P100-PCIE-16GB\n\n--- nvidia-smi (Hardware Check) ---\nWed Nov 12 19:39:09 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n| N/A   34C    P0             27W /  250W |       3MiB /  16384MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!grep -n \"numeric\" main_eval.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:39:09.901745Z","iopub.execute_input":"2025-11-12T19:39:09.902673Z","iopub.status.idle":"2025-11-12T19:39:10.035843Z","shell.execute_reply.started":"2025-11-12T19:39:09.902645Z","shell.execute_reply":"2025-11-12T19:39:10.035096Z"}},"outputs":[{"name":"stdout","text":"3:from pandas.api.types import is_numeric_dtype as numeric\n260:        mse5 = (df.mean(0, numeric_only=True)['5'], df.std(numeric_only=True)['5'])\n261:        mse10 = (df.mean(0, numeric_only=True)['10'], df.std(numeric_only=True)['10'])\n262:        mse20 = (df.mean(0, numeric_only=True)['20'], df.std(numeric_only=True)['20'])\n272:        pse = (df.mean(0, numeric_only=True)['mean'], df.std(numeric_only=True)['mean'])\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"%cd /kaggle/working/dendPLRNN/BPTT_TF/Experiments/Table1/ECG\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:39:10.036995Z","iopub.execute_input":"2025-11-12T19:39:10.037454Z","iopub.status.idle":"2025-11-12T19:39:10.043220Z","shell.execute_reply.started":"2025-11-12T19:39:10.037418Z","shell.execute_reply":"2025-11-12T19:39:10.042665Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/dendPLRNN/BPTT_TF/Experiments/Table1/ECG\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!sed -i \"s/Argument('use_gpu', \\[[0-9]\\+\\])/Argument('use_gpu', [1])/\" ubermain.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:39:10.045294Z","iopub.execute_input":"2025-11-12T19:39:10.045817Z","iopub.status.idle":"2025-11-12T19:39:10.182007Z","shell.execute_reply.started":"2025-11-12T19:39:10.045799Z","shell.execute_reply":"2025-11-12T19:39:10.180760Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"!grep -n \"use_gpu\" ubermain.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:39:10.183382Z","iopub.execute_input":"2025-11-12T19:39:10.183718Z","iopub.status.idle":"2025-11-12T19:39:10.313612Z","shell.execute_reply.started":"2025-11-12T19:39:10.183680Z","shell.execute_reply":"2025-11-12T19:39:10.312569Z"}},"outputs":[{"name":"stdout","text":"11:    When using GPU for training (i.e. Argument 'use_gpu 1')  it is generally\n17:    args.append(Argument('use_gpu', [1])) # may wanna use gpu here\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"!sed -i \"s/n_runs = [0-9]\\+/n_runs = 1/\" ubermain.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:39:10.314758Z","iopub.execute_input":"2025-11-12T19:39:10.315107Z","iopub.status.idle":"2025-11-12T19:39:10.443568Z","shell.execute_reply.started":"2025-11-12T19:39:10.315082Z","shell.execute_reply":"2025-11-12T19:39:10.442709Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"!grep -n \"n_runs\" ubermain.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:39:10.444589Z","iopub.execute_input":"2025-11-12T19:39:10.444811Z","iopub.status.idle":"2025-11-12T19:39:10.574056Z","shell.execute_reply.started":"2025-11-12T19:39:10.444789Z","shell.execute_reply":"2025-11-12T19:39:10.573238Z"}},"outputs":[{"name":"stdout","text":"4:def ubermain(n_runs):\n28:    args.append(Argument('run', list(range(1, 1 + n_runs))))\n36:    n_runs = 1\n42:    args = ubermain(n_runs)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# --- Configuration Cell: Set 250 Epochs & 50-Epoch Metrics ---\n\n# We must be in the experiment directory to edit the file\n%cd /kaggle/working/dendPLRNN/BPTT_TF/Experiments/Table1/ECG\n\nprint(\"--- Modifying n_epochs ---\")\n# 1. Change n_epochs from [10] (or whatever it is) to [250]\n!sed -i \"s/Argument('n_epochs', \\[[0-9]*\\])/Argument('n_epochs', [250])/\" ubermain.py\n# Verify the change\n!grep -n \"n_epochs\" ubermain.py\n\nprint(\"\\n--- Modifying test_interval (for metrics) ---\")\n# 2. Change test_interval from its default (e.g., 10) to [50]\n!sed -i \"s/Argument('test_interval', \\[[0-9]*\\])/Argument('test_interval', [50])/\" ubermain.py\n# Verify the change\n!grep -n \"test_interval\" ubermain.py\n\n# 3. Go back to the main BPTT_TF directory\n%cd /kaggle/working/dendPLRNN/BPTT_TF\n\nprint(\"\\n✅ Configuration complete. Ready to run final training.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:39:10.575197Z","iopub.execute_input":"2025-11-12T19:39:10.575500Z","iopub.status.idle":"2025-11-12T19:39:11.080721Z","shell.execute_reply.started":"2025-11-12T19:39:10.575472Z","shell.execute_reply":"2025-11-12T19:39:11.079941Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/dendPLRNN/BPTT_TF/Experiments/Table1/ECG\n--- Modifying n_epochs ---\n23:    args.append(Argument('n_epochs', [250]))\n\n--- Modifying test_interval (for metrics) ---\n/kaggle/working/dendPLRNN/BPTT_TF\n\n✅ Configuration complete. Ready to run final training.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# --- Cell 3 (FINAL EXPERIMENT): Run 250 Epochs ---\n\n!echo \"Running python syntax check...\"\n!python -m py_compile bptt/novelties.py\n!python -m py_compile bptt/bptt_algorithm.py\n!echo \"✅ Syntax OK.\"\n!echo \" \"\n\n!echo \"Launching FINAL 250-epoch training (metrics every 50 epochs)...\"\n\n# Get the full, absolute paths for PYTHONPATH\nROOT_DIR = \"/kaggle/working/dendPLRNN\"\nBPTT_DIR = \"/kaggle/working/dendPLRNN/BPTT_TF\"\nFULL_PYTHONPATH = f\"{BPTT_DIR}:{ROOT_DIR}\"\n\n# This command will now run your full experiment\n!PYTHONPATH=\"{FULL_PYTHONPATH}\" python -u Experiments/Table1/ECG/ubermain.py\n\n!echo \"--- Training finished ---\"\n!echo \" \"\n\n# Find and copy the latest model\n!bash -c 'LATEST_MODEL=$(find results -type f -name \"*.pt\" | sort | tail -n 1 || true); \\\nif [ -f \"$LATEST_MODEL\" ]; then \\\n  cp \"$LATEST_MODEL\" /kaggle/working/final_trained_model.pt; \\\n  echo \"✅ Saved model to /kaggle/working/final_trained_model.pt\"; \\\nelse \\\n  echo \"⚠️ No .pt checkpoint found in results/ — check the log above for errors.\"; \\\nfi'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:39:11.081722Z","iopub.execute_input":"2025-11-12T19:39:11.082021Z","iopub.status.idle":"2025-11-12T20:49:04.168856Z","shell.execute_reply.started":"2025-11-12T19:39:11.081996Z","shell.execute_reply":"2025-11-12T20:49:04.167848Z"}},"outputs":[{"name":"stdout","text":"Running python syntax check...\n✅ Syntax OK.\n \nLaunching FINAL 250-epoch training (metrics every 50 epochs)...\n'use_gpu' flag is set.\nWill distribute tasks to GPUs automatically.\nThere are not enough GPU Resources available to spawn 20 processes. Reducing number of parallel runs to 1\nUsing device Tesla P100-PCIE-16GB for training (cuda:0).\nForcing interval set by user: 10\nEpoch 1 took 15.6s | Cumulative time (h:mm:ss): 0:00:15 | Loss = 0.8501778841018677\nEpoch 2 took 14.89s | Cumulative time (h:mm:ss): 0:00:30 | Loss = 0.8555380702018738\nEpoch 3 took 15.02s | Cumulative time (h:mm:ss): 0:00:45 | Loss = 0.8366745710372925\nEpoch 4 took 15.03s | Cumulative time (h:mm:ss): 0:01:00 | Loss = 0.7984108328819275\nEpoch 5 took 15.12s | Cumulative time (h:mm:ss): 0:01:15 | Loss = 0.7170669436454773\nEpoch 6 took 14.63s | Cumulative time (h:mm:ss): 0:01:30 | Loss = 0.6030749082565308\nEpoch 7 took 14.69s | Cumulative time (h:mm:ss): 0:01:44 | Loss = 0.48880812525749207\nEpoch 8 took 14.67s | Cumulative time (h:mm:ss): 0:01:59 | Loss = 0.3782401978969574\nEpoch 9 took 14.66s | Cumulative time (h:mm:ss): 0:02:14 | Loss = 0.3609253466129303\nEpoch 10 took 14.68s | Cumulative time (h:mm:ss): 0:02:28 | Loss = 0.31438857316970825\nEpoch 11 took 14.58s | Cumulative time (h:mm:ss): 0:02:43 | Loss = 0.26952528953552246\nEpoch 12 took 14.68s | Cumulative time (h:mm:ss): 0:02:58 | Loss = 0.3348761200904846\nEpoch 13 took 14.64s | Cumulative time (h:mm:ss): 0:03:12 | Loss = 0.23350012302398682\nEpoch 14 took 14.67s | Cumulative time (h:mm:ss): 0:03:27 | Loss = 0.22914466261863708\nEpoch 15 took 14.91s | Cumulative time (h:mm:ss): 0:03:42 | Loss = 0.21901282668113708\nEpoch 16 took 14.76s | Cumulative time (h:mm:ss): 0:03:57 | Loss = 0.2030225247144699\nEpoch 17 took 14.63s | Cumulative time (h:mm:ss): 0:04:11 | Loss = 0.19507603347301483\nEpoch 18 took 14.7s | Cumulative time (h:mm:ss): 0:04:26 | Loss = 0.21627651154994965\nEpoch 19 took 14.54s | Cumulative time (h:mm:ss): 0:04:41 | Loss = 0.1907355934381485\nEpoch 20 took 14.69s | Cumulative time (h:mm:ss): 0:04:55 | Loss = 0.17979936301708221\nEpoch 21 took 14.73s | Cumulative time (h:mm:ss): 0:05:10 | Loss = 0.1796262413263321\nEpoch 22 took 14.68s | Cumulative time (h:mm:ss): 0:05:25 | Loss = 0.1667110174894333\nEpoch 23 took 14.65s | Cumulative time (h:mm:ss): 0:05:39 | Loss = 0.1597391813993454\nEpoch 24 took 14.63s | Cumulative time (h:mm:ss): 0:05:54 | Loss = 0.16339750587940216\nEpoch 25 took 14.64s | Cumulative time (h:mm:ss): 0:06:09 | Loss = 0.16570745408535004\n\tMSE-1 0.1362856775522232\n\tMSE-5 0.398391455411911\n\tMSE-10 0.6311649680137634\n\tMSE-20 0.6757136583328247\n20 20000\n\tPSE 0.7519023949912714\n\tPSE per dim [0.7318069491933705, 0.8778343917373788, 0.8697961810926046, 0.46344845332389367, 0.7922619003508405, 0.7762708477059415, 0.7518980415348716]\nComputing KLx-GMM\n\tKLx 0.7375434637069702\nEpoch 26 took 15.32s | Cumulative time (h:mm:ss): 0:06:24 | Loss = 0.15315404534339905\nEpoch 27 took 14.86s | Cumulative time (h:mm:ss): 0:06:39 | Loss = 0.13118238747119904\nEpoch 28 took 14.81s | Cumulative time (h:mm:ss): 0:06:54 | Loss = 0.1223892793059349\nEpoch 29 took 14.66s | Cumulative time (h:mm:ss): 0:07:08 | Loss = 0.13414126634597778\nEpoch 30 took 14.81s | Cumulative time (h:mm:ss): 0:07:23 | Loss = 0.12644562125205994\nEpoch 31 took 14.72s | Cumulative time (h:mm:ss): 0:07:38 | Loss = 0.125578373670578\nEpoch 32 took 14.91s | Cumulative time (h:mm:ss): 0:07:53 | Loss = 0.143095925450325\nEpoch 33 took 14.87s | Cumulative time (h:mm:ss): 0:08:08 | Loss = 0.12091132253408432\nEpoch 34 took 14.86s | Cumulative time (h:mm:ss): 0:08:22 | Loss = 0.14358000457286835\nEpoch 35 took 14.89s | Cumulative time (h:mm:ss): 0:08:37 | Loss = 0.1295783519744873\nEpoch 36 took 14.87s | Cumulative time (h:mm:ss): 0:08:52 | Loss = 0.12374872714281082\nEpoch 37 took 14.86s | Cumulative time (h:mm:ss): 0:09:07 | Loss = 0.12810666859149933\nEpoch 38 took 14.81s | Cumulative time (h:mm:ss): 0:09:22 | Loss = 0.1222660169005394\nEpoch 39 took 14.82s | Cumulative time (h:mm:ss): 0:09:37 | Loss = 0.12540008127689362\nEpoch 40 took 14.94s | Cumulative time (h:mm:ss): 0:09:52 | Loss = 0.10957453399896622\nEpoch 41 took 14.97s | Cumulative time (h:mm:ss): 0:10:07 | Loss = 0.11912865936756134\nEpoch 42 took 15.0s | Cumulative time (h:mm:ss): 0:10:22 | Loss = 0.11163754016160965\nEpoch 43 took 15.18s | Cumulative time (h:mm:ss): 0:10:37 | Loss = 0.11305677890777588\nEpoch 44 took 14.93s | Cumulative time (h:mm:ss): 0:10:52 | Loss = 0.14656606316566467\nEpoch 45 took 14.79s | Cumulative time (h:mm:ss): 0:11:07 | Loss = 0.11466267704963684\nEpoch 46 took 14.91s | Cumulative time (h:mm:ss): 0:11:21 | Loss = 0.1135745719075203\nEpoch 47 took 14.99s | Cumulative time (h:mm:ss): 0:11:36 | Loss = 0.10335730761289597\nEpoch 48 took 14.81s | Cumulative time (h:mm:ss): 0:11:51 | Loss = 0.12359793484210968\nEpoch 49 took 15.06s | Cumulative time (h:mm:ss): 0:12:06 | Loss = 0.09959959238767624\nEpoch 50 took 15.14s | Cumulative time (h:mm:ss): 0:12:21 | Loss = 0.10575006902217865\n\tMSE-1 0.13270214200019836\n\tMSE-5 0.37658795714378357\n\tMSE-10 0.6049439907073975\n\tMSE-20 0.650597870349884\n20 20000\n\tPSE 0.9028218784875167\n\tPSE per dim [0.9112544475296637, 0.9501264218771843, 0.8987804504530168, 0.8911578392905443, 0.8801064597242398, 0.9252377425468151, 0.8630897879911525]\nComputing KLx-GMM\n\tKLx 0.3778356909751892\nEpoch 51 took 15.03s | Cumulative time (h:mm:ss): 0:12:36 | Loss = 0.11795450747013092\nEpoch 52 took 15.09s | Cumulative time (h:mm:ss): 0:12:52 | Loss = 0.12396234273910522\nEpoch 53 took 14.94s | Cumulative time (h:mm:ss): 0:13:06 | Loss = 0.11300115287303925\nEpoch 54 took 15.05s | Cumulative time (h:mm:ss): 0:13:22 | Loss = 0.1148686334490776\nEpoch 55 took 15.01s | Cumulative time (h:mm:ss): 0:13:37 | Loss = 0.10803550481796265\nEpoch 56 took 15.01s | Cumulative time (h:mm:ss): 0:13:52 | Loss = 0.10495781153440475\nEpoch 57 took 14.94s | Cumulative time (h:mm:ss): 0:14:06 | Loss = 0.11459822952747345\nEpoch 58 took 15.08s | Cumulative time (h:mm:ss): 0:14:22 | Loss = 0.10578062385320663\nEpoch 59 took 15.07s | Cumulative time (h:mm:ss): 0:14:37 | Loss = 0.10160192102193832\nEpoch 60 took 15.14s | Cumulative time (h:mm:ss): 0:14:52 | Loss = 0.1015590950846672\nEpoch 61 took 15.08s | Cumulative time (h:mm:ss): 0:15:07 | Loss = 0.10210359841585159\nEpoch 62 took 14.99s | Cumulative time (h:mm:ss): 0:15:22 | Loss = 0.10087083280086517\nEpoch 63 took 15.36s | Cumulative time (h:mm:ss): 0:15:37 | Loss = 0.10181663185358047\nEpoch 64 took 15.17s | Cumulative time (h:mm:ss): 0:15:52 | Loss = 0.10388607531785965\nEpoch 65 took 15.18s | Cumulative time (h:mm:ss): 0:16:08 | Loss = 0.10021770000457764\nEpoch 66 took 15.54s | Cumulative time (h:mm:ss): 0:16:23 | Loss = 0.09256277233362198\nEpoch 67 took 15.44s | Cumulative time (h:mm:ss): 0:16:39 | Loss = 0.10365264117717743\nEpoch 68 took 14.72s | Cumulative time (h:mm:ss): 0:16:53 | Loss = 0.09029300510883331\nEpoch 69 took 14.88s | Cumulative time (h:mm:ss): 0:17:08 | Loss = 0.0995873287320137\nEpoch 70 took 15.32s | Cumulative time (h:mm:ss): 0:17:23 | Loss = 0.09654264152050018\nEpoch 71 took 14.84s | Cumulative time (h:mm:ss): 0:17:38 | Loss = 0.09840738028287888\nEpoch 72 took 14.96s | Cumulative time (h:mm:ss): 0:17:53 | Loss = 0.10501445829868317\nEpoch 73 took 15.1s | Cumulative time (h:mm:ss): 0:18:08 | Loss = 0.10130836814641953\nEpoch 74 took 15.1s | Cumulative time (h:mm:ss): 0:18:23 | Loss = 0.09172193706035614\nEpoch 75 took 15.18s | Cumulative time (h:mm:ss): 0:18:39 | Loss = 0.0926000252366066\n\tMSE-1 0.12320291250944138\n\tMSE-5 0.3544653058052063\n\tMSE-10 0.5902755856513977\n\tMSE-20 0.6133539080619812\n20 20000\n\tPSE 0.8611966073603611\n\tPSE per dim [0.8964900323158137, 0.9432011303903747, 0.8681872829679775, 0.6764256772039322, 0.8630113900197384, 0.8896393902526628, 0.8914213483720281]\nComputing KLx-GMM\n\tKLx 0.551182210445404\nEpoch 76 took 14.76s | Cumulative time (h:mm:ss): 0:18:53 | Loss = 0.098924420773983\nEpoch 77 took 14.73s | Cumulative time (h:mm:ss): 0:19:08 | Loss = 0.10016463696956635\nEpoch 78 took 14.75s | Cumulative time (h:mm:ss): 0:19:23 | Loss = 0.10150613635778427\nEpoch 79 took 14.79s | Cumulative time (h:mm:ss): 0:19:38 | Loss = 0.09603952616453171\nEpoch 80 took 14.65s | Cumulative time (h:mm:ss): 0:19:52 | Loss = 0.10422251373529434\nEpoch 81 took 14.74s | Cumulative time (h:mm:ss): 0:20:07 | Loss = 0.09481413662433624\nEpoch 82 took 14.7s | Cumulative time (h:mm:ss): 0:20:22 | Loss = 0.09528043866157532\nEpoch 83 took 14.89s | Cumulative time (h:mm:ss): 0:20:37 | Loss = 0.09274922311306\nEpoch 84 took 14.73s | Cumulative time (h:mm:ss): 0:20:51 | Loss = 0.08661708235740662\nEpoch 85 took 14.79s | Cumulative time (h:mm:ss): 0:21:06 | Loss = 0.08205011487007141\nEpoch 86 took 15.01s | Cumulative time (h:mm:ss): 0:21:21 | Loss = 0.09067540615797043\nEpoch 87 took 14.71s | Cumulative time (h:mm:ss): 0:21:36 | Loss = 0.12084217369556427\nEpoch 88 took 14.81s | Cumulative time (h:mm:ss): 0:21:51 | Loss = 0.08781188726425171\nEpoch 89 took 15.13s | Cumulative time (h:mm:ss): 0:22:06 | Loss = 0.10090376436710358\nEpoch 90 took 15.15s | Cumulative time (h:mm:ss): 0:22:21 | Loss = 0.09249361604452133\nEpoch 91 took 15.09s | Cumulative time (h:mm:ss): 0:22:36 | Loss = 0.101972796022892\nEpoch 92 took 14.98s | Cumulative time (h:mm:ss): 0:22:51 | Loss = 0.08737093955278397\nEpoch 93 took 15.06s | Cumulative time (h:mm:ss): 0:23:06 | Loss = 0.08468952029943466\nEpoch 94 took 14.97s | Cumulative time (h:mm:ss): 0:23:21 | Loss = 0.09643818438053131\nEpoch 95 took 14.94s | Cumulative time (h:mm:ss): 0:23:36 | Loss = 0.08454374223947525\nEpoch 96 took 15.0s | Cumulative time (h:mm:ss): 0:23:51 | Loss = 0.08574477583169937\nEpoch 97 took 15.12s | Cumulative time (h:mm:ss): 0:24:06 | Loss = 0.09159018844366074\nEpoch 98 took 15.15s | Cumulative time (h:mm:ss): 0:24:21 | Loss = 0.08369636535644531\nEpoch 99 took 15.24s | Cumulative time (h:mm:ss): 0:24:37 | Loss = 0.07873862236738205\nEpoch 100 took 15.06s | Cumulative time (h:mm:ss): 0:24:52 | Loss = 0.0896231085062027\n\tMSE-1 0.11464766412973404\n\tMSE-5 0.3401683568954468\n\tMSE-10 0.5805569887161255\n\tMSE-20 0.6151571273803711\n20 20000\n\tPSE 0.9328243993307697\n\tPSE per dim [0.9277137390837339, 0.9566898056872342, 0.9279823864140996, 0.9091183361992371, 0.920281932770606, 0.9523325837512835, 0.9356520114091927]\nComputing KLx-GMM\n\tKLx 0.31597620248794556\nEpoch 101 took 15.02s | Cumulative time (h:mm:ss): 0:25:07 | Loss = 0.08460754156112671\nEpoch 102 took 14.98s | Cumulative time (h:mm:ss): 0:25:22 | Loss = 0.09129245579242706\nEpoch 103 took 14.97s | Cumulative time (h:mm:ss): 0:25:37 | Loss = 0.08454596996307373\nEpoch 104 took 14.92s | Cumulative time (h:mm:ss): 0:25:51 | Loss = 0.08763106912374496\nEpoch 105 took 15.1s | Cumulative time (h:mm:ss): 0:26:07 | Loss = 0.0982750952243805\nEpoch 106 took 15.18s | Cumulative time (h:mm:ss): 0:26:22 | Loss = 0.08477024734020233\nEpoch 107 took 15.05s | Cumulative time (h:mm:ss): 0:26:37 | Loss = 0.08006574213504791\nEpoch 108 took 15.13s | Cumulative time (h:mm:ss): 0:26:52 | Loss = 0.0857379287481308\nEpoch 109 took 15.14s | Cumulative time (h:mm:ss): 0:27:07 | Loss = 0.07929619401693344\nEpoch 110 took 14.95s | Cumulative time (h:mm:ss): 0:27:22 | Loss = 0.08198262751102448\nEpoch 111 took 14.89s | Cumulative time (h:mm:ss): 0:27:37 | Loss = 0.07530602067708969\nEpoch 112 took 14.84s | Cumulative time (h:mm:ss): 0:27:52 | Loss = 0.08987171947956085\nEpoch 113 took 15.33s | Cumulative time (h:mm:ss): 0:28:07 | Loss = 0.08528722822666168\nEpoch 114 took 15.24s | Cumulative time (h:mm:ss): 0:28:22 | Loss = 0.0892554223537445\nEpoch 115 took 14.79s | Cumulative time (h:mm:ss): 0:28:37 | Loss = 0.08114513754844666\nEpoch 116 took 14.93s | Cumulative time (h:mm:ss): 0:28:52 | Loss = 0.09338553994894028\nEpoch 117 took 14.88s | Cumulative time (h:mm:ss): 0:29:07 | Loss = 0.08323266357183456\nEpoch 118 took 14.86s | Cumulative time (h:mm:ss): 0:29:22 | Loss = 0.07553409039974213\nEpoch 119 took 14.92s | Cumulative time (h:mm:ss): 0:29:37 | Loss = 0.07915370911359787\nEpoch 120 took 14.73s | Cumulative time (h:mm:ss): 0:29:51 | Loss = 0.09008578211069107\nEpoch 121 took 15.01s | Cumulative time (h:mm:ss): 0:30:06 | Loss = 0.07196038961410522\nEpoch 122 took 14.84s | Cumulative time (h:mm:ss): 0:30:21 | Loss = 0.07299916446208954\nEpoch 123 took 14.88s | Cumulative time (h:mm:ss): 0:30:36 | Loss = 0.07746051996946335\nEpoch 124 took 14.78s | Cumulative time (h:mm:ss): 0:30:51 | Loss = 0.07288632541894913\nEpoch 125 took 14.93s | Cumulative time (h:mm:ss): 0:31:06 | Loss = 0.07024821639060974\n\tMSE-1 0.10444499552249908\n\tMSE-5 0.3283996880054474\n\tMSE-10 0.5665426254272461\n\tMSE-20 0.60125333070755\n20 20000\n\tPSE 0.8803690471028206\n\tPSE per dim [0.9088242480963261, 0.935212230090144, 0.8565264322791579, 0.7635415367951653, 0.8568177162317493, 0.9005073268344181, 0.9411538393927832]\nComputing KLx-GMM\n\tKLx 0.6242799758911133\nEpoch 126 took 14.91s | Cumulative time (h:mm:ss): 0:31:21 | Loss = 0.08958950638771057\nEpoch 127 took 15.22s | Cumulative time (h:mm:ss): 0:31:36 | Loss = 0.07444798201322556\nEpoch 128 took 14.87s | Cumulative time (h:mm:ss): 0:31:51 | Loss = 0.07320687174797058\nEpoch 129 took 14.92s | Cumulative time (h:mm:ss): 0:32:06 | Loss = 0.08603011816740036\nEpoch 130 took 14.97s | Cumulative time (h:mm:ss): 0:32:21 | Loss = 0.07182483375072479\nEpoch 131 took 15.06s | Cumulative time (h:mm:ss): 0:32:36 | Loss = 0.06904837489128113\nEpoch 132 took 15.15s | Cumulative time (h:mm:ss): 0:32:51 | Loss = 0.0769030898809433\nEpoch 133 took 15.39s | Cumulative time (h:mm:ss): 0:33:06 | Loss = 0.09114723652601242\nEpoch 134 took 15.27s | Cumulative time (h:mm:ss): 0:33:22 | Loss = 0.07857062667608261\nEpoch 135 took 14.77s | Cumulative time (h:mm:ss): 0:33:36 | Loss = 0.07444310933351517\nEpoch 136 took 14.78s | Cumulative time (h:mm:ss): 0:33:51 | Loss = 0.07129520922899246\nEpoch 137 took 14.7s | Cumulative time (h:mm:ss): 0:34:06 | Loss = 0.0739293172955513\nEpoch 138 took 14.91s | Cumulative time (h:mm:ss): 0:34:21 | Loss = 0.07774440199136734\nEpoch 139 took 14.87s | Cumulative time (h:mm:ss): 0:34:36 | Loss = 0.08188638836145401\nEpoch 140 took 14.82s | Cumulative time (h:mm:ss): 0:34:51 | Loss = 0.08057200163602829\nEpoch 141 took 14.62s | Cumulative time (h:mm:ss): 0:35:05 | Loss = 0.07822343707084656\nEpoch 142 took 15.1s | Cumulative time (h:mm:ss): 0:35:20 | Loss = 0.06735445559024811\nEpoch 143 took 14.98s | Cumulative time (h:mm:ss): 0:35:35 | Loss = 0.07092157006263733\nEpoch 144 took 14.83s | Cumulative time (h:mm:ss): 0:35:50 | Loss = 0.07819811999797821\nEpoch 145 took 14.95s | Cumulative time (h:mm:ss): 0:36:05 | Loss = 0.07300320267677307\nEpoch 146 took 14.88s | Cumulative time (h:mm:ss): 0:36:20 | Loss = 0.07130251079797745\nEpoch 147 took 15.21s | Cumulative time (h:mm:ss): 0:36:35 | Loss = 0.08411470800638199\nEpoch 148 took 15.01s | Cumulative time (h:mm:ss): 0:36:50 | Loss = 0.07553818076848984\nEpoch 149 took 15.26s | Cumulative time (h:mm:ss): 0:37:05 | Loss = 0.07947491109371185\nEpoch 150 took 14.8s | Cumulative time (h:mm:ss): 0:37:20 | Loss = 0.07969000935554504\n\tMSE-1 0.0962967500090599\n\tMSE-5 0.31749334931373596\n\tMSE-10 0.5625675916671753\n\tMSE-20 0.58788001537323\n20 20000\n\tPSE 0.925978563122235\n\tPSE per dim [0.9316432378220755, 0.968187479358937, 0.9322897984784506, 0.8864973148087815, 0.8888594779223836, 0.9495066878710438, 0.9248659455939724]\nComputing KLx-GMM\n\tKLx 0.5139962434768677\nEpoch 151 took 14.86s | Cumulative time (h:mm:ss): 0:37:35 | Loss = 0.06734864413738251\nEpoch 152 took 15.16s | Cumulative time (h:mm:ss): 0:37:50 | Loss = 0.06863721460103989\nEpoch 153 took 15.25s | Cumulative time (h:mm:ss): 0:38:05 | Loss = 0.07044530659914017\nEpoch 154 took 15.02s | Cumulative time (h:mm:ss): 0:38:20 | Loss = 0.06588272750377655\nEpoch 155 took 14.78s | Cumulative time (h:mm:ss): 0:38:35 | Loss = 0.07154165208339691\nEpoch 156 took 14.99s | Cumulative time (h:mm:ss): 0:38:50 | Loss = 0.063777394592762\nEpoch 157 took 15.01s | Cumulative time (h:mm:ss): 0:39:05 | Loss = 0.07196458429098129\nEpoch 158 took 14.85s | Cumulative time (h:mm:ss): 0:39:20 | Loss = 0.06997095793485641\nEpoch 159 took 14.69s | Cumulative time (h:mm:ss): 0:39:35 | Loss = 0.06923247128725052\nEpoch 160 took 14.71s | Cumulative time (h:mm:ss): 0:39:49 | Loss = 0.06869740039110184\nEpoch 161 took 14.76s | Cumulative time (h:mm:ss): 0:40:04 | Loss = 0.06886462867259979\nEpoch 162 took 14.73s | Cumulative time (h:mm:ss): 0:40:19 | Loss = 0.06483494490385056\nEpoch 163 took 14.62s | Cumulative time (h:mm:ss): 0:40:34 | Loss = 0.06383255124092102\nEpoch 164 took 15.02s | Cumulative time (h:mm:ss): 0:40:49 | Loss = 0.07323785126209259\nEpoch 165 took 15.23s | Cumulative time (h:mm:ss): 0:41:04 | Loss = 0.0635899156332016\nEpoch 166 took 15.15s | Cumulative time (h:mm:ss): 0:41:19 | Loss = 0.0617484413087368\nEpoch 167 took 15.32s | Cumulative time (h:mm:ss): 0:41:34 | Loss = 0.06329020857810974\nEpoch 168 took 15.16s | Cumulative time (h:mm:ss): 0:41:49 | Loss = 0.06471358984708786\nEpoch 169 took 14.99s | Cumulative time (h:mm:ss): 0:42:04 | Loss = 0.05998772382736206\nEpoch 170 took 15.01s | Cumulative time (h:mm:ss): 0:42:19 | Loss = 0.056112512946128845\nEpoch 171 took 14.95s | Cumulative time (h:mm:ss): 0:42:34 | Loss = 0.06287723034620285\nEpoch 172 took 14.78s | Cumulative time (h:mm:ss): 0:42:49 | Loss = 0.0628647431731224\nEpoch 173 took 14.75s | Cumulative time (h:mm:ss): 0:43:04 | Loss = 0.068113774061203\nEpoch 174 took 14.95s | Cumulative time (h:mm:ss): 0:43:19 | Loss = 0.06360535323619843\nEpoch 175 took 15.52s | Cumulative time (h:mm:ss): 0:43:34 | Loss = 0.057681698352098465\n\tMSE-1 0.08826005458831787\n\tMSE-5 0.31080958247184753\n\tMSE-10 0.5493845343589783\n\tMSE-20 0.5713515281677246\n20 20000\n\tPSE 0.9307280925258087\n\tPSE per dim [0.9157156626673746, 0.9574884090127367, 0.9391967907105382, 0.9092955753951635, 0.9208296785355073, 0.9456246739053948, 0.9269458574539455]\nComputing KLx-GMM\n\tKLx 0.4547550678253174\nEpoch 176 took 15.12s | Cumulative time (h:mm:ss): 0:43:50 | Loss = 0.05930262804031372\nEpoch 177 took 15.1s | Cumulative time (h:mm:ss): 0:44:05 | Loss = 0.06570383161306381\nEpoch 178 took 15.47s | Cumulative time (h:mm:ss): 0:44:20 | Loss = 0.05970608815550804\nEpoch 179 took 15.38s | Cumulative time (h:mm:ss): 0:44:36 | Loss = 0.05725276470184326\nEpoch 180 took 15.01s | Cumulative time (h:mm:ss): 0:44:51 | Loss = 0.06706085056066513\nEpoch 181 took 15.32s | Cumulative time (h:mm:ss): 0:45:06 | Loss = 0.05834164097905159\nEpoch 182 took 15.12s | Cumulative time (h:mm:ss): 0:45:21 | Loss = 0.0581025630235672\nEpoch 183 took 15.22s | Cumulative time (h:mm:ss): 0:45:36 | Loss = 0.06543610244989395\nEpoch 184 took 14.88s | Cumulative time (h:mm:ss): 0:45:51 | Loss = 0.05770016834139824\nEpoch 185 took 14.99s | Cumulative time (h:mm:ss): 0:46:06 | Loss = 0.059411682188510895\nEpoch 186 took 14.77s | Cumulative time (h:mm:ss): 0:46:21 | Loss = 0.05756193399429321\nEpoch 187 took 15.0s | Cumulative time (h:mm:ss): 0:46:36 | Loss = 0.06349405646324158\nEpoch 188 took 14.93s | Cumulative time (h:mm:ss): 0:46:51 | Loss = 0.0627019926905632\nEpoch 189 took 15.14s | Cumulative time (h:mm:ss): 0:47:06 | Loss = 0.05569561570882797\nEpoch 190 took 14.83s | Cumulative time (h:mm:ss): 0:47:21 | Loss = 0.05923740938305855\nEpoch 191 took 14.65s | Cumulative time (h:mm:ss): 0:47:35 | Loss = 0.05789477750658989\nEpoch 192 took 14.6s | Cumulative time (h:mm:ss): 0:47:50 | Loss = 0.062126122415065765\nEpoch 193 took 14.65s | Cumulative time (h:mm:ss): 0:48:05 | Loss = 0.05982630327343941\nEpoch 194 took 14.74s | Cumulative time (h:mm:ss): 0:48:19 | Loss = 0.0596703439950943\nEpoch 195 took 15.22s | Cumulative time (h:mm:ss): 0:48:35 | Loss = 0.05655251443386078\nEpoch 196 took 15.11s | Cumulative time (h:mm:ss): 0:48:50 | Loss = 0.054045408964157104\nEpoch 197 took 14.68s | Cumulative time (h:mm:ss): 0:49:04 | Loss = 0.06657235324382782\nEpoch 198 took 14.66s | Cumulative time (h:mm:ss): 0:49:19 | Loss = 0.057779863476753235\nEpoch 199 took 15.0s | Cumulative time (h:mm:ss): 0:49:34 | Loss = 0.06885252147912979\nEpoch 200 took 14.74s | Cumulative time (h:mm:ss): 0:49:49 | Loss = 0.06168316677212715\n\tMSE-1 0.08156900107860565\n\tMSE-5 0.3026414215564728\n\tMSE-10 0.5382781028747559\n\tMSE-20 0.598763644695282\n20 20000\n\tPSE 0.9279706399307504\n\tPSE per dim [0.9142934637395163, 0.971075018122543, 0.9346073233660159, 0.8924943101440992, 0.9215324730537531, 0.9283070352184732, 0.9334848558708511]\nComputing KLx-GMM\n\tKLx 0.40633249282836914\nEpoch 201 took 14.69s | Cumulative time (h:mm:ss): 0:50:03 | Loss = 0.0667223408818245\nEpoch 202 took 14.71s | Cumulative time (h:mm:ss): 0:50:18 | Loss = 0.0592290423810482\nEpoch 203 took 14.68s | Cumulative time (h:mm:ss): 0:50:33 | Loss = 0.05808652937412262\nEpoch 204 took 14.73s | Cumulative time (h:mm:ss): 0:50:48 | Loss = 0.05457253009080887\nEpoch 205 took 14.76s | Cumulative time (h:mm:ss): 0:51:02 | Loss = 0.0513174906373024\nEpoch 206 took 14.62s | Cumulative time (h:mm:ss): 0:51:17 | Loss = 0.07149994373321533\nEpoch 207 took 14.98s | Cumulative time (h:mm:ss): 0:51:32 | Loss = 0.058896273374557495\nEpoch 208 took 14.6s | Cumulative time (h:mm:ss): 0:51:47 | Loss = 0.05824091285467148\nEpoch 209 took 14.64s | Cumulative time (h:mm:ss): 0:52:01 | Loss = 0.05599302798509598\nEpoch 210 took 14.69s | Cumulative time (h:mm:ss): 0:52:16 | Loss = 0.05582733824849129\nEpoch 211 took 14.7s | Cumulative time (h:mm:ss): 0:52:31 | Loss = 0.04947509244084358\nEpoch 212 took 14.64s | Cumulative time (h:mm:ss): 0:52:45 | Loss = 0.052028704434633255\nEpoch 213 took 14.65s | Cumulative time (h:mm:ss): 0:53:00 | Loss = 0.06045842543244362\nEpoch 214 took 14.96s | Cumulative time (h:mm:ss): 0:53:15 | Loss = 0.05360811576247215\nEpoch 215 took 15.29s | Cumulative time (h:mm:ss): 0:53:30 | Loss = 0.06279908120632172\nEpoch 216 took 14.98s | Cumulative time (h:mm:ss): 0:53:45 | Loss = 0.057042766362428665\nEpoch 217 took 14.56s | Cumulative time (h:mm:ss): 0:54:00 | Loss = 0.06358659267425537\nEpoch 218 took 14.61s | Cumulative time (h:mm:ss): 0:54:14 | Loss = 0.0646267682313919\nEpoch 219 took 14.55s | Cumulative time (h:mm:ss): 0:54:29 | Loss = 0.054431263357400894\nEpoch 220 took 14.71s | Cumulative time (h:mm:ss): 0:54:44 | Loss = 0.06226355955004692\nEpoch 221 took 14.59s | Cumulative time (h:mm:ss): 0:54:58 | Loss = 0.05154051631689072\nEpoch 222 took 15.09s | Cumulative time (h:mm:ss): 0:55:13 | Loss = 0.05541570857167244\nEpoch 223 took 14.81s | Cumulative time (h:mm:ss): 0:55:28 | Loss = 0.05415702611207962\nEpoch 224 took 14.62s | Cumulative time (h:mm:ss): 0:55:43 | Loss = 0.07004646956920624\nEpoch 225 took 14.94s | Cumulative time (h:mm:ss): 0:55:58 | Loss = 0.05564766377210617\n\tMSE-1 0.08110449463129044\n\tMSE-5 0.30022960901260376\n\tMSE-10 0.5350556373596191\n\tMSE-20 0.5799861550331116\n20 20000\n\tPSE 0.9046996915121746\n\tPSE per dim [0.8939478438489877, 0.9607530398855256, 0.9078905314225225, 0.8462776269883844, 0.8998003914862319, 0.8943869209912814, 0.9298414859622879]\nComputing KLx-GMM\n\tKLx 0.5714491605758667\nEpoch 226 took 14.61s | Cumulative time (h:mm:ss): 0:56:12 | Loss = 0.05776311457157135\nEpoch 227 took 14.55s | Cumulative time (h:mm:ss): 0:56:27 | Loss = 0.05836198851466179\nEpoch 228 took 14.57s | Cumulative time (h:mm:ss): 0:56:41 | Loss = 0.056915491819381714\nEpoch 229 took 14.94s | Cumulative time (h:mm:ss): 0:56:56 | Loss = 0.05799602344632149\nEpoch 230 took 14.65s | Cumulative time (h:mm:ss): 0:57:11 | Loss = 0.058072805404663086\nEpoch 231 took 14.71s | Cumulative time (h:mm:ss): 0:57:26 | Loss = 0.056386563926935196\nEpoch 232 took 14.79s | Cumulative time (h:mm:ss): 0:57:40 | Loss = 0.05542851984500885\nEpoch 233 took 14.72s | Cumulative time (h:mm:ss): 0:57:55 | Loss = 0.05237855762243271\nEpoch 234 took 14.72s | Cumulative time (h:mm:ss): 0:58:10 | Loss = 0.06248331069946289\nEpoch 235 took 15.23s | Cumulative time (h:mm:ss): 0:58:25 | Loss = 0.05410418286919594\nEpoch 236 took 15.01s | Cumulative time (h:mm:ss): 0:58:40 | Loss = 0.06342966109514236\nEpoch 237 took 14.65s | Cumulative time (h:mm:ss): 0:58:55 | Loss = 0.05406493321061134\nEpoch 238 took 14.67s | Cumulative time (h:mm:ss): 0:59:09 | Loss = 0.05331255868077278\nEpoch 239 took 14.7s | Cumulative time (h:mm:ss): 0:59:24 | Loss = 0.05799476057291031\nEpoch 240 took 14.78s | Cumulative time (h:mm:ss): 0:59:39 | Loss = 0.05591793358325958\nEpoch 241 took 14.85s | Cumulative time (h:mm:ss): 0:59:54 | Loss = 0.06187773868441582\nEpoch 242 took 14.69s | Cumulative time (h:mm:ss): 1:00:08 | Loss = 0.061713866889476776\nEpoch 243 took 14.83s | Cumulative time (h:mm:ss): 1:00:23 | Loss = 0.06587632745504379\nEpoch 244 took 15.13s | Cumulative time (h:mm:ss): 1:00:38 | Loss = 0.05945366621017456\nEpoch 245 took 15.34s | Cumulative time (h:mm:ss): 1:00:54 | Loss = 0.0553782619535923\nEpoch 246 took 15.46s | Cumulative time (h:mm:ss): 1:01:09 | Loss = 0.05385758355259895\nEpoch 247 took 15.45s | Cumulative time (h:mm:ss): 1:01:25 | Loss = 0.05699080228805542\nEpoch 248 took 15.48s | Cumulative time (h:mm:ss): 1:01:40 | Loss = 0.05902780592441559\nEpoch 249 took 15.42s | Cumulative time (h:mm:ss): 1:01:56 | Loss = 0.0609101802110672\nEpoch 250 took 15.5s | Cumulative time (h:mm:ss): 1:02:11 | Loss = 0.07402464002370834\n\tMSE-1 0.08104237914085388\n\tMSE-5 0.3004508912563324\n\tMSE-10 0.5358701348304749\n\tMSE-20 0.5829234719276428\n20 20000\n\tPSE 0.9310101722618983\n\tPSE per dim [0.9266668213606482, 0.9659690353465462, 0.9366224872515674, 0.9032985463866483, 0.9186694404682402, 0.9302413034208306, 0.9356035715988071]\nComputing KLx-GMM\n\tKLx 0.49201875925064087\n--- Training finished ---\n \n✅ Saved model to /kaggle/working/final_trained_model.pt\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}