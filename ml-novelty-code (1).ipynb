{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Cell 1: Environment Setup & Reset ---\n\n!rm -rf /kaggle/working/dendPLRNN\n!git clone --depth 1 https://github.com/DurstewitzLab/dendPLRNN.git /kaggle/working/dendPLRNN\nprint(\"✅ Repository cloned.\")\n\n# Install dependencies\n!pip install numpy scipy scikit-learn matplotlib tensorboardX\n!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\nprint(\"✅ Dependencies installed.\")\n\n# Ensure Python can find project modules\nimport sys\nsys.path.insert(0, \"/kaggle/working/dendPLRNN/BPTT_TF\")\nsys.path.insert(0, \"/kaggle/working/dendPLRNN\")\nprint(\"✅ PYTHONPATH ready.\")\n\n# Change to the correct directory\n%cd /kaggle/working/dendPLRNN/BPTT_TF\nprint(f\"✅ Current directory: $(pwd)\")\n\n# Fix known import and experiment issues\n!sed -i \"s/from pandas.core.indexes import numeric/from pandas.api.types import is_numeric_dtype as numeric/\" main_eval.py\n!sed -i \"s/Argument('n_epochs', \\[[0-9]*\\])/Argument('n_epochs', [10])/\" \\\n/kaggle/working/dendPLRNN/BPTT_TF/Experiments/Table1/ECG/ubermain.py\nprint(\"✅ Pandas and Epoch fixes applied.\")\n\n!nvidia-smi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T14:29:10.424338Z","iopub.execute_input":"2025-11-12T14:29:10.424569Z","iopub.status.idle":"2025-11-12T14:30:32.473051Z","shell.execute_reply.started":"2025-11-12T14:29:10.424544Z","shell.execute_reply":"2025-11-12T14:30:32.472325Z"}},"outputs":[{"name":"stdout","text":"Cloning into '/kaggle/working/dendPLRNN'...\nremote: Enumerating objects: 137, done.\u001b[K\nremote: Counting objects: 100% (137/137), done.\u001b[K\nremote: Compressing objects: 100% (117/117), done.\u001b[K\nremote: Total 137 (delta 27), reused 122 (delta 19), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (137/137), 60.51 MiB | 40.87 MiB/s, done.\nResolving deltas: 100% (27/27), done.\n✅ Repository cloned.\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.3)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.2)\nCollecting tensorboardX\n  Downloading tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.3.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (6.33.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\nDownloading tensorboardx-2.6.4-py3-none-any.whl (87 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: tensorboardX\nSuccessfully installed tensorboardX-2.6.4\nLooking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu113\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\nRequirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.20.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.15.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.10.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m98.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n✅ Dependencies installed.\n✅ PYTHONPATH ready.\n/kaggle/working/dendPLRNN/BPTT_TF\n✅ Current directory: $(pwd)\n✅ Pandas and Epoch fixes applied.\nWed Nov 12 14:30:32 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n| N/A   35C    P0             27W /  250W |       0MiB /  16384MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# --- Cell 2 (CORRECTED): Python-based File Modification ---\n\nimport os\nimport textwrap # <-- This import fixes the indentation\n\n# --- 1. Define the \"Novelty\" Module Code ---\nnovelty_code = \"\"\"\n\\\"\\\"\\\"\nMinimal, defensive novelty utilities for PLRNN training.\nKeeps logic isolated to avoid repeated in-place edits of big files.\n\\\"\\\"\\\"\n\nimport torch as tc\nimport torch.nn as nn\n\ndef inject_latent_noise(x, training=True, noise_std=0.02):\n    \\\"\\\"\\\"Add Gaussian noise to input tensor x if training is True.\\\"\\\"\\\"\n    if not training:\n        return x\n    try:\n        return x + tc.randn_like(x) * float(noise_std)\n    except Exception:\n        return x\n\ndef manifold_attractor_regularization(model, lambda_mar=0.01, frac=0.2):\n    \\\"\\\"\\\"\n    Minimal manifold-attractor regularizer.\n    - If model has attribute 'A' (tensor or parameter), compute small penalty.\n    - Returns a scalar tensor on same device; returns 0.0 tensor if not applicable.\n    \\\"\\\"\\\"\n    # safe guards\n    try:\n        if not hasattr(model, \"A\"):\n            return tc.tensor(0.0, device=next(model.parameters()).device)\n        A = getattr(model, \"A\")\n        # convert to tensor if Parameter\n        if isinstance(A, nn.Parameter):\n            A = A.data\n        A = tc.as_tensor(A)\n        device = A.device\n        W = getattr(model, \"W\", None)\n        h = getattr(model, \"h\", None)\n        M = A.shape[0]\n        Mreg = max(1, int(M * float(frac)))\n        A_diag = tc.diag(A)\n        mar_loss = tc.tensor(0.0, device=device)\n        for i in range(Mreg):\n            mar_loss = mar_loss + (A_diag[i] - 1.0) ** 2\n            if h is not None:\n                try:\n                    mar_loss = mar_loss + (tc.as_tensor(h[i]) ** 2)\n                except Exception:\n                    pass\n            if W is not None:\n                try:\n                    row = tc.as_tensor(W[i, :])\n                    mar_loss = mar_loss + (tc.sum(row ** 2) - row[i] ** 2)\n                except Exception:\n                    pass\n        return float(lambda_mar) * mar_loss\n    except Exception:\n        # On any unexpected issue, return zero tensor safely\n        try:\n            return tc.tensor(0.0, device=next(model.parameters()).device)\n        except Exception:\n            return tc.tensor(0.0)\n\ndef temporal_self_distillation_from_model(model, weight=0.05):\n    \\\"\\\"\\\"\n    Try to extract a latent trajectory from the model and compute MSE between adjacent timesteps.\n    Looks for common attributes (Z, last_z, z_hist). If none found, returns 0.\n    \\\"\\\"\\\"\n    try:\n        z = None\n        for attr in (\"Z\", \"z_hist\", \"last_z\", \"Z_hist\"):\n            if hasattr(model, attr):\n                z = getattr(model, attr)\n                break\n        # if z is a list or tuple, coerce to tensor if possible\n        if z is None:\n            return tc.tensor(0.0, device=next(model.parameters()).device)\n        \n        # Ensure z is a tensor\n        if not isinstance(z, tc.Tensor):\n            # Try to convert list of tensors, e.g., from a .cpu() loop\n            if isinstance(z, (list, tuple)) and len(z) > 1 and isinstance(z[0], tc.Tensor):\n                 zt = tc.stack(z)\n            else:\n                 zt = tc.as_tensor(z)\n        else:\n            zt = z\n\n        if zt.dim() < 2 or zt.shape[0] < 2:\n            return tc.tensor(0.0, device=zt.device)\n        diff = zt[1:] - zt[:-1].detach()\n        return float(weight) * tc.mean(diff ** 2)\n    except Exception:\n        try:\n            return tc.tensor(0.0, device=next(model.parameters()).device)\n        except Exception:\n            return tc.tensor(0.0)\n\ndef init_dendritic_if_possible(model, init_scale=0.05):\n    \\\"\\\"\\\"\n    If model has W and supports register_parameter, create and register 'U' parameter.\n    Safe no-op if not possible.\n    \\\"\\\"\\\"\n    try:\n        if hasattr(model, \"W\") and hasattr(model, \"register_parameter\") and not hasattr(model, \"U\"):\n            W = getattr(model, \"W\")\n            # create a parameter of same shape as W\n            U = nn.Parameter(tc.randn_like(tc.as_tensor(W)) * float(init_scale))\n            model.register_parameter(\"U\", U)\n            return True\n    except Exception:\n        pass\n    return False\n\ndef apply_dendritic_gate(z, model):\n    \\\"\\\"\\\"\n    Apply gating if model has parameter U; else return z unchanged.\n    \\\"\\\"\\\"\n    try:\n        if hasattr(model, \"U\"):\n            U = getattr(model, \"U\")\n            # Apply dendritic-style gating\n            return tc.sigmoid(tc.matmul(z, U)) * tc.relu(z)\n    except Exception:\n        pass\n    # Fallback to standard relu if no U or if error\n    return tc.relu(z)\n\"\"\"\n\n# --- 2. Define the Code to Inject (DEDENTED) ---\n# This block is wrapped in textwrap.dedent()\n# This is the key fix that corrects the indentation.\ninsertion_block = textwrap.dedent(\"\"\"\n    # --- novelties (MAR / TSD / safe noise) ---\n    try:\n        # manifold-attractor regularization (safe)\n        mar = novelties.manifold_attractor_regularization(self.model, lambda_mar=0.01)\n        if isinstance(mar, float) or (hasattr(mar, \"item\") and callable(getattr(mar, \"item\"))):\n            loss = loss + (mar if isinstance(mar, float) else mar)\n    except Exception:\n        pass\n    try:\n        # temporal self-distillation (if model exposes latent trajectory)\n        tsd = novelties.temporal_self_distillation_from_model(self.model, weight=0.05)\n        if hasattr(tsd, \"item\"):\n            loss = loss + tsd\n    except Exception:\n        pass\n    try:\n        # add an extra small noise via helper (this is optional and safe)\n        if \"inp\" in locals():\n            inp = novelties.inject_latent_noise(inp, training=self.model.training if hasattr(self.model, \"training\") else True, noise_level=getattr(self, \"noise_level\", 0.02))\n    except Exception:\n        pass\n\"\"\")\n\n# Define file paths\nnovelties_file = \"bptt/novelties.py\"\nalgo_file = \"bptt/bptt_algorithm.py\"\n\n# --- 3. Write the new novelties.py file ---\ntry:\n    with open(novelties_file, \"w\") as f:\n        f.write(novelty_code)\n    print(f\"✅ Successfully created {novelties_file}\")\n\n    # --- 4. Patch bptt_algorithm.py ---\n    \n    # Add the import at the top\n    with open(algo_file, \"r\") as f:\n        content = f.read()\n    \n    if \"from bptt import novelties\" not in content:\n        content = \"from bptt import novelties\\n\" + content\n        with open(algo_file, \"w\") as f:\n            f.write(content)\n        print(f\"✅ Added import to {algo_file}\")\n    \n    # Insert the loss block\n    with open(algo_file, \"r\") as f:\n        lines = f.readlines()\n\n    new_lines = []\n    inserted = False\n    target_line = \"loss = self.compute_loss(pred, target)\"\n\n    for line in lines:\n        new_lines.append(line)\n        # Check if this is the target line and we haven't inserted yet\n        if target_line in line and not inserted:\n            # Find the indentation of the target line\n            indentation = line[:len(line) - len(line.lstrip())]\n            # Add the insertion block with the same indentation\n            indented_insertion = \"\\n\".join([f\"{indentation}{l}\" for l in insertion_block.splitlines() if l])\n            new_lines.append(indented_insertion + \"\\n\")\n            inserted = True\n            print(f\"✅ Successfully inserted novelty block into {algo_file}\")\n\n    if not inserted:\n        print(f\"⚠️ WARNING: Could not find target line '{target_line}' in {algo_file}. Patch failed.\")\n    else:\n        # Write the modified content back\n        with open(algo_file, \"w\") as f:\n            f.writelines(new_lines)\n        print(\"✅ File patching complete.\")\n\nexcept Exception as e:\n    print(f\"❌ An error occurred during file modification: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T14:31:51.645234Z","iopub.execute_input":"2025-11-12T14:31:51.645571Z","iopub.status.idle":"2025-11-12T14:31:51.658676Z","shell.execute_reply.started":"2025-11-12T14:31:51.645536Z","shell.execute_reply":"2025-11-12T14:31:51.657938Z"}},"outputs":[{"name":"stdout","text":"✅ Successfully created bptt/novelties.py\n✅ Added import to bptt/bptt_algorithm.py\n✅ Successfully inserted novelty block into bptt/bptt_algorithm.py\n✅ File patching complete.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# --- Cell 3: Syntax Check and Run Training ---\n\n!echo \"Running python syntax check...\"\n!python -m py_compile bptt/novelties.py\n!python -m py_compile bptt/bptt_algorithm.py\n!echo \"✅ Syntax OK.\"\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T14:38:26.061512Z","iopub.execute_input":"2025-11-12T14:38:26.061845Z","iopub.status.idle":"2025-11-12T14:38:26.693395Z","shell.execute_reply.started":"2025-11-12T14:38:26.061812Z","shell.execute_reply":"2025-11-12T14:38:26.692002Z"}},"outputs":[{"name":"stdout","text":"Running python syntax check...\n✅ Syntax OK.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# --- GPU Diagnostic Cell ---\nimport torch\n\nprint(f\"PyTorch Version: {torch.__version__}\")\nprint(\"---\")\nprint(f\"Is CUDA (GPU) available? ==> {torch.cuda.is_available()}\")\nprint(\"---\")\n\nif torch.cuda.is_available():\n    print(f\"Current GPU Name: {torch.cuda.get_device_name(0)}\")\nelse:\n    print(\"WARNING: PyTorch cannot find the GPU.\")\n    print(\"Please go to 'Settings' on the right and ensure 'Accelerator' is set to 'GPU'.\")\n\nprint(\"\\n--- nvidia-smi (Hardware Check) ---\")\n!nvidia-smi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T14:38:36.768907Z","iopub.execute_input":"2025-11-12T14:38:36.769280Z","iopub.status.idle":"2025-11-12T14:38:38.972724Z","shell.execute_reply.started":"2025-11-12T14:38:36.769222Z","shell.execute_reply":"2025-11-12T14:38:38.971792Z"}},"outputs":[{"name":"stdout","text":"PyTorch Version: 2.6.0+cu124\n---\nIs CUDA (GPU) available? ==> True\n---\nCurrent GPU Name: Tesla P100-PCIE-16GB\n\n--- nvidia-smi (Hardware Check) ---\nWed Nov 12 14:38:38 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n| N/A   35C    P0             27W /  250W |       3MiB /  16384MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# --- Check Command: Add a \"print\" statement ---\n\n!sed -i \"/def manifold_attractor_regularization/a \\    print('--- NOVELTY CODE IS RUNNING (MAR) ---', flush=True)\" \\\n    /kaggle/working/dendPLRNN/BPTT_TF/bptt/novelties.py\n\nprint(\"✅ 'print' statement has been injected into 'bptt/novelties.py'.\")\nprint(\"   Ready for final verification.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T15:39:05.106487Z","iopub.execute_input":"2025-11-12T15:39:05.107504Z","iopub.status.idle":"2025-11-12T15:39:05.245504Z","shell.execute_reply.started":"2025-11-12T15:39:05.107450Z","shell.execute_reply":"2025-11-12T15:39:05.244619Z"}},"outputs":[{"name":"stdout","text":"✅ 'print' statement has been injected into 'bptt/novelties.py'.\n   Ready for final verification.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"!grep -n \"numeric\" main_eval.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T15:51:10.644514Z","iopub.execute_input":"2025-11-12T15:51:10.644827Z","iopub.status.idle":"2025-11-12T15:51:10.778917Z","shell.execute_reply.started":"2025-11-12T15:51:10.644798Z","shell.execute_reply":"2025-11-12T15:51:10.777995Z"}},"outputs":[{"name":"stdout","text":"3:from pandas.api.types import is_numeric_dtype as numeric\n260:        mse5 = (df.mean(0, numeric_only=True)['5'], df.std(numeric_only=True)['5'])\n261:        mse10 = (df.mean(0, numeric_only=True)['10'], df.std(numeric_only=True)['10'])\n262:        mse20 = (df.mean(0, numeric_only=True)['20'], df.std(numeric_only=True)['20'])\n272:        pse = (df.mean(0, numeric_only=True)['mean'], df.std(numeric_only=True)['mean'])\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"%cd /kaggle/working/dendPLRNN/BPTT_TF/Experiments/Table1/ECG\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T15:51:34.673541Z","iopub.execute_input":"2025-11-12T15:51:34.674469Z","iopub.status.idle":"2025-11-12T15:51:34.679989Z","shell.execute_reply.started":"2025-11-12T15:51:34.674428Z","shell.execute_reply":"2025-11-12T15:51:34.679218Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/dendPLRNN/BPTT_TF/Experiments/Table1/ECG\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"!sed -i \"s/Argument('use_gpu', \\[[0-9]\\+\\])/Argument('use_gpu', [1])/\" ubermain.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T15:51:44.930548Z","iopub.execute_input":"2025-11-12T15:51:44.930843Z","iopub.status.idle":"2025-11-12T15:51:45.060586Z","shell.execute_reply.started":"2025-11-12T15:51:44.930820Z","shell.execute_reply":"2025-11-12T15:51:45.059701Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"!grep -n \"use_gpu\" ubermain.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T15:52:01.786564Z","iopub.execute_input":"2025-11-12T15:52:01.787322Z","iopub.status.idle":"2025-11-12T15:52:01.917730Z","shell.execute_reply.started":"2025-11-12T15:52:01.787290Z","shell.execute_reply":"2025-11-12T15:52:01.916932Z"}},"outputs":[{"name":"stdout","text":"11:    When using GPU for training (i.e. Argument 'use_gpu 1')  it is generally\n17:    args.append(Argument('use_gpu', [1])) # may wanna use gpu here\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"!sed -i \"s/n_runs = [0-9]\\+/n_runs = 1/\" ubermain.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T15:52:26.783224Z","iopub.execute_input":"2025-11-12T15:52:26.783976Z","iopub.status.idle":"2025-11-12T15:52:26.913852Z","shell.execute_reply.started":"2025-11-12T15:52:26.783947Z","shell.execute_reply":"2025-11-12T15:52:26.912984Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"!grep -n \"n_runs\" ubermain.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T15:52:39.755229Z","iopub.execute_input":"2025-11-12T15:52:39.756003Z","iopub.status.idle":"2025-11-12T15:52:39.884501Z","shell.execute_reply.started":"2025-11-12T15:52:39.755967Z","shell.execute_reply":"2025-11-12T15:52:39.883800Z"}},"outputs":[{"name":"stdout","text":"4:def ubermain(n_runs):\n28:    args.append(Argument('run', list(range(1, 1 + n_runs))))\n36:    n_runs = 1\n42:    args = ubermain(n_runs)\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# --- Cell to Clean Up the Log ---\n# This finds our print statement in novelties.py and deletes it.\n\n!sed -i \"/--- NOVELTY CODE IS RUNNING (MAR) ---/d\" /kaggle/working/dendPLRNN/BPTT_TF/bptt/novelties.py\n\nprint(\"✅ 'novelties.py' has been cleaned.\")\nprint(\"You are now ready to run your final training with a clean log.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T16:00:27.923134Z","iopub.execute_input":"2025-11-12T16:00:27.923837Z","iopub.status.idle":"2025-11-12T16:00:28.056309Z","shell.execute_reply.started":"2025-11-12T16:00:27.923809Z","shell.execute_reply":"2025-11-12T16:00:28.055554Z"}},"outputs":[{"name":"stdout","text":"✅ 'novelties.py' has been cleaned.\nYou are now ready to run your final training with a clean log.\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"%cd /kaggle/working/dendPLRNN/BPTT_TF\n!PYTHONPATH=\"/kaggle/working/dendPLRNN/BPTT_TF:/kaggle/working/dendPLRNN\" \\\npython Experiments/Table1/ECG/ubermain.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T16:01:03.120777Z","iopub.execute_input":"2025-11-12T16:01:03.121371Z","iopub.status.idle":"2025-11-12T16:04:03.123728Z","shell.execute_reply.started":"2025-11-12T16:01:03.121342Z","shell.execute_reply":"2025-11-12T16:04:03.123003Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/dendPLRNN/BPTT_TF\n'use_gpu' flag is set.\nWill distribute tasks to GPUs automatically.\nThere are not enough GPU Resources available to spawn 20 processes. Reducing number of parallel runs to 1\nUsing device Tesla P100-PCIE-16GB for training (cuda:0).\nForcing interval set by user: 10\nEpoch 1 took 15.94s | Cumulative time (h:mm:ss): 0:00:15 | Loss = 0.9121284484863281\nEpoch 2 took 15.81s | Cumulative time (h:mm:ss): 0:00:31 | Loss = 0.8989867568016052\nEpoch 3 took 15.9s | Cumulative time (h:mm:ss): 0:00:47 | Loss = 0.9115966558456421\nEpoch 4 took 15.64s | Cumulative time (h:mm:ss): 0:01:03 | Loss = 0.8702396154403687\nEpoch 5 took 15.51s | Cumulative time (h:mm:ss): 0:01:18 | Loss = 0.8451351523399353\nEpoch 6 took 15.61s | Cumulative time (h:mm:ss): 0:01:34 | Loss = 0.8590642213821411\nEpoch 7 took 15.74s | Cumulative time (h:mm:ss): 0:01:50 | Loss = 0.912825882434845\nEpoch 8 took 15.59s | Cumulative time (h:mm:ss): 0:02:05 | Loss = 0.9110747575759888\nEpoch 9 took 15.61s | Cumulative time (h:mm:ss): 0:02:21 | Loss = 0.8892135620117188\nEpoch 10 took 15.58s | Cumulative time (h:mm:ss): 0:02:36 | Loss = 0.8892417550086975\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# --- Configuration Cell: Set 250 Epochs & 50-Epoch Metrics ---\n\n# We must be in the experiment directory to edit the file\n%cd /kaggle/working/dendPLRNN/BPTT_TF/Experiments/Table1/ECG\n\nprint(\"--- Modifying n_epochs ---\")\n# 1. Change n_epochs from [10] (or whatever it is) to [250]\n!sed -i \"s/Argument('n_epochs', \\[[0-9]*\\])/Argument('n_epochs', [250])/\" ubermain.py\n# Verify the change\n!grep -n \"n_epochs\" ubermain.py\n\nprint(\"\\n--- Modifying test_interval (for metrics) ---\")\n# 2. Change test_interval from its default (e.g., 10) to [50]\n!sed -i \"s/Argument('test_interval', \\[[0-9]*\\])/Argument('test_interval', [50])/\" ubermain.py\n# Verify the change\n!grep -n \"test_interval\" ubermain.py\n\n# 3. Go back to the main BPTT_TF directory\n%cd /kaggle/working/dendPLRNN/BPTT_TF\n\nprint(\"\\n✅ Configuration complete. Ready to run final training.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T16:24:30.441404Z","iopub.execute_input":"2025-11-12T16:24:30.442156Z","iopub.status.idle":"2025-11-12T16:24:30.941235Z","shell.execute_reply.started":"2025-11-12T16:24:30.442127Z","shell.execute_reply":"2025-11-12T16:24:30.940310Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/dendPLRNN/BPTT_TF/Experiments/Table1/ECG\n--- Modifying n_epochs ---\n23:    args.append(Argument('n_epochs', [250]))\n\n--- Modifying test_interval (for metrics) ---\n/kaggle/working/dendPLRNN/BPTT_TF\n\n✅ Configuration complete. Ready to run final training.\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# --- Cell 3 (FINAL EXPERIMENT): Run 250 Epochs ---\n\n!echo \"Running python syntax check...\"\n!python -m py_compile bptt/novelties.py\n!python -m py_compile bptt/bptt_algorithm.py\n!echo \"✅ Syntax OK.\"\n!echo \" \"\n\n!echo \"Launching FINAL 250-epoch training (metrics every 50 epochs)...\"\n\n# Get the full, absolute paths for PYTHONPATH\nROOT_DIR = \"/kaggle/working/dendPLRNN\"\nBPTT_DIR = \"/kaggle/working/dendPLRNN/BPTT_TF\"\nFULL_PYTHONPATH = f\"{BPTT_DIR}:{ROOT_DIR}\"\n\n# This command will now run your full experiment\n!PYTHONPATH=\"{FULL_PYTHONPATH}\" python -u Experiments/Table1/ECG/ubermain.py\n\n!echo \"--- Training finished ---\"\n!echo \" \"\n\n# Find and copy the latest model\n!bash -c 'LATEST_MODEL=$(find results -type f -name \"*.pt\" | sort | tail -n 1 || true); \\\nif [ -f \"$LATEST_MODEL\" ]; then \\\n  cp \"$LATEST_MODEL\" /kaggle/working/final_trained_model.pt; \\\n  echo \"✅ Saved model to /kaggle/working/final_trained_model.pt\"; \\\nelse \\\n  echo \"⚠️ No .pt checkpoint found in results/ — check the log above for errors.\"; \\\nfi'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T16:24:46.888063Z","iopub.execute_input":"2025-11-12T16:24:46.888398Z","iopub.status.idle":"2025-11-12T17:39:52.294857Z","shell.execute_reply.started":"2025-11-12T16:24:46.888369Z","shell.execute_reply":"2025-11-12T17:39:52.293843Z"}},"outputs":[{"name":"stdout","text":"Running python syntax check...\n✅ Syntax OK.\n \nLaunching FINAL 250-epoch training (metrics every 50 epochs)...\n'use_gpu' flag is set.\nWill distribute tasks to GPUs automatically.\nThere are not enough GPU Resources available to spawn 20 processes. Reducing number of parallel runs to 1\nUsing device Tesla P100-PCIE-16GB for training (cuda:0).\nForcing interval set by user: 10\nEpoch 1 took 15.66s | Cumulative time (h:mm:ss): 0:00:15 | Loss = 0.9285472631454468\nEpoch 2 took 15.56s | Cumulative time (h:mm:ss): 0:00:31 | Loss = 0.8544243574142456\nEpoch 3 took 15.38s | Cumulative time (h:mm:ss): 0:00:46 | Loss = 0.8121728897094727\nEpoch 4 took 15.51s | Cumulative time (h:mm:ss): 0:01:02 | Loss = 0.8122896552085876\nEpoch 5 took 15.41s | Cumulative time (h:mm:ss): 0:01:17 | Loss = 0.8150342702865601\nEpoch 6 took 15.71s | Cumulative time (h:mm:ss): 0:01:33 | Loss = 0.7717113494873047\nEpoch 7 took 15.54s | Cumulative time (h:mm:ss): 0:01:48 | Loss = 0.6030418276786804\nEpoch 8 took 15.45s | Cumulative time (h:mm:ss): 0:02:04 | Loss = 0.5144316554069519\nEpoch 9 took 15.51s | Cumulative time (h:mm:ss): 0:02:19 | Loss = 0.44526395201683044\nEpoch 10 took 15.72s | Cumulative time (h:mm:ss): 0:02:35 | Loss = 0.41079574823379517\nEpoch 11 took 15.5s | Cumulative time (h:mm:ss): 0:02:50 | Loss = 0.36089038848876953\nEpoch 12 took 15.46s | Cumulative time (h:mm:ss): 0:03:06 | Loss = 0.4095296859741211\nEpoch 13 took 15.39s | Cumulative time (h:mm:ss): 0:03:21 | Loss = 0.3160916566848755\nEpoch 14 took 15.4s | Cumulative time (h:mm:ss): 0:03:37 | Loss = 0.2668103575706482\nEpoch 15 took 15.62s | Cumulative time (h:mm:ss): 0:03:52 | Loss = 0.23768463730812073\nEpoch 16 took 15.71s | Cumulative time (h:mm:ss): 0:04:08 | Loss = 0.21168152987957\nEpoch 17 took 15.73s | Cumulative time (h:mm:ss): 0:04:24 | Loss = 0.25971969962120056\nEpoch 18 took 15.41s | Cumulative time (h:mm:ss): 0:04:39 | Loss = 0.19965068995952606\nEpoch 19 took 15.25s | Cumulative time (h:mm:ss): 0:04:54 | Loss = 0.17887063324451447\nEpoch 20 took 15.46s | Cumulative time (h:mm:ss): 0:05:10 | Loss = 0.13783496618270874\nEpoch 21 took 15.45s | Cumulative time (h:mm:ss): 0:05:25 | Loss = 0.1385607272386551\nEpoch 22 took 15.37s | Cumulative time (h:mm:ss): 0:05:41 | Loss = 0.15581096708774567\nEpoch 23 took 15.3s | Cumulative time (h:mm:ss): 0:05:56 | Loss = 0.1298959106206894\nEpoch 24 took 15.52s | Cumulative time (h:mm:ss): 0:06:11 | Loss = 0.11218144744634628\nEpoch 25 took 15.32s | Cumulative time (h:mm:ss): 0:06:27 | Loss = 0.11831025034189224\n\tMSE-1 0.11882200092077255\n\tMSE-5 0.29086869955062866\n\tMSE-10 0.5716002583503723\n\tMSE-20 0.6961192488670349\n20 20000\n\tPSE 0.8978113141400105\n\tPSE per dim [0.9622983892783384, 0.8937831751451578, 0.8533515954419915, 0.9548068094674536, 0.848039405137675, 0.9444886585052562, 0.8279111660042011]\nComputing KLx-GMM\n\tKLx 2.4223146438598633\nEpoch 26 took 15.55s | Cumulative time (h:mm:ss): 0:06:42 | Loss = 0.12810218334197998\nEpoch 27 took 15.53s | Cumulative time (h:mm:ss): 0:06:58 | Loss = 0.12068586051464081\nEpoch 28 took 16.16s | Cumulative time (h:mm:ss): 0:07:14 | Loss = 0.11706650257110596\nEpoch 29 took 15.78s | Cumulative time (h:mm:ss): 0:07:30 | Loss = 0.11678456515073776\nEpoch 30 took 16.61s | Cumulative time (h:mm:ss): 0:07:46 | Loss = 0.12389963120222092\nEpoch 31 took 16.52s | Cumulative time (h:mm:ss): 0:08:03 | Loss = 0.10168226808309555\nEpoch 32 took 16.43s | Cumulative time (h:mm:ss): 0:08:19 | Loss = 0.1000763401389122\nEpoch 33 took 16.13s | Cumulative time (h:mm:ss): 0:08:36 | Loss = 0.10100812464952469\nEpoch 34 took 16.25s | Cumulative time (h:mm:ss): 0:08:52 | Loss = 0.10640192776918411\nEpoch 35 took 15.99s | Cumulative time (h:mm:ss): 0:09:08 | Loss = 0.09719812870025635\nEpoch 36 took 15.95s | Cumulative time (h:mm:ss): 0:09:24 | Loss = 0.10469895601272583\nEpoch 37 took 15.9s | Cumulative time (h:mm:ss): 0:09:40 | Loss = 0.10087597370147705\nEpoch 38 took 16.03s | Cumulative time (h:mm:ss): 0:09:56 | Loss = 0.11116909235715866\nEpoch 39 took 16.07s | Cumulative time (h:mm:ss): 0:10:12 | Loss = 0.09848291426897049\nEpoch 40 took 16.13s | Cumulative time (h:mm:ss): 0:10:28 | Loss = 0.09652010351419449\nEpoch 41 took 15.83s | Cumulative time (h:mm:ss): 0:10:44 | Loss = 0.10511430352926254\nEpoch 42 took 15.51s | Cumulative time (h:mm:ss): 0:10:59 | Loss = 0.10112462192773819\nEpoch 43 took 15.76s | Cumulative time (h:mm:ss): 0:11:15 | Loss = 0.1011819988489151\nEpoch 44 took 15.76s | Cumulative time (h:mm:ss): 0:11:31 | Loss = 0.09652553498744965\nEpoch 45 took 15.45s | Cumulative time (h:mm:ss): 0:11:46 | Loss = 0.09654374420642853\nEpoch 46 took 15.44s | Cumulative time (h:mm:ss): 0:12:02 | Loss = 0.10094238817691803\nEpoch 47 took 15.58s | Cumulative time (h:mm:ss): 0:12:17 | Loss = 0.0998559296131134\nEpoch 48 took 15.56s | Cumulative time (h:mm:ss): 0:12:33 | Loss = 0.08929862082004547\nEpoch 49 took 15.39s | Cumulative time (h:mm:ss): 0:12:48 | Loss = 0.09781822562217712\nEpoch 50 took 15.35s | Cumulative time (h:mm:ss): 0:13:03 | Loss = 0.10166001319885254\n\tMSE-1 0.11770759522914886\n\tMSE-5 0.28461354970932007\n\tMSE-10 0.5509321093559265\n\tMSE-20 0.6958988308906555\n20 20000\n\tPSE 0.891701478376671\n\tPSE per dim [0.9677755845121045, 0.893744027748698, 0.8583975357562301, 0.9405812624804328, 0.8573835422324634, 0.8881145634059082, 0.8359138325008597]\nComputing KLx-GMM\n\tKLx 2.5341968536376953\nEpoch 51 took 15.23s | Cumulative time (h:mm:ss): 0:13:19 | Loss = 0.0958484336733818\nEpoch 52 took 15.44s | Cumulative time (h:mm:ss): 0:13:34 | Loss = 0.0929902046918869\nEpoch 53 took 15.47s | Cumulative time (h:mm:ss): 0:13:50 | Loss = 0.09252665936946869\nEpoch 54 took 15.27s | Cumulative time (h:mm:ss): 0:14:05 | Loss = 0.10434680432081223\nEpoch 55 took 15.51s | Cumulative time (h:mm:ss): 0:14:20 | Loss = 0.0923752635717392\nEpoch 56 took 15.79s | Cumulative time (h:mm:ss): 0:14:36 | Loss = 0.09489516168832779\nEpoch 57 took 15.67s | Cumulative time (h:mm:ss): 0:14:52 | Loss = 0.07901506870985031\nEpoch 58 took 15.53s | Cumulative time (h:mm:ss): 0:15:07 | Loss = 0.10052885860204697\nEpoch 59 took 15.76s | Cumulative time (h:mm:ss): 0:15:23 | Loss = 0.0872524231672287\nEpoch 60 took 15.4s | Cumulative time (h:mm:ss): 0:15:39 | Loss = 0.09390024840831757\nEpoch 61 took 15.72s | Cumulative time (h:mm:ss): 0:15:54 | Loss = 0.08789672702550888\nEpoch 62 took 15.5s | Cumulative time (h:mm:ss): 0:16:10 | Loss = 0.08306047320365906\nEpoch 63 took 15.35s | Cumulative time (h:mm:ss): 0:16:25 | Loss = 0.09106609970331192\nEpoch 64 took 15.43s | Cumulative time (h:mm:ss): 0:16:41 | Loss = 0.09430531412363052\nEpoch 65 took 15.43s | Cumulative time (h:mm:ss): 0:16:56 | Loss = 0.09498808532953262\nEpoch 66 took 16.3s | Cumulative time (h:mm:ss): 0:17:12 | Loss = 0.08782213926315308\nEpoch 67 took 16.56s | Cumulative time (h:mm:ss): 0:17:29 | Loss = 0.0942370742559433\nEpoch 68 took 17.58s | Cumulative time (h:mm:ss): 0:17:46 | Loss = 0.08366362750530243\nEpoch 69 took 17.18s | Cumulative time (h:mm:ss): 0:18:04 | Loss = 0.08936150372028351\nEpoch 70 took 17.47s | Cumulative time (h:mm:ss): 0:18:21 | Loss = 0.09173879772424698\nEpoch 71 took 17.36s | Cumulative time (h:mm:ss): 0:18:38 | Loss = 0.07990890741348267\nEpoch 72 took 18.12s | Cumulative time (h:mm:ss): 0:18:57 | Loss = 0.08583606034517288\nEpoch 73 took 17.85s | Cumulative time (h:mm:ss): 0:19:14 | Loss = 0.09532913565635681\nEpoch 74 took 17.73s | Cumulative time (h:mm:ss): 0:19:32 | Loss = 0.08355488628149033\nEpoch 75 took 18.08s | Cumulative time (h:mm:ss): 0:19:50 | Loss = 0.08081244677305222\n\tMSE-1 0.11396370083093643\n\tMSE-5 0.28618499636650085\n\tMSE-10 0.5347147583961487\n\tMSE-20 0.6906389594078064\n20 20000\n\tPSE 0.9081589438041137\n\tPSE per dim [0.9575050430908929, 0.9247783258477495, 0.8509062920672212, 0.9285685357106225, 0.8685670122503056, 0.9209987005277229, 0.9057886971342811]\nComputing KLx-GMM\n\tKLx 2.670642137527466\nEpoch 76 took 17.79s | Cumulative time (h:mm:ss): 0:20:08 | Loss = 0.08391331881284714\nEpoch 77 took 17.7s | Cumulative time (h:mm:ss): 0:20:26 | Loss = 0.08110468834638596\nEpoch 78 took 17.44s | Cumulative time (h:mm:ss): 0:20:43 | Loss = 0.08944042026996613\nEpoch 79 took 16.52s | Cumulative time (h:mm:ss): 0:21:00 | Loss = 0.08553595840930939\nEpoch 80 took 16.59s | Cumulative time (h:mm:ss): 0:21:16 | Loss = 0.08206024765968323\nEpoch 81 took 16.57s | Cumulative time (h:mm:ss): 0:21:33 | Loss = 0.0764840841293335\nEpoch 82 took 16.51s | Cumulative time (h:mm:ss): 0:21:49 | Loss = 0.07584909349679947\nEpoch 83 took 16.61s | Cumulative time (h:mm:ss): 0:22:06 | Loss = 0.08557813614606857\nEpoch 84 took 16.96s | Cumulative time (h:mm:ss): 0:22:23 | Loss = 0.08794742822647095\nEpoch 85 took 16.47s | Cumulative time (h:mm:ss): 0:22:39 | Loss = 0.07521089911460876\nEpoch 86 took 16.51s | Cumulative time (h:mm:ss): 0:22:56 | Loss = 0.08182875066995621\nEpoch 87 took 16.67s | Cumulative time (h:mm:ss): 0:23:13 | Loss = 0.08915572613477707\nEpoch 88 took 16.46s | Cumulative time (h:mm:ss): 0:23:29 | Loss = 0.07496120780706406\nEpoch 89 took 16.41s | Cumulative time (h:mm:ss): 0:23:45 | Loss = 0.07323665916919708\nEpoch 90 took 16.46s | Cumulative time (h:mm:ss): 0:24:02 | Loss = 0.07810785621404648\nEpoch 91 took 16.6s | Cumulative time (h:mm:ss): 0:24:18 | Loss = 0.07576297223567963\nEpoch 92 took 16.41s | Cumulative time (h:mm:ss): 0:24:35 | Loss = 0.09091877192258835\nEpoch 93 took 16.3s | Cumulative time (h:mm:ss): 0:24:51 | Loss = 0.07191494107246399\nEpoch 94 took 16.4s | Cumulative time (h:mm:ss): 0:25:08 | Loss = 0.07970720529556274\nEpoch 95 took 16.33s | Cumulative time (h:mm:ss): 0:25:24 | Loss = 0.0745951235294342\nEpoch 96 took 16.14s | Cumulative time (h:mm:ss): 0:25:40 | Loss = 0.07623465359210968\nEpoch 97 took 16.45s | Cumulative time (h:mm:ss): 0:25:56 | Loss = 0.07355767488479614\nEpoch 98 took 16.43s | Cumulative time (h:mm:ss): 0:26:13 | Loss = 0.07330834865570068\nEpoch 99 took 16.57s | Cumulative time (h:mm:ss): 0:26:29 | Loss = 0.07081038504838943\nEpoch 100 took 16.49s | Cumulative time (h:mm:ss): 0:26:46 | Loss = 0.07283549010753632\n\tMSE-1 0.11067064851522446\n\tMSE-5 0.28143274784088135\n\tMSE-10 0.5217238068580627\n\tMSE-20 0.6613772511482239\n20 20000\n\tPSE 0.9088274132395788\n\tPSE per dim [0.9459808394611368, 0.9028851094888364, 0.8962815401500942, 0.9024689750140005, 0.9002734995275048, 0.9011403980432893, 0.9127615309921896]\nComputing KLx-GMM\n\tKLx 2.6847236156463623\nEpoch 101 took 16.2s | Cumulative time (h:mm:ss): 0:27:02 | Loss = 0.07214627414941788\nEpoch 102 took 16.14s | Cumulative time (h:mm:ss): 0:27:18 | Loss = 0.06942550837993622\nEpoch 103 took 16.39s | Cumulative time (h:mm:ss): 0:27:35 | Loss = 0.07509610056877136\nEpoch 104 took 16.14s | Cumulative time (h:mm:ss): 0:27:51 | Loss = 0.07508869469165802\nEpoch 105 took 16.19s | Cumulative time (h:mm:ss): 0:28:07 | Loss = 0.08112777024507523\nEpoch 106 took 16.05s | Cumulative time (h:mm:ss): 0:28:23 | Loss = 0.07854339480400085\nEpoch 107 took 16.4s | Cumulative time (h:mm:ss): 0:28:39 | Loss = 0.06818877160549164\nEpoch 108 took 16.39s | Cumulative time (h:mm:ss): 0:28:56 | Loss = 0.07920248061418533\nEpoch 109 took 16.63s | Cumulative time (h:mm:ss): 0:29:13 | Loss = 0.07317794114351273\nEpoch 110 took 17.29s | Cumulative time (h:mm:ss): 0:29:30 | Loss = 0.0716429054737091\nEpoch 111 took 17.6s | Cumulative time (h:mm:ss): 0:29:47 | Loss = 0.07556752115488052\nEpoch 112 took 18.4s | Cumulative time (h:mm:ss): 0:30:06 | Loss = 0.07069642096757889\nEpoch 113 took 18.14s | Cumulative time (h:mm:ss): 0:30:24 | Loss = 0.07736872136592865\nEpoch 114 took 18.32s | Cumulative time (h:mm:ss): 0:30:42 | Loss = 0.07003311067819595\nEpoch 115 took 18.31s | Cumulative time (h:mm:ss): 0:31:01 | Loss = 0.07201555371284485\nEpoch 116 took 18.41s | Cumulative time (h:mm:ss): 0:31:19 | Loss = 0.06848101317882538\nEpoch 117 took 18.47s | Cumulative time (h:mm:ss): 0:31:37 | Loss = 0.0648123249411583\nEpoch 118 took 18.37s | Cumulative time (h:mm:ss): 0:31:56 | Loss = 0.07154946029186249\nEpoch 119 took 18.33s | Cumulative time (h:mm:ss): 0:32:14 | Loss = 0.06186196580529213\nEpoch 120 took 18.47s | Cumulative time (h:mm:ss): 0:32:33 | Loss = 0.06428048759698868\nEpoch 121 took 18.29s | Cumulative time (h:mm:ss): 0:32:51 | Loss = 0.07227139919996262\nEpoch 122 took 18.46s | Cumulative time (h:mm:ss): 0:33:09 | Loss = 0.07675125449895859\nEpoch 123 took 16.72s | Cumulative time (h:mm:ss): 0:33:26 | Loss = 0.06652294099330902\nEpoch 124 took 16.22s | Cumulative time (h:mm:ss): 0:33:42 | Loss = 0.07315424084663391\nEpoch 125 took 16.03s | Cumulative time (h:mm:ss): 0:33:58 | Loss = 0.07323063164949417\n\tMSE-1 0.10973603278398514\n\tMSE-5 0.2719701826572418\n\tMSE-10 0.4984961450099945\n\tMSE-20 0.6291332840919495\n20 20000\n\tPSE 0.8806310257928498\n\tPSE per dim [0.9199338575605573, 0.8606348926755598, 0.8611941487553427, 0.848595914410174, 0.9135312756413758, 0.8814914569815544, 0.8790356345253847]\nComputing KLx-GMM\n\tKLx 4.01835823059082\nEpoch 126 took 15.98s | Cumulative time (h:mm:ss): 0:34:14 | Loss = 0.06756885349750519\nEpoch 127 took 15.52s | Cumulative time (h:mm:ss): 0:34:30 | Loss = 0.06934485584497452\nEpoch 128 took 15.3s | Cumulative time (h:mm:ss): 0:34:45 | Loss = 0.06911858916282654\nEpoch 129 took 15.67s | Cumulative time (h:mm:ss): 0:35:01 | Loss = 0.05931422486901283\nEpoch 130 took 15.43s | Cumulative time (h:mm:ss): 0:35:16 | Loss = 0.06869129091501236\nEpoch 131 took 15.33s | Cumulative time (h:mm:ss): 0:35:32 | Loss = 0.06980147957801819\nEpoch 132 took 15.28s | Cumulative time (h:mm:ss): 0:35:47 | Loss = 0.06216411665081978\nEpoch 133 took 15.35s | Cumulative time (h:mm:ss): 0:36:02 | Loss = 0.0642833486199379\nEpoch 134 took 15.38s | Cumulative time (h:mm:ss): 0:36:18 | Loss = 0.07029411941766739\nEpoch 135 took 15.39s | Cumulative time (h:mm:ss): 0:36:33 | Loss = 0.06164839491248131\nEpoch 136 took 15.24s | Cumulative time (h:mm:ss): 0:36:48 | Loss = 0.06065469607710838\nEpoch 137 took 15.38s | Cumulative time (h:mm:ss): 0:37:04 | Loss = 0.06527601927518845\nEpoch 138 took 15.36s | Cumulative time (h:mm:ss): 0:37:19 | Loss = 0.0653492733836174\nEpoch 139 took 15.24s | Cumulative time (h:mm:ss): 0:37:34 | Loss = 0.0758792906999588\nEpoch 140 took 15.48s | Cumulative time (h:mm:ss): 0:37:50 | Loss = 0.06754467636346817\nEpoch 141 took 15.43s | Cumulative time (h:mm:ss): 0:38:05 | Loss = 0.07976914942264557\nEpoch 142 took 15.7s | Cumulative time (h:mm:ss): 0:38:21 | Loss = 0.07258786261081696\nEpoch 143 took 15.42s | Cumulative time (h:mm:ss): 0:38:36 | Loss = 0.06430718302726746\nEpoch 144 took 15.35s | Cumulative time (h:mm:ss): 0:38:52 | Loss = 0.05524595454335213\nEpoch 145 took 15.49s | Cumulative time (h:mm:ss): 0:39:07 | Loss = 0.06470704078674316\nEpoch 146 took 15.54s | Cumulative time (h:mm:ss): 0:39:23 | Loss = 0.06191670522093773\nEpoch 147 took 15.41s | Cumulative time (h:mm:ss): 0:39:38 | Loss = 0.08102262020111084\nEpoch 148 took 15.31s | Cumulative time (h:mm:ss): 0:39:53 | Loss = 0.06100400909781456\nEpoch 149 took 15.3s | Cumulative time (h:mm:ss): 0:40:09 | Loss = 0.06368394941091537\nEpoch 150 took 15.45s | Cumulative time (h:mm:ss): 0:40:24 | Loss = 0.059937480837106705\n\tMSE-1 0.10614528506994247\n\tMSE-5 0.2643819749355316\n\tMSE-10 0.4836631715297699\n\tMSE-20 0.5935107469558716\n20 20000\n\tPSE 0.8687588165900161\n\tPSE per dim [0.8327028543593114, 0.8279229688844647, 0.8786050440831522, 0.8133254420975673, 0.9558005509768849, 0.8847443274482882, 0.8882105282804447]\nComputing KLx-GMM\n\tKLx 5.094087600708008\nEpoch 151 took 15.27s | Cumulative time (h:mm:ss): 0:40:39 | Loss = 0.06872940063476562\nEpoch 152 took 15.4s | Cumulative time (h:mm:ss): 0:40:55 | Loss = 0.05622417852282524\nEpoch 153 took 15.49s | Cumulative time (h:mm:ss): 0:41:10 | Loss = 0.06008855625987053\nEpoch 154 took 15.58s | Cumulative time (h:mm:ss): 0:41:26 | Loss = 0.07028673589229584\nEpoch 155 took 15.43s | Cumulative time (h:mm:ss): 0:41:41 | Loss = 0.06591387838125229\nEpoch 156 took 15.96s | Cumulative time (h:mm:ss): 0:41:57 | Loss = 0.0603579618036747\nEpoch 157 took 15.74s | Cumulative time (h:mm:ss): 0:42:13 | Loss = 0.06515712291002274\nEpoch 158 took 15.77s | Cumulative time (h:mm:ss): 0:42:29 | Loss = 0.05797671899199486\nEpoch 159 took 16.0s | Cumulative time (h:mm:ss): 0:42:45 | Loss = 0.05333876237273216\nEpoch 160 took 16.56s | Cumulative time (h:mm:ss): 0:43:01 | Loss = 0.06040153652429581\nEpoch 161 took 15.99s | Cumulative time (h:mm:ss): 0:43:17 | Loss = 0.0785408765077591\nEpoch 162 took 15.76s | Cumulative time (h:mm:ss): 0:43:33 | Loss = 0.0685780867934227\nEpoch 163 took 15.85s | Cumulative time (h:mm:ss): 0:43:49 | Loss = 0.06097939983010292\nEpoch 164 took 15.91s | Cumulative time (h:mm:ss): 0:44:05 | Loss = 0.06386878341436386\nEpoch 165 took 15.96s | Cumulative time (h:mm:ss): 0:44:21 | Loss = 0.0551675409078598\nEpoch 166 took 15.64s | Cumulative time (h:mm:ss): 0:44:36 | Loss = 0.055190347135066986\nEpoch 167 took 15.79s | Cumulative time (h:mm:ss): 0:44:52 | Loss = 0.062332652509212494\nEpoch 168 took 16.02s | Cumulative time (h:mm:ss): 0:45:08 | Loss = 0.07130585610866547\nEpoch 169 took 15.68s | Cumulative time (h:mm:ss): 0:45:24 | Loss = 0.056947190314531326\nEpoch 170 took 15.83s | Cumulative time (h:mm:ss): 0:45:40 | Loss = 0.07161081582307816\nEpoch 171 took 15.78s | Cumulative time (h:mm:ss): 0:45:55 | Loss = 0.05414680764079094\nEpoch 172 took 15.82s | Cumulative time (h:mm:ss): 0:46:11 | Loss = 0.06346263736486435\nEpoch 173 took 15.93s | Cumulative time (h:mm:ss): 0:46:27 | Loss = 0.060756076127290726\nEpoch 174 took 15.89s | Cumulative time (h:mm:ss): 0:46:43 | Loss = 0.058037661015987396\nEpoch 175 took 16.1s | Cumulative time (h:mm:ss): 0:46:59 | Loss = 0.054269902408123016\n\tMSE-1 0.10534008592367172\n\tMSE-5 0.25878167152404785\n\tMSE-10 0.46889254450798035\n\tMSE-20 0.5751866698265076\n20 20000\n\tPSE 0.8880579430743307\n\tPSE per dim [0.9232997126311605, 0.9014218293852017, 0.8843294674901032, 0.8270931308907572, 0.9348352571421163, 0.8690625636875184, 0.8763636402934578]\nComputing KLx-GMM\n\tKLx 0.7878003120422363\nEpoch 176 took 16.33s | Cumulative time (h:mm:ss): 0:47:16 | Loss = 0.05614706873893738\nEpoch 177 took 16.28s | Cumulative time (h:mm:ss): 0:47:32 | Loss = 0.05518195405602455\nEpoch 178 took 16.01s | Cumulative time (h:mm:ss): 0:47:48 | Loss = 0.06775717437267303\nEpoch 179 took 16.58s | Cumulative time (h:mm:ss): 0:48:04 | Loss = 0.05390532687306404\nEpoch 180 took 17.02s | Cumulative time (h:mm:ss): 0:48:21 | Loss = 0.05150033161044121\nEpoch 181 took 16.19s | Cumulative time (h:mm:ss): 0:48:38 | Loss = 0.05221729353070259\nEpoch 182 took 16.2s | Cumulative time (h:mm:ss): 0:48:54 | Loss = 0.05698917806148529\nEpoch 183 took 15.94s | Cumulative time (h:mm:ss): 0:49:10 | Loss = 0.05662660300731659\nEpoch 184 took 16.43s | Cumulative time (h:mm:ss): 0:49:26 | Loss = 0.0740041434764862\nEpoch 185 took 17.02s | Cumulative time (h:mm:ss): 0:49:43 | Loss = 0.062054961919784546\nEpoch 186 took 16.73s | Cumulative time (h:mm:ss): 0:50:00 | Loss = 0.06684103608131409\nEpoch 187 took 16.46s | Cumulative time (h:mm:ss): 0:50:16 | Loss = 0.05159438028931618\nEpoch 188 took 16.56s | Cumulative time (h:mm:ss): 0:50:33 | Loss = 0.05776521563529968\nEpoch 189 took 16.51s | Cumulative time (h:mm:ss): 0:50:49 | Loss = 0.05421043932437897\nEpoch 190 took 16.23s | Cumulative time (h:mm:ss): 0:51:06 | Loss = 0.05153343826532364\nEpoch 191 took 15.7s | Cumulative time (h:mm:ss): 0:51:21 | Loss = 0.060632482171058655\nEpoch 192 took 15.81s | Cumulative time (h:mm:ss): 0:51:37 | Loss = 0.062114566564559937\nEpoch 193 took 15.8s | Cumulative time (h:mm:ss): 0:51:53 | Loss = 0.049763452261686325\nEpoch 194 took 15.64s | Cumulative time (h:mm:ss): 0:52:09 | Loss = 0.059432558715343475\nEpoch 195 took 15.91s | Cumulative time (h:mm:ss): 0:52:25 | Loss = 0.05025540292263031\nEpoch 196 took 16.08s | Cumulative time (h:mm:ss): 0:52:41 | Loss = 0.059113070368766785\nEpoch 197 took 15.92s | Cumulative time (h:mm:ss): 0:52:57 | Loss = 0.07524119317531586\nEpoch 198 took 15.8s | Cumulative time (h:mm:ss): 0:53:12 | Loss = 0.05970311537384987\nEpoch 199 took 15.8s | Cumulative time (h:mm:ss): 0:53:28 | Loss = 0.05399981513619423\nEpoch 200 took 16.02s | Cumulative time (h:mm:ss): 0:53:44 | Loss = 0.05298282206058502\n\tMSE-1 0.10107385367155075\n\tMSE-5 0.2545873522758484\n\tMSE-10 0.45735499262809753\n\tMSE-20 0.5424184203147888\n20 20000\n\tPSE 0.9418732639167453\n\tPSE per dim [0.9450215989884948, 0.9326480166875882, 0.9493726992962551, 0.9522179097952861, 0.9486781950937442, 0.9312690218535428, 0.9339054057023067]\nComputing KLx-GMM\n\tKLx 3.1289784908294678\nEpoch 201 took 15.75s | Cumulative time (h:mm:ss): 0:54:00 | Loss = 0.05215843766927719\nEpoch 202 took 15.77s | Cumulative time (h:mm:ss): 0:54:16 | Loss = 0.05283895507454872\nEpoch 203 took 15.86s | Cumulative time (h:mm:ss): 0:54:32 | Loss = 0.06332428008317947\nEpoch 204 took 15.9s | Cumulative time (h:mm:ss): 0:54:47 | Loss = 0.05066884309053421\nEpoch 205 took 15.68s | Cumulative time (h:mm:ss): 0:55:03 | Loss = 0.04762359708547592\nEpoch 206 took 15.71s | Cumulative time (h:mm:ss): 0:55:19 | Loss = 0.04805333539843559\nEpoch 207 took 15.7s | Cumulative time (h:mm:ss): 0:55:35 | Loss = 0.05481453239917755\nEpoch 208 took 16.08s | Cumulative time (h:mm:ss): 0:55:51 | Loss = 0.05323779582977295\nEpoch 209 took 15.74s | Cumulative time (h:mm:ss): 0:56:06 | Loss = 0.05002693459391594\nEpoch 210 took 15.7s | Cumulative time (h:mm:ss): 0:56:22 | Loss = 0.044038452208042145\nEpoch 211 took 15.69s | Cumulative time (h:mm:ss): 0:56:38 | Loss = 0.05532461404800415\nEpoch 212 took 15.84s | Cumulative time (h:mm:ss): 0:56:54 | Loss = 0.054859720170497894\nEpoch 213 took 15.9s | Cumulative time (h:mm:ss): 0:57:09 | Loss = 0.04910790175199509\nEpoch 214 took 15.86s | Cumulative time (h:mm:ss): 0:57:25 | Loss = 0.05234942212700844\nEpoch 215 took 15.68s | Cumulative time (h:mm:ss): 0:57:41 | Loss = 0.048917144536972046\nEpoch 216 took 15.86s | Cumulative time (h:mm:ss): 0:57:57 | Loss = 0.053535543382167816\nEpoch 217 took 15.66s | Cumulative time (h:mm:ss): 0:58:13 | Loss = 0.06227954849600792\nEpoch 218 took 15.76s | Cumulative time (h:mm:ss): 0:58:28 | Loss = 0.05651838332414627\nEpoch 219 took 15.86s | Cumulative time (h:mm:ss): 0:58:44 | Loss = 0.053331732749938965\nEpoch 220 took 16.09s | Cumulative time (h:mm:ss): 0:59:00 | Loss = 0.056283704936504364\nEpoch 221 took 16.19s | Cumulative time (h:mm:ss): 0:59:16 | Loss = 0.04837118461728096\nEpoch 222 took 15.94s | Cumulative time (h:mm:ss): 0:59:32 | Loss = 0.051045387983322144\nEpoch 223 took 16.18s | Cumulative time (h:mm:ss): 0:59:49 | Loss = 0.0492410883307457\nEpoch 224 took 15.91s | Cumulative time (h:mm:ss): 1:00:04 | Loss = 0.0529223307967186\nEpoch 225 took 16.01s | Cumulative time (h:mm:ss): 1:00:20 | Loss = 0.05114277824759483\n\tMSE-1 0.10163416713476181\n\tMSE-5 0.252692848443985\n\tMSE-10 0.4550991952419281\n\tMSE-20 0.529949963092804\n20 20000\n\tPSE 0.9443042890391381\n\tPSE per dim [0.9385354828792238, 0.9353125316635404, 0.9462970249259777, 0.9545733677787915, 0.9441718601636144, 0.9410562700858228, 0.9501834857769965]\nComputing KLx-GMM\n\tKLx 7.478603839874268\nEpoch 226 took 16.13s | Cumulative time (h:mm:ss): 1:00:37 | Loss = 0.057756636291742325\nEpoch 227 took 16.26s | Cumulative time (h:mm:ss): 1:00:53 | Loss = 0.05056292563676834\nEpoch 228 took 16.05s | Cumulative time (h:mm:ss): 1:01:09 | Loss = 0.05255173146724701\nEpoch 229 took 16.29s | Cumulative time (h:mm:ss): 1:01:25 | Loss = 0.052065491676330566\nEpoch 230 took 16.67s | Cumulative time (h:mm:ss): 1:01:42 | Loss = 0.04859308525919914\nEpoch 231 took 16.33s | Cumulative time (h:mm:ss): 1:01:58 | Loss = 0.0438404344022274\nEpoch 232 took 16.24s | Cumulative time (h:mm:ss): 1:02:14 | Loss = 0.04978656396269798\nEpoch 233 took 16.05s | Cumulative time (h:mm:ss): 1:02:30 | Loss = 0.04546197131276131\nEpoch 234 took 16.08s | Cumulative time (h:mm:ss): 1:02:47 | Loss = 0.06617124378681183\nEpoch 235 took 16.15s | Cumulative time (h:mm:ss): 1:03:03 | Loss = 0.05353723466396332\nEpoch 236 took 16.24s | Cumulative time (h:mm:ss): 1:03:19 | Loss = 0.04719948023557663\nEpoch 237 took 16.21s | Cumulative time (h:mm:ss): 1:03:35 | Loss = 0.06058134883642197\nEpoch 238 took 16.27s | Cumulative time (h:mm:ss): 1:03:51 | Loss = 0.05163887143135071\nEpoch 239 took 16.08s | Cumulative time (h:mm:ss): 1:04:08 | Loss = 0.05748441815376282\nEpoch 240 took 16.21s | Cumulative time (h:mm:ss): 1:04:24 | Loss = 0.04922177642583847\nEpoch 241 took 16.25s | Cumulative time (h:mm:ss): 1:04:40 | Loss = 0.05253445357084274\nEpoch 242 took 16.27s | Cumulative time (h:mm:ss): 1:04:56 | Loss = 0.05344479903578758\nEpoch 243 took 16.34s | Cumulative time (h:mm:ss): 1:05:13 | Loss = 0.05004361644387245\nEpoch 244 took 16.12s | Cumulative time (h:mm:ss): 1:05:29 | Loss = 0.062171537429094315\nEpoch 245 took 16.51s | Cumulative time (h:mm:ss): 1:05:45 | Loss = 0.045929014682769775\nEpoch 246 took 16.21s | Cumulative time (h:mm:ss): 1:06:01 | Loss = 0.04966781288385391\nEpoch 247 took 16.15s | Cumulative time (h:mm:ss): 1:06:18 | Loss = 0.05606471747159958\nEpoch 248 took 16.13s | Cumulative time (h:mm:ss): 1:06:34 | Loss = 0.057621702551841736\nEpoch 249 took 16.29s | Cumulative time (h:mm:ss): 1:06:50 | Loss = 0.047282714396715164\nEpoch 250 took 16.18s | Cumulative time (h:mm:ss): 1:07:06 | Loss = 0.0643593817949295\n\tMSE-1 0.10167814791202545\n\tMSE-5 0.2527868151664734\n\tMSE-10 0.45468658208847046\n\tMSE-20 0.5304433703422546\n20 20000\n\tPSE 0.90627217066221\n\tPSE per dim [0.9322095033061495, 0.8979453660034467, 0.9124519794862022, 0.9277749885665442, 0.9079666126193827, 0.8625953859587572, 0.9029613586949882]\nComputing KLx-GMM\n\tKLx 4.5031633377075195\n--- Training finished ---\n \n✅ Saved model to /kaggle/working/final_trained_model.pt\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# --- 2. Define the Code to Inject (DEDENTED) ---\ninsertion_block = textwrap.dedent(\"\"\"\n    # --- novelties (MAR / TSD / safe noise) ---\n\n    # --- MAR ABLATED ---\n    # try:\n    #     mar = novelties.manifold_attractor_regularization(self.model, lambda_mar=0.01)\n    #     if isinstance(mar, float) or (hasattr(mar, \"item\") and callable(getattr(mar, \"item\"))):\n    #         loss = loss + (mar if isinstance(mar, float) else mar)\n    # except Exception: pass\n\n    try:\n        tsd = novelties.temporal_self_distillation_from_model(self.model, weight=0.05)\n        if hasattr(tsd, \"item\"): loss = loss + tsd\n    except Exception: pass\n\n    try:\n        if \"inp\" in locals():\n            inp = novelties.inject_latent_noise(inp, training=self.model.training if hasattr(self.model, \"training\") else True, noise_level=getattr(self, \"noise_level\", 0.02))\n    except Exception: pass\n\"\"\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}