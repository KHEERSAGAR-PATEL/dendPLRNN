{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Cell 1: Environment Setup & Reset ---\n\n!rm -rf /kaggle/working/dendPLRNN\n!git clone --depth 1 https://github.com/DurstewitzLab/dendPLRNN.git /kaggle/working/dendPLRNN\nprint(\"✅ Repository cloned.\")\n\n# Install dependencies\n!pip install numpy scipy scikit-learn matplotlib tensorboardX\n!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\nprint(\"✅ Dependencies installed.\")\n\n# Ensure Python can find project modules\nimport sys\nsys.path.insert(0, \"/kaggle/working/dendPLRNN/BPTT_TF\")\nsys.path.insert(0, \"/kaggle/working/dendPLRNN\")\nprint(\"✅ PYTHONPATH ready.\")\n\n# Change to the correct directory\n%cd /kaggle/working/dendPLRNN/BPTT_TF\nprint(f\"✅ Current directory: $(pwd)\")\n\n# Fix known import and experiment issues\n!sed -i \"s/from pandas.core.indexes import numeric/from pandas.api.types import is_numeric_dtype as numeric/\" main_eval.py\n!sed -i \"s/Argument('n_epochs', \\[[0-9]*\\])/Argument('n_epochs', [10])/\" \\\n/kaggle/working/dendPLRNN/BPTT_TF/Experiments/Table1/ECG/ubermain.py\nprint(\"✅ Pandas and Epoch fixes applied.\")\n\n!nvidia-smi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:34:04.914645Z","iopub.execute_input":"2025-11-12T19:34:04.914876Z","iopub.status.idle":"2025-11-12T19:35:27.069227Z","shell.execute_reply.started":"2025-11-12T19:34:04.914857Z","shell.execute_reply":"2025-11-12T19:35:27.068455Z"}},"outputs":[{"name":"stdout","text":"Cloning into '/kaggle/working/dendPLRNN'...\nremote: Enumerating objects: 137, done.\u001b[K\nremote: Counting objects: 100% (137/137), done.\u001b[K\nremote: Compressing objects: 100% (117/117), done.\u001b[K\nremote: Total 137 (delta 27), reused 122 (delta 19), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (137/137), 60.51 MiB | 39.87 MiB/s, done.\nResolving deltas: 100% (27/27), done.\n✅ Repository cloned.\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.3)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.2)\nCollecting tensorboardX\n  Downloading tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.3.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (6.33.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\nDownloading tensorboardx-2.6.4-py3-none-any.whl (87 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: tensorboardX\nSuccessfully installed tensorboardX-2.6.4\nLooking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu113\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\nRequirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.20.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.15.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.10.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m98.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n✅ Dependencies installed.\n✅ PYTHONPATH ready.\n/kaggle/working/dendPLRNN/BPTT_TF\n✅ Current directory: $(pwd)\n✅ Pandas and Epoch fixes applied.\nWed Nov 12 19:35:26 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n| N/A   39C    P0             26W /  250W |       0MiB /  16384MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# --- Cell 2 (Ablate TSD): Installs MAR + Noise ONLY ---\n\nimport os\nimport textwrap\n\n# --- 1. Define the \"Novelty\" Module Code ---\n# This code is identical to your original\nnovelty_code = \"\"\"\n\\\"\\\"\\\"\nMinimal, defensive novelty utilities for PLRNN training.\nKeeps logic isolated to avoid repeated in-place edits of big files.\n\\\"\\\"\\\"\n\nimport torch as tc\nimport torch.nn as nn\n\ndef inject_latent_noise(x, training=True, noise_std=0.02):\n    \\\"\\\"\\\"Add Gaussian noise to input tensor x if training is True.\\\"\\\"\\\"\n    if not training:\n        return x\n    try:\n        return x + tc.randn_like(x) * float(noise_std)\n    except Exception:\n        return x\n\ndef manifold_attractor_regularization(model, lambda_mar=0.01, frac=0.2):\n    \\\"\\\"\\\"\n    Minimal manifold-attractor regularizer.\n    - If model has attribute 'A' (tensor or parameter), compute small penalty.\n    - Returns a scalar tensor on same device; returns 0.0 tensor if not applicable.\n    \\\"\\\"\\\"\n    # safe guards\n    try:\n        if not hasattr(model, \"A\"):\n            return tc.tensor(0.0, device=next(model.parameters()).device)\n        A = getattr(model, \"A\")\n        # convert to tensor if Parameter\n        if isinstance(A, nn.Parameter):\n            A = A.data\n        A = tc.as_tensor(A)\n        device = A.device\n        W = getattr(model, \"W\", None)\n        h = getattr(model, \"h\", None)\n        M = A.shape[0]\n        Mreg = max(1, int(M * float(frac)))\n        A_diag = tc.diag(A)\n        mar_loss = tc.tensor(0.0, device=device)\n        for i in range(Mreg):\n            mar_loss = mar_loss + (A_diag[i] - 1.0) ** 2\n            if h is not None:\n                try:\n                    mar_loss = mar_loss + (tc.as_tensor(h[i]) ** 2)\n                except Exception:\n                    pass\n            if W is not None:\n                try:\n                    row = tc.as_tensor(W[i, :])\n                    mar_loss = mar_loss + (tc.sum(row ** 2) - row[i] ** 2)\n                except Exception:\n                    pass\n        return float(lambda_mar) * mar_loss\n    except Exception:\n        # On any unexpected issue, return zero tensor safely\n        try:\n            return tc.tensor(0.0, device=next(model.parameters()).device)\n        except Exception:\n            return tc.tensor(0.0)\n\ndef temporal_self_distillation_from_model(model, weight=0.05):\n    \\\"\\\"\\\"\n    Try to extract a latent trajectory from the model and compute MSE between adjacent timesteps.\n    Looks for common attributes (Z, last_z, z_hist). If none found, returns 0.\n    \\\"\\\"\\\"\n    try:\n        z = None\n        for attr in (\"Z\", \"z_hist\", \"last_z\", \"Z_hist\"):\n            if hasattr(model, attr):\n                z = getattr(model, attr)\n                break\n        # if z is a list or tuple, coerce to tensor if possible\n        if z is None:\n            return tc.tensor(0.0, device=next(model.parameters()).device)\n        \n        # Ensure z is a tensor\n        if not isinstance(z, tc.Tensor):\n            # Try to convert list of tensors, e.g., from a .cpu() loop\n            if isinstance(z, (list, tuple)) and len(z) > 1 and isinstance(z[0], tc.Tensor):\n                 zt = tc.stack(z)\n            else:\n                 zt = tc.as_tensor(z)\n        else:\n            zt = z\n\n        if zt.dim() < 2 or zt.shape[0] < 2:\n            return tc.tensor(0.0, device=zt.device)\n        diff = zt[1:] - zt[:-1].detach()\n        return float(weight) * tc.mean(diff ** 2)\n    except Exception:\n        try:\n            return tc.tensor(0.0, device=next(model.parameters()).device)\n        except Exception:\n            return tc.tensor(0.0)\n\ndef init_dendritic_if_possible(model, init_scale=0.05):\n    \\\"\\\"\\\"\n    If model has W and supports register_parameter, create and register 'U' parameter.\n    Safe no-op if not possible.\n    \\\"\\\"\\\"\n    try:\n        if hasattr(model, \"W\") and hasattr(model, \"register_parameter\") and not hasattr(model, \"U\"):\n            W = getattr(model, \"W\")\n            # create a parameter of same shape as W\n            U = nn.Parameter(tc.randn_like(tc.as_tensor(W)) * float(init_scale))\n            model.register_parameter(\"U\", U)\n            return True\n    except Exception:\n        pass\n    return False\n\ndef apply_dendritic_gate(z, model):\n    \\\"\\\"\\\"\n    Apply gating if model has parameter U; else return z unchanged.\n    \\\"\\\"\\\"\n    try:\n        if hasattr(model, \"U\"):\n            U = getattr(model, \"U\")\n            # Apply dendritic-style gating\n            return tc.sigmoid(tc.matmul(z, U)) * tc.relu(z)\n    except Exception:\n        pass\n    # Fallback to standard relu if no U or if error\n    return tc.relu(z)\n\"\"\"\n\n# --- 2. Define the Code to Inject (TSD ABLATED) ---\n# This block is modified to comment out the TSD section.\ninsertion_block = textwrap.dedent(\"\"\"\n    # --- novelties (MAR + safe noise) ---\n    try:\n        # manifold-attractor regularization (safe)\n        mar = novelties.manifold_attractor_regularization(self.model, lambda_mar=0.01)\n        if isinstance(mar, float) or (hasattr(mar, \"item\") and callable(getattr(mar, \"item\"))):\n            loss = loss + (mar if isinstance(mar, float) else mar)\n    except Exception:\n        pass\n\n    # --- TSD ABLATED ---\n    # try:\n    #     # temporal self-distillation (if model exposes latent trajectory)\n    #     tsd = novelties.temporal_self_distillation_from_model(self.model, weight=0.05)\n    #     if hasattr(tsd, \"item\"):\n    #         loss = loss + tsd\n    # except Exception:\n    #     pass\n        \n    try:\n        # add an extra small noise via helper (this is optional and safe)\n        if \"inp\" in locals():\n            inp = novelties.inject_latent_noise(inp, training=self.model.training if hasattr(self.model, \"training\") else True, noise_level=getattr(self, \"noise_level\", 0.02))\n    except Exception:\n        pass\n\"\"\")\n\n# Define file paths\nnovelties_file = \"bptt/novelties.py\"\nalgo_file = \"bptt/bptt_algorithm.py\"\n\n# --- 3. Write the new novelties.py file ---\ntry:\n    with open(novelties_file, \"w\") as f:\n        f.write(novelty_code)\n    print(f\"✅ Successfully created {novelties_file}\")\n\n    # --- 4. Patch bptt_algorithm.py ---\n    \n    # Add the import at the top\n    with open(algo_file, \"r\") as f:\n        content = f.read()\n    \n    if \"from bptt import novelties\" not in content:\n        content = \"from bptt import novelties\\n\" + content\n        with open(algo_file, \"w\") as f:\n            f.write(content)\n        print(f\"✅ Added import to {algo_file}\")\n    \n    # Insert the loss block\n    with open(algo_file, \"r\") as f:\n        lines = f.readlines()\n\n    new_lines = []\n    inserted = False\n    target_line = \"loss = self.compute_loss(pred, target)\"\n\n    for line in lines:\n        new_lines.append(line)\n        # Check if this is the target line and we haven't inserted yet\n        if target_line in line and not inserted:\n            # Find the indentation of the target line\n            indentation = line[:len(line) - len(line.lstrip())]\n            # Add the insertion block with the same indentation\n            indented_insertion = \"\\n\".join([f\"{indentation}{l}\" for l in insertion_block.splitlines() if l])\n            new_lines.append(indented_insertion + \"\\n\")\n            inserted = True\n            print(f\"✅ Successfully inserted (Ablated-TSD) block into {algo_file}\")\n\n    if not inserted:\n        print(f\"⚠️ WARNING: Could not find target line '{target_line}' in {algo_file}. Patch failed.\")\n    else:\n        # Write the modified content back\n        with open(algo_file, \"w\") as f:\n            f.writelines(new_lines)\n        print(\"✅ File patching complete.\")\n\nexcept Exception as e:\n    print(f\"❌ An error occurred during file modification: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:35:27.073607Z","iopub.execute_input":"2025-11-12T19:35:27.074087Z","iopub.status.idle":"2025-11-12T19:35:27.086349Z","shell.execute_reply.started":"2025-11-12T19:35:27.074058Z","shell.execute_reply":"2025-11-12T19:35:27.085556Z"}},"outputs":[{"name":"stdout","text":"✅ Successfully created bptt/novelties.py\n✅ Added import to bptt/bptt_algorithm.py\n✅ Successfully inserted (Ablated-TSD) block into bptt/bptt_algorithm.py\n✅ File patching complete.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# --- Cell 3: Syntax Check and Run Training ---\n\n!echo \"Running python syntax check...\"\n!python -m py_compile bptt/novelties.py\n!python -m py_compile bptt/bptt_algorithm.py\n!echo \"✅ Syntax OK.\"\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:35:27.087208Z","iopub.execute_input":"2025-11-12T19:35:27.087484Z","iopub.status.idle":"2025-11-12T19:35:27.693436Z","shell.execute_reply.started":"2025-11-12T19:35:27.087468Z","shell.execute_reply":"2025-11-12T19:35:27.692702Z"}},"outputs":[{"name":"stdout","text":"Running python syntax check...\n✅ Syntax OK.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# --- GPU Diagnostic Cell ---\nimport torch\n\nprint(f\"PyTorch Version: {torch.__version__}\")\nprint(\"---\")\nprint(f\"Is CUDA (GPU) available? ==> {torch.cuda.is_available()}\")\nprint(\"---\")\n\nif torch.cuda.is_available():\n    print(f\"Current GPU Name: {torch.cuda.get_device_name(0)}\")\nelse:\n    print(\"WARNING: PyTorch cannot find the GPU.\")\n    print(\"Please go to 'Settings' on the right and ensure 'Accelerator' is set to 'GPU'.\")\n\nprint(\"\\n--- nvidia-smi (Hardware Check) ---\")\n!nvidia-smi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:35:27.694368Z","iopub.execute_input":"2025-11-12T19:35:27.694628Z","iopub.status.idle":"2025-11-12T19:35:30.829850Z","shell.execute_reply.started":"2025-11-12T19:35:27.694591Z","shell.execute_reply":"2025-11-12T19:35:30.828687Z"}},"outputs":[{"name":"stdout","text":"PyTorch Version: 2.6.0+cu124\n---\nIs CUDA (GPU) available? ==> True\n---\nCurrent GPU Name: Tesla P100-PCIE-16GB\n\n--- nvidia-smi (Hardware Check) ---\nWed Nov 12 19:35:30 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n| N/A   38C    P0             26W /  250W |       3MiB /  16384MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!grep -n \"numeric\" main_eval.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:35:30.832653Z","iopub.execute_input":"2025-11-12T19:35:30.833473Z","iopub.status.idle":"2025-11-12T19:35:30.963169Z","shell.execute_reply.started":"2025-11-12T19:35:30.833444Z","shell.execute_reply":"2025-11-12T19:35:30.962250Z"}},"outputs":[{"name":"stdout","text":"3:from pandas.api.types import is_numeric_dtype as numeric\n260:        mse5 = (df.mean(0, numeric_only=True)['5'], df.std(numeric_only=True)['5'])\n261:        mse10 = (df.mean(0, numeric_only=True)['10'], df.std(numeric_only=True)['10'])\n262:        mse20 = (df.mean(0, numeric_only=True)['20'], df.std(numeric_only=True)['20'])\n272:        pse = (df.mean(0, numeric_only=True)['mean'], df.std(numeric_only=True)['mean'])\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"%cd /kaggle/working/dendPLRNN/BPTT_TF/Experiments/Table1/ECG\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:35:30.964417Z","iopub.execute_input":"2025-11-12T19:35:30.964892Z","iopub.status.idle":"2025-11-12T19:35:34.166267Z","shell.execute_reply.started":"2025-11-12T19:35:30.964864Z","shell.execute_reply":"2025-11-12T19:35:34.165555Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/dendPLRNN/BPTT_TF/Experiments/Table1/ECG\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!sed -i \"s/Argument('use_gpu', \\[[0-9]\\+\\])/Argument('use_gpu', [1])/\" ubermain.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:35:34.167047Z","iopub.execute_input":"2025-11-12T19:35:34.167284Z","iopub.status.idle":"2025-11-12T19:35:34.302979Z","shell.execute_reply.started":"2025-11-12T19:35:34.167261Z","shell.execute_reply":"2025-11-12T19:35:34.302050Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"!grep -n \"use_gpu\" ubermain.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:35:34.304051Z","iopub.execute_input":"2025-11-12T19:35:34.304306Z","iopub.status.idle":"2025-11-12T19:35:34.428521Z","shell.execute_reply.started":"2025-11-12T19:35:34.304280Z","shell.execute_reply":"2025-11-12T19:35:34.427868Z"}},"outputs":[{"name":"stdout","text":"11:    When using GPU for training (i.e. Argument 'use_gpu 1')  it is generally\n17:    args.append(Argument('use_gpu', [1])) # may wanna use gpu here\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"!sed -i \"s/n_runs = [0-9]\\+/n_runs = 1/\" ubermain.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:35:34.429395Z","iopub.execute_input":"2025-11-12T19:35:34.429580Z","iopub.status.idle":"2025-11-12T19:35:34.553065Z","shell.execute_reply.started":"2025-11-12T19:35:34.429558Z","shell.execute_reply":"2025-11-12T19:35:34.552073Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"!grep -n \"n_runs\" ubermain.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:35:34.554183Z","iopub.execute_input":"2025-11-12T19:35:34.554435Z","iopub.status.idle":"2025-11-12T19:35:34.679781Z","shell.execute_reply.started":"2025-11-12T19:35:34.554410Z","shell.execute_reply":"2025-11-12T19:35:34.679090Z"}},"outputs":[{"name":"stdout","text":"4:def ubermain(n_runs):\n28:    args.append(Argument('run', list(range(1, 1 + n_runs))))\n36:    n_runs = 1\n42:    args = ubermain(n_runs)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# --- Configuration Cell: Set 250 Epochs & 50-Epoch Metrics ---\n\n# We must be in the experiment directory to edit the file\n%cd /kaggle/working/dendPLRNN/BPTT_TF/Experiments/Table1/ECG\n\nprint(\"--- Modifying n_epochs ---\")\n# 1. Change n_epochs from [10] (or whatever it is) to [250]\n!sed -i \"s/Argument('n_epochs', \\[[0-9]*\\])/Argument('n_epochs', [250])/\" ubermain.py\n# Verify the change\n!grep -n \"n_epochs\" ubermain.py\n\nprint(\"\\n--- Modifying test_interval (for metrics) ---\")\n# 2. Change test_interval from its default (e.g., 10) to [50]\n!sed -i \"s/Argument('test_interval', \\[[0-9]*\\])/Argument('test_interval', [50])/\" ubermain.py\n# Verify the change\n!grep -n \"test_interval\" ubermain.py\n\n# 3. Go back to the main BPTT_TF directory\n%cd /kaggle/working/dendPLRNN/BPTT_TF\n\nprint(\"\\n✅ Configuration complete. Ready to run final training.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:35:34.680954Z","iopub.execute_input":"2025-11-12T19:35:34.681212Z","iopub.status.idle":"2025-11-12T19:35:35.161013Z","shell.execute_reply.started":"2025-11-12T19:35:34.681186Z","shell.execute_reply":"2025-11-12T19:35:35.160239Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/dendPLRNN/BPTT_TF/Experiments/Table1/ECG\n--- Modifying n_epochs ---\n23:    args.append(Argument('n_epochs', [250]))\n\n--- Modifying test_interval (for metrics) ---\n/kaggle/working/dendPLRNN/BPTT_TF\n\n✅ Configuration complete. Ready to run final training.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# --- Cell 3 (FINAL EXPERIMENT): Run 250 Epochs ---\n\n!echo \"Running python syntax check...\"\n!python -m py_compile bptt/novelties.py\n!python -m py_compile bptt/bptt_algorithm.py\n!echo \"✅ Syntax OK.\"\n!echo \" \"\n\n!echo \"Launching FINAL 250-epoch training (metrics every 50 epochs)...\"\n\n# Get the full, absolute paths for PYTHONPATH\nROOT_DIR = \"/kaggle/working/dendPLRNN\"\nBPTT_DIR = \"/kaggle/working/dendPLRNN/BPTT_TF\"\nFULL_PYTHONPATH = f\"{BPTT_DIR}:{ROOT_DIR}\"\n\n# This command will now run your full experiment\n!PYTHONPATH=\"{FULL_PYTHONPATH}\" python -u Experiments/Table1/ECG/ubermain.py\n\n!echo \"--- Training finished ---\"\n!echo \" \"\n\n# Find and copy the latest model\n!bash -c 'LATEST_MODEL=$(find results -type f -name \"*.pt\" | sort | tail -n 1 || true); \\\nif [ -f \"$LATEST_MODEL\" ]; then \\\n  cp \"$LATEST_MODEL\" /kaggle/working/final_trained_model.pt; \\\n  echo \"✅ Saved model to /kaggle/working/final_trained_model.pt\"; \\\nelse \\\n  echo \"⚠️ No .pt checkpoint found in results/ — check the log above for errors.\"; \\\nfi'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:35:35.162049Z","iopub.execute_input":"2025-11-12T19:35:35.162306Z","iopub.status.idle":"2025-11-12T20:46:14.511854Z","shell.execute_reply.started":"2025-11-12T19:35:35.162283Z","shell.execute_reply":"2025-11-12T20:46:14.511076Z"}},"outputs":[{"name":"stdout","text":"Running python syntax check...\n✅ Syntax OK.\n \nLaunching FINAL 250-epoch training (metrics every 50 epochs)...\n'use_gpu' flag is set.\nWill distribute tasks to GPUs automatically.\nThere are not enough GPU Resources available to spawn 20 processes. Reducing number of parallel runs to 1\nUsing device Tesla P100-PCIE-16GB for training (cuda:0).\nForcing interval set by user: 10\nEpoch 1 took 15.6s | Cumulative time (h:mm:ss): 0:00:15 | Loss = 0.923313319683075\nEpoch 2 took 14.96s | Cumulative time (h:mm:ss): 0:00:30 | Loss = 0.9017714262008667\nEpoch 3 took 15.02s | Cumulative time (h:mm:ss): 0:00:45 | Loss = 0.8019206523895264\nEpoch 4 took 14.9s | Cumulative time (h:mm:ss): 0:01:00 | Loss = 0.8530234098434448\nEpoch 5 took 14.99s | Cumulative time (h:mm:ss): 0:01:15 | Loss = 0.7572692036628723\nEpoch 6 took 14.97s | Cumulative time (h:mm:ss): 0:01:30 | Loss = 0.7303807139396667\nEpoch 7 took 15.06s | Cumulative time (h:mm:ss): 0:01:45 | Loss = 0.6052403450012207\nEpoch 8 took 14.97s | Cumulative time (h:mm:ss): 0:02:00 | Loss = 0.49483081698417664\nEpoch 9 took 14.97s | Cumulative time (h:mm:ss): 0:02:15 | Loss = 0.3968490958213806\nEpoch 10 took 14.94s | Cumulative time (h:mm:ss): 0:02:30 | Loss = 0.32554134726524353\nEpoch 11 took 15.18s | Cumulative time (h:mm:ss): 0:02:45 | Loss = 0.3105514645576477\nEpoch 12 took 14.92s | Cumulative time (h:mm:ss): 0:03:00 | Loss = 0.29927483201026917\nEpoch 13 took 14.91s | Cumulative time (h:mm:ss): 0:03:15 | Loss = 0.2618517577648163\nEpoch 14 took 14.91s | Cumulative time (h:mm:ss): 0:03:30 | Loss = 0.23732072114944458\nEpoch 15 took 15.47s | Cumulative time (h:mm:ss): 0:03:45 | Loss = 0.21305394172668457\nEpoch 16 took 14.78s | Cumulative time (h:mm:ss): 0:04:00 | Loss = 0.18609875440597534\nEpoch 17 took 14.97s | Cumulative time (h:mm:ss): 0:04:15 | Loss = 0.18204748630523682\nEpoch 18 took 14.86s | Cumulative time (h:mm:ss): 0:04:30 | Loss = 0.17109207808971405\nEpoch 19 took 14.97s | Cumulative time (h:mm:ss): 0:04:45 | Loss = 0.1762153059244156\nEpoch 20 took 14.89s | Cumulative time (h:mm:ss): 0:05:00 | Loss = 0.13929061591625214\nEpoch 21 took 14.95s | Cumulative time (h:mm:ss): 0:05:15 | Loss = 0.1575508862733841\nEpoch 22 took 14.87s | Cumulative time (h:mm:ss): 0:05:30 | Loss = 0.13549958169460297\nEpoch 23 took 15.03s | Cumulative time (h:mm:ss): 0:05:45 | Loss = 0.13568797707557678\nEpoch 24 took 14.77s | Cumulative time (h:mm:ss): 0:05:59 | Loss = 0.12466327846050262\nEpoch 25 took 14.87s | Cumulative time (h:mm:ss): 0:06:14 | Loss = 0.13288165628910065\n\tMSE-1 0.13966584205627441\n\tMSE-5 0.2806917726993561\n\tMSE-10 0.48738032579421997\n\tMSE-20 0.7063091993331909\n20 20000\n\tPSE 0.920631322939368\n\tPSE per dim [0.8856258953548685, 0.901872107193733, 0.9521917352565219, 0.9581316580668645, 0.8943884019067989, 0.9105141273354473, 0.9416953354613424]\nComputing KLx-GMM\n\tKLx 0.5774612426757812\nEpoch 26 took 15.65s | Cumulative time (h:mm:ss): 0:06:30 | Loss = 0.11501946300268173\nEpoch 27 took 15.45s | Cumulative time (h:mm:ss): 0:06:45 | Loss = 0.11857549846172333\nEpoch 28 took 15.63s | Cumulative time (h:mm:ss): 0:07:01 | Loss = 0.10473766922950745\nEpoch 29 took 15.57s | Cumulative time (h:mm:ss): 0:07:17 | Loss = 0.11661405861377716\nEpoch 30 took 15.38s | Cumulative time (h:mm:ss): 0:07:32 | Loss = 0.11569158732891083\nEpoch 31 took 15.07s | Cumulative time (h:mm:ss): 0:07:47 | Loss = 0.13684192299842834\nEpoch 32 took 15.01s | Cumulative time (h:mm:ss): 0:08:02 | Loss = 0.1126202642917633\nEpoch 33 took 14.96s | Cumulative time (h:mm:ss): 0:08:17 | Loss = 0.10233137756586075\nEpoch 34 took 14.98s | Cumulative time (h:mm:ss): 0:08:32 | Loss = 0.11241082102060318\nEpoch 35 took 15.11s | Cumulative time (h:mm:ss): 0:08:47 | Loss = 0.10655604302883148\nEpoch 36 took 15.46s | Cumulative time (h:mm:ss): 0:09:03 | Loss = 0.10525462031364441\nEpoch 37 took 15.56s | Cumulative time (h:mm:ss): 0:09:18 | Loss = 0.09245192259550095\nEpoch 38 took 15.37s | Cumulative time (h:mm:ss): 0:09:33 | Loss = 0.10163725912570953\nEpoch 39 took 15.08s | Cumulative time (h:mm:ss): 0:09:49 | Loss = 0.09767541289329529\nEpoch 40 took 15.4s | Cumulative time (h:mm:ss): 0:10:04 | Loss = 0.10003942996263504\nEpoch 41 took 15.5s | Cumulative time (h:mm:ss): 0:10:19 | Loss = 0.10120879113674164\nEpoch 42 took 15.26s | Cumulative time (h:mm:ss): 0:10:35 | Loss = 0.11365329474210739\nEpoch 43 took 15.11s | Cumulative time (h:mm:ss): 0:10:50 | Loss = 0.1019478291273117\nEpoch 44 took 15.17s | Cumulative time (h:mm:ss): 0:11:05 | Loss = 0.0992511659860611\nEpoch 45 took 15.01s | Cumulative time (h:mm:ss): 0:11:20 | Loss = 0.1112259179353714\nEpoch 46 took 15.28s | Cumulative time (h:mm:ss): 0:11:35 | Loss = 0.12386123836040497\nEpoch 47 took 15.19s | Cumulative time (h:mm:ss): 0:11:50 | Loss = 0.09861042350530624\nEpoch 48 took 15.25s | Cumulative time (h:mm:ss): 0:12:06 | Loss = 0.10723574459552765\nEpoch 49 took 15.42s | Cumulative time (h:mm:ss): 0:12:21 | Loss = 0.10530237853527069\nEpoch 50 took 15.47s | Cumulative time (h:mm:ss): 0:12:37 | Loss = 0.10310248285531998\n\tMSE-1 0.14479094743728638\n\tMSE-5 0.2762235105037689\n\tMSE-10 0.4731123447418213\n\tMSE-20 0.7315609455108643\n20 20000\n\tPSE 0.8920862618018143\n\tPSE per dim [0.8909369007453904, 0.8449720398036901, 0.9331961660877892, 0.9390959133320317, 0.8285512426758039, 0.8899841982811177, 0.9178673716868765]\nComputing KLx-GMM\n\tKLx 0.8666968941688538\nEpoch 51 took 14.8s | Cumulative time (h:mm:ss): 0:12:51 | Loss = 0.09581544250249863\nEpoch 52 took 14.82s | Cumulative time (h:mm:ss): 0:13:06 | Loss = 0.10039202868938446\nEpoch 53 took 15.08s | Cumulative time (h:mm:ss): 0:13:21 | Loss = 0.0978873074054718\nEpoch 54 took 14.93s | Cumulative time (h:mm:ss): 0:13:36 | Loss = 0.09165708720684052\nEpoch 55 took 14.93s | Cumulative time (h:mm:ss): 0:13:51 | Loss = 0.09947042167186737\nEpoch 56 took 15.02s | Cumulative time (h:mm:ss): 0:14:06 | Loss = 0.09670519828796387\nEpoch 57 took 15.06s | Cumulative time (h:mm:ss): 0:14:21 | Loss = 0.08950775861740112\nEpoch 58 took 14.96s | Cumulative time (h:mm:ss): 0:14:36 | Loss = 0.08686866611242294\nEpoch 59 took 15.11s | Cumulative time (h:mm:ss): 0:14:51 | Loss = 0.09483414143323898\nEpoch 60 took 14.96s | Cumulative time (h:mm:ss): 0:15:06 | Loss = 0.08370460569858551\nEpoch 61 took 15.07s | Cumulative time (h:mm:ss): 0:15:21 | Loss = 0.09488682448863983\nEpoch 62 took 14.92s | Cumulative time (h:mm:ss): 0:15:36 | Loss = 0.0971045270562172\nEpoch 63 took 15.36s | Cumulative time (h:mm:ss): 0:15:52 | Loss = 0.08406799286603928\nEpoch 64 took 15.14s | Cumulative time (h:mm:ss): 0:16:07 | Loss = 0.09831652045249939\nEpoch 65 took 15.56s | Cumulative time (h:mm:ss): 0:16:22 | Loss = 0.09399718046188354\nEpoch 66 took 15.53s | Cumulative time (h:mm:ss): 0:16:38 | Loss = 0.10065670311450958\nEpoch 67 took 15.54s | Cumulative time (h:mm:ss): 0:16:53 | Loss = 0.08972450345754623\nEpoch 68 took 15.47s | Cumulative time (h:mm:ss): 0:17:09 | Loss = 0.08843862265348434\nEpoch 69 took 15.45s | Cumulative time (h:mm:ss): 0:17:24 | Loss = 0.08924168348312378\nEpoch 70 took 15.33s | Cumulative time (h:mm:ss): 0:17:40 | Loss = 0.08356675505638123\nEpoch 71 took 15.29s | Cumulative time (h:mm:ss): 0:17:55 | Loss = 0.08095396310091019\nEpoch 72 took 15.48s | Cumulative time (h:mm:ss): 0:18:10 | Loss = 0.08832933008670807\nEpoch 73 took 15.4s | Cumulative time (h:mm:ss): 0:18:26 | Loss = 0.08251865953207016\nEpoch 74 took 15.2s | Cumulative time (h:mm:ss): 0:18:41 | Loss = 0.09418097883462906\nEpoch 75 took 14.94s | Cumulative time (h:mm:ss): 0:18:56 | Loss = 0.09172984957695007\n\tMSE-1 0.1464763879776001\n\tMSE-5 0.2713107764720917\n\tMSE-10 0.46772605180740356\n\tMSE-20 0.7289613485336304\n20 20000\n\tPSE 0.8969470374788637\n\tPSE per dim [0.906636427164794, 0.8659138137604998, 0.8935452996338614, 0.9224998288256978, 0.8739938767073866, 0.8846133090094846, 0.9314267072503222]\nComputing KLx-GMM\n\tKLx 0.5790431499481201\nEpoch 76 took 15.01s | Cumulative time (h:mm:ss): 0:19:11 | Loss = 0.09057531505823135\nEpoch 77 took 15.1s | Cumulative time (h:mm:ss): 0:19:26 | Loss = 0.08666758239269257\nEpoch 78 took 15.02s | Cumulative time (h:mm:ss): 0:19:41 | Loss = 0.07730123400688171\nEpoch 79 took 15.02s | Cumulative time (h:mm:ss): 0:19:56 | Loss = 0.0909092128276825\nEpoch 80 took 14.92s | Cumulative time (h:mm:ss): 0:20:11 | Loss = 0.0873500257730484\nEpoch 81 took 15.06s | Cumulative time (h:mm:ss): 0:20:26 | Loss = 0.09199826419353485\nEpoch 82 took 14.95s | Cumulative time (h:mm:ss): 0:20:41 | Loss = 0.08264147490262985\nEpoch 83 took 15.1s | Cumulative time (h:mm:ss): 0:20:56 | Loss = 0.07322008907794952\nEpoch 84 took 15.14s | Cumulative time (h:mm:ss): 0:21:11 | Loss = 0.08726521581411362\nEpoch 85 took 15.07s | Cumulative time (h:mm:ss): 0:21:26 | Loss = 0.09351379424333572\nEpoch 86 took 15.38s | Cumulative time (h:mm:ss): 0:21:42 | Loss = 0.08380267024040222\nEpoch 87 took 15.03s | Cumulative time (h:mm:ss): 0:21:57 | Loss = 0.08943227678537369\nEpoch 88 took 14.95s | Cumulative time (h:mm:ss): 0:22:12 | Loss = 0.07982790470123291\nEpoch 89 took 14.98s | Cumulative time (h:mm:ss): 0:22:27 | Loss = 0.08013338595628738\nEpoch 90 took 14.94s | Cumulative time (h:mm:ss): 0:22:42 | Loss = 0.07633941620588303\nEpoch 91 took 15.08s | Cumulative time (h:mm:ss): 0:22:57 | Loss = 0.07892949134111404\nEpoch 92 took 15.01s | Cumulative time (h:mm:ss): 0:23:12 | Loss = 0.073745496571064\nEpoch 93 took 15.77s | Cumulative time (h:mm:ss): 0:23:27 | Loss = 0.07364121824502945\nEpoch 94 took 15.45s | Cumulative time (h:mm:ss): 0:23:43 | Loss = 0.07873668521642685\nEpoch 95 took 15.09s | Cumulative time (h:mm:ss): 0:23:58 | Loss = 0.07303743809461594\nEpoch 96 took 15.24s | Cumulative time (h:mm:ss): 0:24:13 | Loss = 0.0839112177491188\nEpoch 97 took 15.14s | Cumulative time (h:mm:ss): 0:24:28 | Loss = 0.08119694143533707\nEpoch 98 took 15.01s | Cumulative time (h:mm:ss): 0:24:43 | Loss = 0.07468251138925552\nEpoch 99 took 15.01s | Cumulative time (h:mm:ss): 0:24:58 | Loss = 0.07490753382444382\nEpoch 100 took 15.02s | Cumulative time (h:mm:ss): 0:25:13 | Loss = 0.07756507396697998\n\tMSE-1 0.1460561603307724\n\tMSE-5 0.2709694504737854\n\tMSE-10 0.4686852693557739\n\tMSE-20 0.7234464287757874\n20 20000\n\tPSE 0.8965814368179758\n\tPSE per dim [0.8641112510868715, 0.8779879295233037, 0.9405399144583257, 0.9429349977208792, 0.8846220488267043, 0.8362425028810574, 0.9296314132286883]\nComputing KLx-GMM\n\tKLx 1.8875362873077393\nEpoch 101 took 15.02s | Cumulative time (h:mm:ss): 0:25:28 | Loss = 0.08708066493272781\nEpoch 102 took 15.17s | Cumulative time (h:mm:ss): 0:25:44 | Loss = 0.07416797429323196\nEpoch 103 took 14.83s | Cumulative time (h:mm:ss): 0:25:58 | Loss = 0.07511567324399948\nEpoch 104 took 14.89s | Cumulative time (h:mm:ss): 0:26:13 | Loss = 0.07794757932424545\nEpoch 105 took 15.02s | Cumulative time (h:mm:ss): 0:26:28 | Loss = 0.07679566740989685\nEpoch 106 took 15.44s | Cumulative time (h:mm:ss): 0:26:44 | Loss = 0.06384891271591187\nEpoch 107 took 15.24s | Cumulative time (h:mm:ss): 0:26:59 | Loss = 0.08482523262500763\nEpoch 108 took 15.16s | Cumulative time (h:mm:ss): 0:27:14 | Loss = 0.06201823428273201\nEpoch 109 took 15.1s | Cumulative time (h:mm:ss): 0:27:29 | Loss = 0.07654924690723419\nEpoch 110 took 15.14s | Cumulative time (h:mm:ss): 0:27:44 | Loss = 0.07321944087743759\nEpoch 111 took 15.13s | Cumulative time (h:mm:ss): 0:28:00 | Loss = 0.06878620386123657\nEpoch 112 took 15.06s | Cumulative time (h:mm:ss): 0:28:15 | Loss = 0.06247420608997345\nEpoch 113 took 14.98s | Cumulative time (h:mm:ss): 0:28:30 | Loss = 0.07649547606706619\nEpoch 114 took 15.03s | Cumulative time (h:mm:ss): 0:28:45 | Loss = 0.06749408692121506\nEpoch 115 took 15.23s | Cumulative time (h:mm:ss): 0:29:00 | Loss = 0.07032633572816849\nEpoch 116 took 15.19s | Cumulative time (h:mm:ss): 0:29:15 | Loss = 0.07599306106567383\nEpoch 117 took 14.97s | Cumulative time (h:mm:ss): 0:29:30 | Loss = 0.06945652514696121\nEpoch 118 took 15.16s | Cumulative time (h:mm:ss): 0:29:45 | Loss = 0.0890515074133873\nEpoch 119 took 14.89s | Cumulative time (h:mm:ss): 0:30:00 | Loss = 0.0635794922709465\nEpoch 120 took 14.96s | Cumulative time (h:mm:ss): 0:30:15 | Loss = 0.07670444995164871\nEpoch 121 took 14.85s | Cumulative time (h:mm:ss): 0:30:30 | Loss = 0.08607437461614609\nEpoch 122 took 15.07s | Cumulative time (h:mm:ss): 0:30:45 | Loss = 0.06547972559928894\nEpoch 123 took 15.0s | Cumulative time (h:mm:ss): 0:31:00 | Loss = 0.07311461120843887\nEpoch 124 took 14.92s | Cumulative time (h:mm:ss): 0:31:15 | Loss = 0.06717640161514282\nEpoch 125 took 15.0s | Cumulative time (h:mm:ss): 0:31:30 | Loss = 0.07188913226127625\n\tMSE-1 0.13998742401599884\n\tMSE-5 0.2624332308769226\n\tMSE-10 0.4604035019874573\n\tMSE-20 0.7360749244689941\n20 20000\n\tPSE 0.8772766735871597\n\tPSE per dim [0.8579216584278275, 0.8727098813055767, 0.9298378278189693, 0.9377331108396112, 0.7713989298963555, 0.8282368351813673, 0.9430984716404114]\nComputing KLx-GMM\n\tKLx 0.6187156438827515\nEpoch 126 took 15.03s | Cumulative time (h:mm:ss): 0:31:45 | Loss = 0.06642904132604599\nEpoch 127 took 15.17s | Cumulative time (h:mm:ss): 0:32:00 | Loss = 0.07714756578207016\nEpoch 128 took 15.0s | Cumulative time (h:mm:ss): 0:32:15 | Loss = 0.07232511043548584\nEpoch 129 took 14.87s | Cumulative time (h:mm:ss): 0:32:30 | Loss = 0.06184826418757439\nEpoch 130 took 14.96s | Cumulative time (h:mm:ss): 0:32:45 | Loss = 0.06176600977778435\nEpoch 131 took 14.93s | Cumulative time (h:mm:ss): 0:33:00 | Loss = 0.07299447804689407\nEpoch 132 took 14.98s | Cumulative time (h:mm:ss): 0:33:15 | Loss = 0.08052284270524979\nEpoch 133 took 14.93s | Cumulative time (h:mm:ss): 0:33:30 | Loss = 0.08025810867547989\nEpoch 134 took 14.9s | Cumulative time (h:mm:ss): 0:33:45 | Loss = 0.06481115520000458\nEpoch 135 took 15.1s | Cumulative time (h:mm:ss): 0:34:00 | Loss = 0.07067351788282394\nEpoch 136 took 15.39s | Cumulative time (h:mm:ss): 0:34:15 | Loss = 0.058961525559425354\nEpoch 137 took 15.18s | Cumulative time (h:mm:ss): 0:34:30 | Loss = 0.06485048681497574\nEpoch 138 took 15.18s | Cumulative time (h:mm:ss): 0:34:46 | Loss = 0.06550167500972748\nEpoch 139 took 15.3s | Cumulative time (h:mm:ss): 0:35:01 | Loss = 0.06301102787256241\nEpoch 140 took 15.26s | Cumulative time (h:mm:ss): 0:35:16 | Loss = 0.05481889843940735\nEpoch 141 took 15.16s | Cumulative time (h:mm:ss): 0:35:31 | Loss = 0.06591992825269699\nEpoch 142 took 15.38s | Cumulative time (h:mm:ss): 0:35:47 | Loss = 0.060909222811460495\nEpoch 143 took 15.17s | Cumulative time (h:mm:ss): 0:36:02 | Loss = 0.06304722279310226\nEpoch 144 took 15.25s | Cumulative time (h:mm:ss): 0:36:17 | Loss = 0.058122631162405014\nEpoch 145 took 15.21s | Cumulative time (h:mm:ss): 0:36:32 | Loss = 0.06548894941806793\nEpoch 146 took 15.19s | Cumulative time (h:mm:ss): 0:36:47 | Loss = 0.06382996588945389\nEpoch 147 took 15.25s | Cumulative time (h:mm:ss): 0:37:03 | Loss = 0.05429510027170181\nEpoch 148 took 15.42s | Cumulative time (h:mm:ss): 0:37:18 | Loss = 0.06260082125663757\nEpoch 149 took 14.99s | Cumulative time (h:mm:ss): 0:37:33 | Loss = 0.06148421764373779\nEpoch 150 took 15.04s | Cumulative time (h:mm:ss): 0:37:48 | Loss = 0.0612056739628315\n\tMSE-1 0.13694176077842712\n\tMSE-5 0.2586742043495178\n\tMSE-10 0.4567892551422119\n\tMSE-20 0.7441460490226746\n20 20000\n\tPSE 0.8998281991625678\n\tPSE per dim [0.9390200946772287, 0.8830119047492767, 0.9384170061116959, 0.9054068050337007, 0.9150965007055851, 0.8724651291445898, 0.8453799537158976]\nComputing KLx-GMM\n\tKLx 0.5841470956802368\nEpoch 151 took 15.26s | Cumulative time (h:mm:ss): 0:38:03 | Loss = 0.06431614607572556\nEpoch 152 took 15.26s | Cumulative time (h:mm:ss): 0:38:19 | Loss = 0.06036226078867912\nEpoch 153 took 15.14s | Cumulative time (h:mm:ss): 0:38:34 | Loss = 0.07039924710988998\nEpoch 154 took 15.04s | Cumulative time (h:mm:ss): 0:38:49 | Loss = 0.061359018087387085\nEpoch 155 took 14.91s | Cumulative time (h:mm:ss): 0:39:04 | Loss = 0.05317825824022293\nEpoch 156 took 15.03s | Cumulative time (h:mm:ss): 0:39:19 | Loss = 0.06558061391115189\nEpoch 157 took 15.1s | Cumulative time (h:mm:ss): 0:39:34 | Loss = 0.06598790735006332\nEpoch 158 took 15.24s | Cumulative time (h:mm:ss): 0:39:49 | Loss = 0.07374206185340881\nEpoch 159 took 15.06s | Cumulative time (h:mm:ss): 0:40:04 | Loss = 0.07184015214443207\nEpoch 160 took 14.95s | Cumulative time (h:mm:ss): 0:40:19 | Loss = 0.05611738562583923\nEpoch 161 took 14.83s | Cumulative time (h:mm:ss): 0:40:34 | Loss = 0.06124219298362732\nEpoch 162 took 15.0s | Cumulative time (h:mm:ss): 0:40:49 | Loss = 0.0698806419968605\nEpoch 163 took 15.09s | Cumulative time (h:mm:ss): 0:41:04 | Loss = 0.06218797713518143\nEpoch 164 took 15.56s | Cumulative time (h:mm:ss): 0:41:20 | Loss = 0.0738203153014183\nEpoch 165 took 15.27s | Cumulative time (h:mm:ss): 0:41:35 | Loss = 0.06739873439073563\nEpoch 166 took 15.24s | Cumulative time (h:mm:ss): 0:41:50 | Loss = 0.06642532348632812\nEpoch 167 took 15.2s | Cumulative time (h:mm:ss): 0:42:05 | Loss = 0.05645618587732315\nEpoch 168 took 15.12s | Cumulative time (h:mm:ss): 0:42:20 | Loss = 0.05981558561325073\nEpoch 169 took 15.38s | Cumulative time (h:mm:ss): 0:42:36 | Loss = 0.06840959936380386\nEpoch 170 took 15.43s | Cumulative time (h:mm:ss): 0:42:51 | Loss = 0.06817600131034851\nEpoch 171 took 15.5s | Cumulative time (h:mm:ss): 0:43:07 | Loss = 0.05634019523859024\nEpoch 172 took 14.83s | Cumulative time (h:mm:ss): 0:43:22 | Loss = 0.06373907625675201\nEpoch 173 took 14.87s | Cumulative time (h:mm:ss): 0:43:36 | Loss = 0.06521827727556229\nEpoch 174 took 15.0s | Cumulative time (h:mm:ss): 0:43:51 | Loss = 0.060406651347875595\nEpoch 175 took 14.89s | Cumulative time (h:mm:ss): 0:44:06 | Loss = 0.06367235630750656\n\tMSE-1 0.1331060826778412\n\tMSE-5 0.25613197684288025\n\tMSE-10 0.44849511981010437\n\tMSE-20 0.6540614366531372\n20 20000\n\tPSE 0.9330697885822969\n\tPSE per dim [0.9483666965305882, 0.9327756115152757, 0.9590967102837948, 0.9430949353327784, 0.9519390567274233, 0.8924601602324743, 0.9037553494537436]\nComputing KLx-GMM\n\tKLx 1.2520297765731812\nEpoch 176 took 15.1s | Cumulative time (h:mm:ss): 0:44:21 | Loss = 0.062391940504312515\nEpoch 177 took 15.13s | Cumulative time (h:mm:ss): 0:44:37 | Loss = 0.05708514526486397\nEpoch 178 took 15.11s | Cumulative time (h:mm:ss): 0:44:52 | Loss = 0.07023268938064575\nEpoch 179 took 15.0s | Cumulative time (h:mm:ss): 0:45:07 | Loss = 0.07405716925859451\nEpoch 180 took 14.94s | Cumulative time (h:mm:ss): 0:45:22 | Loss = 0.061372287571430206\nEpoch 181 took 14.95s | Cumulative time (h:mm:ss): 0:45:37 | Loss = 0.05356459319591522\nEpoch 182 took 14.92s | Cumulative time (h:mm:ss): 0:45:51 | Loss = 0.06530922651290894\nEpoch 183 took 14.96s | Cumulative time (h:mm:ss): 0:46:06 | Loss = 0.06455473601818085\nEpoch 184 took 14.98s | Cumulative time (h:mm:ss): 0:46:21 | Loss = 0.05872603505849838\nEpoch 185 took 15.03s | Cumulative time (h:mm:ss): 0:46:36 | Loss = 0.05144178867340088\nEpoch 186 took 14.98s | Cumulative time (h:mm:ss): 0:46:51 | Loss = 0.061316557228565216\nEpoch 187 took 14.81s | Cumulative time (h:mm:ss): 0:47:06 | Loss = 0.05597700551152229\nEpoch 188 took 14.97s | Cumulative time (h:mm:ss): 0:47:21 | Loss = 0.06438660621643066\nEpoch 189 took 14.84s | Cumulative time (h:mm:ss): 0:47:36 | Loss = 0.054157618433237076\nEpoch 190 took 14.95s | Cumulative time (h:mm:ss): 0:47:51 | Loss = 0.06678242981433868\nEpoch 191 took 15.31s | Cumulative time (h:mm:ss): 0:48:06 | Loss = 0.057632192969322205\nEpoch 192 took 14.9s | Cumulative time (h:mm:ss): 0:48:21 | Loss = 0.05668477714061737\nEpoch 193 took 15.01s | Cumulative time (h:mm:ss): 0:48:36 | Loss = 0.05065513402223587\nEpoch 194 took 15.33s | Cumulative time (h:mm:ss): 0:48:52 | Loss = 0.058018092066049576\nEpoch 195 took 15.63s | Cumulative time (h:mm:ss): 0:49:07 | Loss = 0.05155765265226364\nEpoch 196 took 15.32s | Cumulative time (h:mm:ss): 0:49:22 | Loss = 0.057417210191488266\nEpoch 197 took 14.92s | Cumulative time (h:mm:ss): 0:49:37 | Loss = 0.05447155237197876\nEpoch 198 took 14.98s | Cumulative time (h:mm:ss): 0:49:52 | Loss = 0.058758679777383804\nEpoch 199 took 15.02s | Cumulative time (h:mm:ss): 0:50:07 | Loss = 0.053872618824243546\nEpoch 200 took 14.99s | Cumulative time (h:mm:ss): 0:50:22 | Loss = 0.051182955503463745\n\tMSE-1 0.12950308620929718\n\tMSE-5 0.2524043619632721\n\tMSE-10 0.44366225600242615\n\tMSE-20 0.6183444261550903\n20 20000\n\tPSE 0.9320475592363636\n\tPSE per dim [0.8957538246476768, 0.9228202350077702, 0.9612505781388155, 0.9595738747427428, 0.9452545658562601, 0.9171769354479226, 0.9225029008133572]\nComputing KLx-GMM\n\tKLx 1.2909330129623413\nEpoch 201 took 15.19s | Cumulative time (h:mm:ss): 0:50:38 | Loss = 0.05533481389284134\nEpoch 202 took 14.9s | Cumulative time (h:mm:ss): 0:50:52 | Loss = 0.051017966121435165\nEpoch 203 took 15.38s | Cumulative time (h:mm:ss): 0:51:08 | Loss = 0.05266139283776283\nEpoch 204 took 15.0s | Cumulative time (h:mm:ss): 0:51:23 | Loss = 0.06709851324558258\nEpoch 205 took 14.99s | Cumulative time (h:mm:ss): 0:51:38 | Loss = 0.049978967756032944\nEpoch 206 took 14.98s | Cumulative time (h:mm:ss): 0:51:53 | Loss = 0.05811059847474098\nEpoch 207 took 15.01s | Cumulative time (h:mm:ss): 0:52:08 | Loss = 0.05116419121623039\nEpoch 208 took 14.99s | Cumulative time (h:mm:ss): 0:52:23 | Loss = 0.05033540353178978\nEpoch 209 took 14.93s | Cumulative time (h:mm:ss): 0:52:38 | Loss = 0.05782054364681244\nEpoch 210 took 14.89s | Cumulative time (h:mm:ss): 0:52:53 | Loss = 0.052862443029880524\nEpoch 211 took 14.9s | Cumulative time (h:mm:ss): 0:53:08 | Loss = 0.050583988428115845\nEpoch 212 took 14.9s | Cumulative time (h:mm:ss): 0:53:22 | Loss = 0.05394233763217926\nEpoch 213 took 15.05s | Cumulative time (h:mm:ss): 0:53:38 | Loss = 0.05765927582979202\nEpoch 214 took 15.05s | Cumulative time (h:mm:ss): 0:53:53 | Loss = 0.054298266768455505\nEpoch 215 took 14.98s | Cumulative time (h:mm:ss): 0:54:08 | Loss = 0.05821467563509941\nEpoch 216 took 14.96s | Cumulative time (h:mm:ss): 0:54:22 | Loss = 0.051141027361154556\nEpoch 217 took 14.97s | Cumulative time (h:mm:ss): 0:54:37 | Loss = 0.0668439045548439\nEpoch 218 took 15.29s | Cumulative time (h:mm:ss): 0:54:53 | Loss = 0.06568565964698792\nEpoch 219 took 15.01s | Cumulative time (h:mm:ss): 0:55:08 | Loss = 0.04800529032945633\nEpoch 220 took 15.03s | Cumulative time (h:mm:ss): 0:55:23 | Loss = 0.057583827525377274\nEpoch 221 took 15.54s | Cumulative time (h:mm:ss): 0:55:38 | Loss = 0.05096607282757759\nEpoch 222 took 15.58s | Cumulative time (h:mm:ss): 0:55:54 | Loss = 0.06194727495312691\nEpoch 223 took 16.07s | Cumulative time (h:mm:ss): 0:56:10 | Loss = 0.051135312765836716\nEpoch 224 took 15.04s | Cumulative time (h:mm:ss): 0:56:25 | Loss = 0.05906611680984497\nEpoch 225 took 15.0s | Cumulative time (h:mm:ss): 0:56:40 | Loss = 0.05735819414258003\n\tMSE-1 0.12895438075065613\n\tMSE-5 0.2523091733455658\n\tMSE-10 0.44164690375328064\n\tMSE-20 0.6200147271156311\n20 20000\n\tPSE 0.9383554851403358\n\tPSE per dim [0.9245173188353023, 0.9325632015342504, 0.9606392411100564, 0.951125189748732, 0.9466314637515689, 0.9252442931597129, 0.9277676878427291]\nComputing KLx-GMM\n\tKLx 1.161453366279602\nEpoch 226 took 15.13s | Cumulative time (h:mm:ss): 0:56:55 | Loss = 0.05370234698057175\nEpoch 227 took 15.14s | Cumulative time (h:mm:ss): 0:57:10 | Loss = 0.06365973502397537\nEpoch 228 took 15.15s | Cumulative time (h:mm:ss): 0:57:25 | Loss = 0.06242561340332031\nEpoch 229 took 15.16s | Cumulative time (h:mm:ss): 0:57:41 | Loss = 0.0584820993244648\nEpoch 230 took 15.04s | Cumulative time (h:mm:ss): 0:57:56 | Loss = 0.04857436567544937\nEpoch 231 took 15.14s | Cumulative time (h:mm:ss): 0:58:11 | Loss = 0.05460485443472862\nEpoch 232 took 15.18s | Cumulative time (h:mm:ss): 0:58:26 | Loss = 0.051015742123126984\nEpoch 233 took 15.1s | Cumulative time (h:mm:ss): 0:58:41 | Loss = 0.05250749737024307\nEpoch 234 took 15.05s | Cumulative time (h:mm:ss): 0:58:56 | Loss = 0.059085115790367126\nEpoch 235 took 15.12s | Cumulative time (h:mm:ss): 0:59:11 | Loss = 0.06278356909751892\nEpoch 236 took 15.69s | Cumulative time (h:mm:ss): 0:59:27 | Loss = 0.05229711905121803\nEpoch 237 took 15.63s | Cumulative time (h:mm:ss): 0:59:43 | Loss = 0.055251311510801315\nEpoch 238 took 15.29s | Cumulative time (h:mm:ss): 0:59:58 | Loss = 0.04915580898523331\nEpoch 239 took 15.13s | Cumulative time (h:mm:ss): 1:00:13 | Loss = 0.04950890317559242\nEpoch 240 took 15.07s | Cumulative time (h:mm:ss): 1:00:28 | Loss = 0.057953279465436935\nEpoch 241 took 15.14s | Cumulative time (h:mm:ss): 1:00:43 | Loss = 0.05499740689992905\nEpoch 242 took 15.04s | Cumulative time (h:mm:ss): 1:00:58 | Loss = 0.057027969509363174\nEpoch 243 took 15.02s | Cumulative time (h:mm:ss): 1:01:13 | Loss = 0.05915067717432976\nEpoch 244 took 14.87s | Cumulative time (h:mm:ss): 1:01:28 | Loss = 0.05233095958828926\nEpoch 245 took 15.01s | Cumulative time (h:mm:ss): 1:01:43 | Loss = 0.056947797536849976\nEpoch 246 took 14.99s | Cumulative time (h:mm:ss): 1:01:58 | Loss = 0.06305679678916931\nEpoch 247 took 15.0s | Cumulative time (h:mm:ss): 1:02:13 | Loss = 0.054509103298187256\nEpoch 248 took 15.57s | Cumulative time (h:mm:ss): 1:02:29 | Loss = 0.04861586540937424\nEpoch 249 took 15.57s | Cumulative time (h:mm:ss): 1:02:44 | Loss = 0.05961756035685539\nEpoch 250 took 15.11s | Cumulative time (h:mm:ss): 1:02:59 | Loss = 0.05681082233786583\n\tMSE-1 0.12924529612064362\n\tMSE-5 0.2521158754825592\n\tMSE-10 0.4422418475151062\n\tMSE-20 0.629858136177063\n20 20000\n\tPSE 0.9123639329732767\n\tPSE per dim [0.931346591443843, 0.903871480885918, 0.928882538386965, 0.9316740894136307, 0.9154689663043147, 0.904446993030172, 0.8708568713480932]\nComputing KLx-GMM\n\tKLx 1.395721197128296\n--- Training finished ---\n \n✅ Saved model to /kaggle/working/final_trained_model.pt\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}